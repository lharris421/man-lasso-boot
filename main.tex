\section{Introduction}

Since its unveiling by \cite{Tibshirani1996}, the Lasso (Least Absolute Shrinkage and Selection Operator) has been a popular model of choice, even in cases which would not necessarily be classified as high-dimensional. Its ability to both do variable selection and estimation lends itself to a particular ease. Additionally, in scenarios where both predictive accuracy and interpretability are desired, the Lasso excels. This is particularly useful for models with a large number of covariates and the underlying model is believed to be sparse but where the exact subset of significant predictors is unknown. In such scenarios, the Lasso helps to identify the most relevant variables leading to a more parsimonious and hence interpretable model. \cite{HTF2009} provide an overview of the Lasso's performance in various settings, demonstrating its optimality in various examples in which the situations and results desired are similar to those previously mentioned. With that said, inference for the lasso has proven to be a difficult task, especially when one desires to obtain proper confidence intervals.

The challenges of constructing confidence intervals have lead to various alternative approaches in the literature. For example, the concept of post-selection inference as discussed in \cite{LeeEtAl2016}, which aims to account for the uncertainty in model selection by conditioning on the selected model or \cite{ZhangZhang2014} which focuses on a desparsified approach to correct the bias of the lasso and facilitate more traditional forms of inference. Some focus has been placed on using the bootstrap to produce confidence intervals, however, the conventional opinion since \cite{Chatterjee2010} is that the traditional use of bootstrapping to produce confidence intervals for penalized regression is suboptimal. The suboptimality arises directly due to the inherent bias and sparsity of estimates produced by soft thresholding.

Thus, we take a different perspective and raise a new question to conventional wisdom: does the bootstrap not work or do we need to rethink the properties of confidence intervals in high dimensional settings? We find that it's a combination of both. There does need to be a methodological fix, but also we need to rethink high dimensional confidence intervals (HDCIs).

\section{Difficulties of Confidence Intervals for lasso}\label{Sec:Difficulties}

We see the difficulty of constructing HDCIs arising from two problems when proposing to use bootstrapping for penalized regression, we will refer to these obstacles as,

\begin{enumerate}
\item the Bias Tradeoff, and
\item the Epsilon Conundrum.
\end{enumerate}

The Bias Tradeoff refers to the inherent bias introduced by penalization in order to be able to fit over-saturated models. This penalization leads to coverages higher than nominal coverage rates for coefficients with true values at or very near zero and leads to lower coverage when coefficients are non-zero. We argue that coverage in the traditional sense is too ridged of a paradigm to apply to penalized regression inference. Instead, we offer a different perspective and argue that the impact that penalization induced bias has is an inherent feature of penalized regression rather than a flaw. Since high dimensional problems often necessitate such an alternate approach, we offer guidance on how to interpret the confidence intervals for lasso as presented in this paper. Additionally, we compare the proposed method to two other inferential methods which take a different perspective.

Before going further, it will be helpful to specify the behavior of the intervals we are interested in. Specifically, we are interest in intervals that are faithful to the model being fit. This inherently excludes most intervals that attempt to debias. This restriction imposes a behavior on the intervals. Since, bias being introduced into the model has varying effects on coefficients of increasing magitude, this implies that interval behavior will vary with the magnitude of $\beta$.

By limiting focus to intervals that are true to the model fit, we have little control over the center of the intervals. As the magnitude of $\beta$ increases, so generally does the bias. As we will see, the width of proposed intervals also generally does increase, but at a relatively lower rate than the bias. As a direct result, we would expect a ``faithful'' confidence interval produce different coverage rates as a function of the magnitude of $\beta$.

Instead, we propose a different focus: overall coverage. As a direct result of this, one must expect that such intervals are going to over cover values of $\beta$ that are small and undercover those which are larger in magnitude. The goal then being finding a method that is well calibrated to provide overall coverage near that of nominal.

The second problem, the Epsilon Conundrum, is also related to the shrinkage but arises as an arguably more disturbing manifestation: confidence intervals of length zero or, more often, with a single endpoint that is exactly zero. Some penalized regression methods, particularly the lasso, often result in a sparse solution. If using a traditional quantile based bootstrap confidence interval, this will lead to an interval of [0,0] if a given variable is rarely or never included in the active set. As the dimensionality of the problem grows, this becomes an increasing occurrence leading to a large majority of intervals possessing a length of zero. When the true value of the coefficient is 0, the interval at least contains the truth. However, this issue is particularly troublesome when one considers what happens when the true value is not precisely zero, as is likely the case in most reasonable scenarios. By just shifting the true value by $\eps$, an immediate drop in coverage would occur, hence the name: the Epsilon Conundrum.

\begin{figure}[hbtp]
  \includegraphics[width=\linewidth]{traditional}
  \caption{\label{Fig:traditional} Caption goes here}
\end{figure}

The later of these two problems is addressed in our novel approach to producing bootstrap based confidence intervals for the lasso.

\section{Notation}

Throughout this paper, certain notation will be repeatedly used. $B$ will represent the total number of bootstrap datasets generated. $\boldsymbol{X}^b$ and $\boldsymbol{y}^b$ refer to the $b^{th}$ bootstrap sample. Similarly, $\hat{\beta}^b_j$ will refer to the estimate for $\beta_j$ from the lasso fit to $\boldsymbol{X}^b$ and $\boldsymbol{y}^b$. However, the previous notion with a superscript $b$ will often be used in a nonspecific manner to indicate that an estimate is with respect to a bootstrap draw rather than a specific bootstrap draw.

\section{Lasso Bootstrap Confidence Intervals}

\subsection{An Overview of Proposed Methods}

In what follows, we examine four alternative methods for obtaining bootstrap draws.

The first is the traditional bootstrap which simply involves taking the point estimate for each $\beta_j$ for each respective bootstrap sample. As mentioned previously, and as we will see, this has clear pitfalls which the subsequent three alternatives attempt to address.

The second is the posterior bootstrap. This leverages the Bayesian formulation of the lasso. For each bootstrap sample, a random draw from the full conditional posterior is obtained instead of taking the point estimate (equivalent to the mode) of the posterior.

The posterior bootstrap tends to be too aggressive in the additional variability introduced. As such, the third alternative borrows behavior from both the traditional and the posterior bootstrap and is appropriately called the hybrid bootstrap in this manuscript. The hybrid bootstrap only samples from the full conditional if the point estimate for a given $\beta_j$ is equal to zero for that bootstrap sample. This alternative will be the main focus of the manuscript.

The fourth and final method is a naive approach at producing debiased intervals and as such is referred to as the debiased bootstrap in this manuscript. Instead of taking the point estimate as the traditional method would, this method uses the correlation between the partial residuals obtained from the lasso with $\boldsymbol{x}_j$. As such, one might expect this to address both issues mentioned above. As we will see, it does in fact, however, its performance in general is suboptimal when $p < n$.

\subsection{Intuition behind the hybrid bootstrap}

Given that the hybrid bootstrap is the main focus of this manuscript, it will be worth while to briefly consider the intuition driving this proposed sampling method. Recall, potentially the most striking issue with the traditional bootstrap is that it can produce intervals which are singleton at 0. This occurs when $\hat{\beta}_j^b = 0$ for at least $B * (1 - \alpha)$ draws. To address this issue, then, adjustment is only needed when $\hat{\beta}_j^{b} = 0$. Next, we consider the behavior of this interval from a logical perspective.

For a moment, hold p constant. As n increases and we let $\lambda$ get selected by CV, this method will converge to that of applying the traditional bootstrap to a classical linear model. This is the case because with increasing n, $\lambda_{CV} \rightarrow 0$. On the other hand, as n decreases $\lambda_{CV} \rightarrow \lambda_{max}$ and $\hat{\boldsymbol{\beta}}\rightarrow \boldsymbol{0}$. On this end of the extreme, bootstrap samples are then drawn from the marginal posterior. The coverage behavior is well understood for the bootstrap applied to linear regression as n increases. By only sampling from the posterior when $\hat{\beta}_j^b = 0$, we minimize unnecessary variability as n increases. The behavior on the other end of the extreme is relatively less understood compared to the asymptotics of the traditional bootstrap. However, we argue that leveraging the posterior lends itself to reasonable behavior especially when the alternative would be confidence intervals which are all identical to zero. Such intervals, one could argue, would necessarily have 0\% coverage. Conversely, as we will see, with smaller values of n, the posterior tends to lead us to wider confidence intervals. See Section~\ref{Sec:full-cond}.

\subsection{Conceptual Implementation}
\label{Sec:full-cond}

In this section, we describe the details and important considerations for the sampling performed in the posterior and the hybrid bootstrap.

As with other penalized regression models, the lasso can be formulated as a Bayesian regression model by setting an appropriate prior. This was addressed initially by \cite{Tibshirani1996} and covered more extensively by \cite{Park2008}. For the lasso, the prior is a Laplace distribution, also referred to as the double-exponential distribution:

\as{\p(\bb) = \prod_{j = 1}^{p} \frac{\gamma}{2}\exp(-\gamma \abs{\beta_j}), \gamma > 0}

With this prior, the lasso estimate $\bbh(\lam)$ is the posterior mode of $\bb$ when $\lam = \gamma \frac{\sigma^2}{n}$. We purpose leveraging this relationship for building confidence intervals.

Unfortunately, a Normal likelihood and Laplace prior are not conjugate and in general, the absolute value in the exponent of the Laplace makes many common manipulations for the posterior more difficult. Luckily, however, the full conditional posterior can be shown to be a mixture of a right and left truncated normal where the truncation occurs at zero for right and left tails respectively. To obtain a full conditional posterior, we use the partial residuals, $\r_{-j}$, in the likelihood. This is natural when considering the common CD algorithms used to arrive at lasso estimates. In this manuscript, we will assume that $\X$ has been standardized s.t. $\x_j^T\x_j = n$.

Note that in what follows, the full conditional represents the distribution for a given value of $\lambda$ and $\sigma^2$. Further discussion will be given to selecting these values later, however, in the meantime to reduce notational distractions we will treat them as known quantities and implicitly condition on them.

Then, for $\beta_j$,

\as{
L(\beta_j | \r_{-j}) &\propto \exp(-\frac{n}{2\sigma^2} (\beta_j^2 - 2z_j\beta_j)), \\
\text{ where } z_{j} &= \frac{1}{n} \x_{j}^{T}\r_{-j} \text{, and} \\
P(\beta_j | \r_{-j}) &\propto
\begin{cases}
C_{-} \exp(-\frac{n}{2\sigma^2} (\beta_j - (z_j + \lambda))^2), \text{ if } \beta_j < 0, \\
C_{+} \exp(-\frac{n}{2\sigma^2} (\beta_j - (z_j - \lambda))^2), \text{ if } \beta_j \geq 0 \\
\end{cases}
}

Where, $C_{-} = \exp(\frac{z_j \lambda n}{\sigma^2})$ and $C_{+} = \exp(-\frac{z_j \lambda n}{\sigma^2})$.

What makes this formulation attractive is that a mapping allows tail probabilities from the posterior to be translated to probabilities onto corresponding known normal distributions (i.e. $\N(z_j \pm \lambda, \frac{\sigma^2}{n})$). The allows for numerically stable and efficient sampling to be obtained from the full conditional posteriors. Details of how sampling is performed are outside of the scope of this manuscript and are provided in Supplement~\ref{Sup:A}. For the remainder of the manuscript, we simply refer to this process as ``sampling from the full conditional.''

Again, recall that this solution corresponds to a particular value of $\lam$ and $\hat{\sigma}^2$. Our recommendation is to use CV to select $\lam$ and produce an estimate for $\sigma^2$ and produce bootstrap confidence intervals corresponding to these values. Performance under this recommendation will be focused on in the Results section. However, it should be noted that producing an estimate for $\sigma^2$ in this manner implicitly depends on $\lam$, so new estimates for $\sigma^2$ should be obtained for each value of $\lam$. When referring to $\hat{\sigma}^2$ it will always be as a function of $\lam$ which we drop for notational convenience.

All together this leads to the following steps to obtain CIs through the various alternative bootstraps:

\begin{enumerate}
\item Perform CV using the original data to select $\lam$ and estimate $\hat{\sigma}^2$
\item For b $\in \lbrace 1, \ldots, B \rbrace$:
\begin{enumerate}
\item Obtain a pairs bootstrap sample, $\X^b$ and $\y^b$
\item Fit lasso with $\X^b$ and $\y^b$, obtain $\bbh^b$, $\boldsymbol{z}^b$
\item For j $\in \lbrace 1, \ldots, p \rbrace$:
	\begin{algorithmic}
	\Switch{method}
    \Case{traditional}
      \Assert{$q_j = \bh_j$}
    \EndCase
	  \Case{posterior}
    \Assert{
      $p^* = \Unif(0, 1)$ \\
      \hspace{1.95cm} $q_j = P^{-1}(p^*|\r_{-j})$
    }
    \EndCase
    \Case{hybrid}
      \Assertif{$\bh_j \neq 0$}{$q_j = \bh_j$}
      \Assertelse{
        $p^* = \Unif(0, 1)$ \\
        \hspace{1.8cm} $q_j = P^{-1}(p^*|\r_{-j})$
      }
    \EndCase
    \Case{debiased}
	    \Assert{$q_j = z_j$}
    \EndCase
	\EndSwitch 
	\end{algorithmic}
\item Save $\q$
\end{enumerate}
\item Row bind all B $\q$ vectors to obtain a $B \times p$ matrix of bootstrap draws
\item For each $\beta_j$, compute the quantiles for $p_L = (.5 - \l/2)$ and $p_U = (.5 + \l/2)$ from the $j^{th}$ column of the draws to produce a final confidence interval estimate for significance level $\l$.
\end{enumerate}

\section{Results}

We make the assertion that the ideal scenario for these methods is when the true distribution of $\bb$ follows a laplace (double exponential) distribution. This may raise the question of what rate should be used. In fact, this question could be generalized for any of the distributions which $\bb$ is drawn from in the simulations that follow. However, the data generating mechanism we use renders the choice of scale arbitrary. In what follows, we set SNR = 1 and $\sigma^2 = 1$ which imposes the restriction that $\bb^T\bb$ also equals 1. As a result, regardless of the scale parameter used, after normalization, the draws for $\bb$ will be identical (assuming the same seed is used). 

\logan{ After considering more distributions, I'm not sure if the laplace being ideal is a necessary assertion to make. It almost seems like the uniform distribution is the ideal distribution and that overcoverage generally results as more density/mass is moved to zero. And, unless we pull Figure~\ref{Fig:true_lambda} out of an appendix, we really didn't end up using that under a laplace we know the true value of $\lambda$.}

\subsection{Interval Behavior}

We compare the proposed CI methods by showing the coverage of each, both overall and as the magnitude of $\beta$ changes, then present results on the width and bias of the intervals as a way of gaining insight into why the methods result in over or under coverage at various magnitudes of $\beta$.

\subsubsection{Coverage}\label{Sec:Coverage}

Figure~\ref{Fig:laplace} displays simulation results for CIs obtained via the four previously described bootstrap methods. In this simulation, 100 independent datasets were generated and each bootstrap method was applied using $B = 1000$ bootstrap iterations. Each dataset was simulated as follows. $\X$ was generated independently with $n = p = 100$ and $\bb$ was generated from a Laplace distribution. Then, $\Y$ was constructed as $\Y = \X\bb + \boldsymbol{\epsilon}$, where $\boldsymbol{\epsilon} \sim N(0, \sigma^2_y)$ with $\sigma^2_y$ set such that the $SNR = 1$. The dotted lines represents the average coverage for each method across all variables for all 100 datasets. The solid lines are estimates of coverage as a smooth function of $\beta$.

\begin{figure}[hbtp]
  \includegraphics[width=\linewidth]{laplace}
  \caption{\label{Fig:laplace} Results are from the simulation described in Section~\ref{Sec:Coverage}. The fitted curves are from Binomial GAMMs (one for each method) fit with coverage being modeled as a smooth function of $|\beta|$ and with a random intercept on dataset to account for deviations in coverage specific to a given randomly generated dataset. The data used for modeling contains a row for each variable per simulated dataset. The dashed lines represent the average for each method across all 100 independently generated datasets and the solid black line indicates the nominal coverage rate. The shaded distribution in the background depicts the positive tail of the distribution the $\beta$s were drawn from, $dexp(\tau = 2)$.}
\end{figure}

Before describing the observed coverage behavior, we draw brief attention to Figure~\ref{Fig:laplace} as a depiction that Hybrid is a mixture of Posterior and Traditional, being more like Posterior when $\beta$ is small in magnitude and increasingly more like Traditional as $\beta$ increases in magnitude.

Figure~\ref{Fig:laplace} also shows that, as is generally the case when applied to the lasso, the Traditional bootstrap coverage is far below that of nominal. At the other end of the extreme, Posterior, which draws from the full conditional for every bootstrap draw, tends to produce intervals that significantly over cover relative to nominal coverage. Both Hybrid, which only samples from the full conditional when $\bh = 0$, and Debiased produce coverage near the $80\%$ nominal coverage rate.

\logan{Is this better?}

To understand the coverage behaviors further, it helps to consider what happens near zero and far-from-zero. Traditional has low coverage both near and far-from zero. Debiased also under covers both near and far-from zero but to a much lesser extent and, of the methods considered here, generally sticks closest to nominal coverage regardless of the magnitude of $\beta$. Conversely, Posterior and Hybrid both display a significant degree of over coverage for values of $\beta$ near zero. However, like the other two methods, Posterior and Hybrid also both under cover far-from-zero but to varying degrees. 

Put more explicitly, what is novel for Posterior and Hybrid is the decreasing trend that causes the coverage rate to be very high for values of $\beta$ near zero and to be lower for values of $\beta$ larger in magnitude. In classical settings, we would expect coverage to be constant despite the value of $\beta$, which is not the case here. This decreasing pattern is typical for these methods and is to be expected from intervals arising from procedures that are faithful to the model fit. As emphasized in Section~\ref{Sec:Difficulties}, introducing bias through penalization is an inherent feature of the lasso. This bias can largely explain the pattern observed. In fact, as we will see, the effect of penalization on the width and bias of the intervals can explain the coverage patterns seen for all four methods, which is where we now turn our attention to.

\subsubsection{Width and Bias}\label{Sec:Width and Bias}

\begin{figure}[hbtp]
  \includegraphics[width=\linewidth]{laplace_width_bias}
  \caption{\label{Fig:laplace_width_bias} Results are from the simulation described in Section~\ref{Sec:Coverage}. The fitted curves are from GAMMs (one for each method) fit with width and central bias being modeled as a smooth function of $|\beta|$. The data used for modeling contains a row for each variable per simulated dataset. Additionally, the models contain a random intercept on dataset to account for deviations in width and central bias specific to a given randomly generated dataset. Central bias is defined here as the difference between the center of an interval and the true value of $\beta$, with positive values indicating bias towards zero. Further discussion on central bias can be found in Section~\ref{Sec:Width and Bias}}
\end{figure}

Figure~\ref{Fig:laplace_width_bias} largely explains the coverage behavior seen in Figure~\ref{Fig:laplace}. The fitted curves in each plot are constructed in the same manner as for Figure~\ref{Fig:laplace}, albeit on the respective outcome of interest. The left side of Figure~\ref{Fig:laplace_width_bias} shows that the interval width tends to increase as $\beta$ increases in magnitude. This behavior is directly related to the proportion of times a variable is selected to be in the model. When a given $\beta$ is near zero, its estimate will often be shrunk to zero which results in less variable bootstrap draws than for a variable which is always selected. Worded differently, a variables with a larger corresponding true value of $\beta$ will have a wider range of plausible estimates for each bootstrap draw than for a variable with a true value of $\beta$ nearer to zero, all else equal. This effect is most notable for the Traditional bootstrap, as it is the only method which will result in a draw exactly equal to zero when $\bh^b_j = 0$. The three other methods all introduce some additional variability in this scenario. This explains why Traditional produces much narrower intervals, especially for values of $\beta$ smaller in magnitude, and consequently largely explains the drastic under coverage observed in the previous section. This is a manifestation of the Epsilon Conundrum which will be discussed in more detail in Section~\ref{Sec:Epsilon}, but of note is the tendency of Traditional to produce the interval $[0,0]$ for such estimates of $\beta$. This plots also the explains the over coverage of the Posterior bootstrap, as it produces significantly wider intervals regardless of the magnitude of $\beta$, which we saw were too wide in the previous section.

The right side of Figure~\ref{Fig:laplace_width_bias} provides an initial depiction of the bias of the confidence intervals produced by each method. Here, a positive value indicates bias towards zero. Before going forward, it is important to emphasize that while informative, representation of interval bias has its limitations. While a clear definition exists for the bias of a point estimate, the bias of an interval lacks an accepted definition. This is for good reason, it is unclear what is meant by the bias of an interval. However, given the construction of the proposed intervals, the median bootstrap draw seems to be a logical definition of center and consequently what we use here for the determination of bias. With this point in mind, we see the expected behavior that center of the intervals tend to be increasingly biased towards zero for larger absolute values of $\beta$. This plot also indicates that Traditional, Posterior, and Hybrid have similar amounts of central bias, with Traditional having just a touch more. Although the Debiased method cuts the bias in about half relative to the other methods (but still shows an increasing trend), we see that it is unable to completely eliminate bias for reasons that are beyond the scope of this manuscript.

At this point, we would agree with any reader who feels that this depiction of bias is incomplete. Indeed, there are intricacies in the coverages that are not captured in Figure~\ref{Fig:laplace_width_bias}. Accordingly, Figure~\ref{Fig:laplace_bias_nfb} provides additional information about the interval behavior, all of which could be considered under the umbrella of bias. Since there is no one great measure of bias, considering these multiple measures provides a reasonable way to communicate the bias related behavior more wholistically. The plots in Figure~\ref{Fig:laplace_bias_nfb} are constructed the same way as Figures \ref{Fig:laplace} and \ref{Fig:laplace_width_bias}. The upper left and right plots depict the probability that an interval misses the truth either towards or away from zero, respectively, as a function of the magnitude of $\beta$. For the most part, as a result of bias, we see that the intervals generally miss towards zero and accordingly increases as the magnitude of $\beta$ increases. This plot also suggests that the benefit of debasing in the manner adopted by Debiased only proves beneficial for values of $\beta$ large in magnitude and seems to actually increase bias towards zero for $\beta$s near zero. We also see this misbehavior for Debiased, again, especially for $\beta$s near zero, with a tendency to miss away from zero to quite a large degree. Although Hybrid also displays this tendency, it does so to an extent an order of magnitude less and quickly drops to negligible levels as the magnitude of $\beta$ increases. The bottom left plot shows the difference between the probability a method misses towards zero and the probability it misses away, and, in fact, the only method that shows a tendency to ``overshoot'' is Debiased when $\beta$ is near zero. However, this is not the whole story. As set up, the probability that a method misses towards zero by definition converges to zero as $\beta \rightarrow 0$ (unless the method can produce bounds that are exactly equal to zero). This is because we defined a ``miss towards zero'' with the requirement that at least part of the interval lie between the truth and zero. If the entire interval was the opposite sign of the true value, we defined this as the dreaded Type 3 error, which is presented in the bottom right plot. Here again we see Debiased as the stand out, having a significant tendency for Type 3 errors, especially near zero. Hybrid too displays this tendency, but as before with misses away from zero, to a much lesser extent.

\begin{figure}[hbtp]
  \includegraphics[width=\linewidth]{laplace_bias_nfb}
  \caption{\label{Fig:laplace_bias_nfb} Caption goes here.}
\end{figure}

Now that we have a deeper understanding of interval behavior, we can understand the trends in coverages seen in Figure~\ref{Fig:laplace}. Traditional has the most amount of bias, notably towards zero regardless of the magnitude of $\beta$, and produces very narrow intervals including ones of length zero. As a result, the Traditional bootstrap under covers regardless of the magnitude of $\beta$. On the opposite end of the extreme, the Posterior bootstrap tends to produce intervals that over cover, mainly due to having intervals that are in general too wide. However, the over coverage does not occur across the range of $\beta$ values. The Posterior and the Hybrid share a common pattern. This pattern is explainable through the effect of bias towards zero and the widths of the intervals (specifically that they are large relative to the bias unlike traditional). As a result, a significant over coverage occurs. For values of $\beta$ near zero, the effect of bias is minimal while the width is only minorly decreased leading to coverage levels near 1. However, as $|\beta|$, and the associated bias, increase, this eventually leads to lower coverage for values of $\beta$ larger in magnitude. So, we see the main difference between Hybrid and Posterior is that Hybrid produces narrower intervals, as it only samples from the Full Conditional Posterior (the drive of the increased with for Posterior) when $\bh = 0$. Another observable effect of this sampling mechanism is that we see that the behavior of Traditional and Hybrid largely coincide for $|\beta| > 0.2$, suggesting at this point $\beta$s this large rarely if ever have draws with $\bh = 0$. Of the four methods considered, Debiased exhibits the most complex behavior. It has intervals similar in width to Hybrid, but nearly half the bias for any given value of $\beta$, yet, it produces coverage below that of nominal but which is rather consistent despite the magnitude of $\beta$. Why is this the case? Well, we saw in Figure~\ref{Fig:laplace_bias_nfb} that it is due to the rather sporadic behavior of the intervals, especially for values of $\beta$ near zero. 

\logan{Make sure this pattern is described adequatly earlier.}

We hope the reader is exceedingly convinced that the Traditional bootstrap is a poor choice. Additionally, the posterior isn't bad, it just generally provides intervals significantly wider than that of the Hybrid bootstrap. As will be demonstrated throughout the remainder of this manuscript, when the Hybrid departs from nominal coverage, it generally does so in the direction of over-covering. Thus, in these scenarios, the wider intervals of the Posterior are strictly worse than those produced by the Hybrid. The remaining two methods, Hybrid and Debiased, both produce coverage levels that are reasonably near that of nominal. As such, we will give both further consideration. As just noted, for the simple case considered in the first simulation, Debiased does tend to produce coverage near nominal. However, Debiased, approaches nominal (with increasing sample size) from below as the uncorrected bias for larger magnitude $\beta$s slowly diminishes. Additionally we saw that even if overall coverage looks good, that the underlying behavior is less than ideal (especially the Type 3 error). Hybrid, on the other hand, behaves more stably and tends to approach nominal from above rather than below. This stable behavior extends to more complex data generating mechanism, something that we will see does not hold true for Debiased in Section~\ref{Sec:Robustness}.

\subsection{Robustness}\label{Sec:Robustness}

This section explores a number of scenarios to help understand the behavior of the Hybrid bootstrap. It begins with a look at the coverage behavior when there is correlation among the predictors and provides a comparison to the behavior of Debiased. The results in this section give the first depiction of how the behavior changes with increasing sample size. As mentioned previously and as we will see later in this section, Debiased begins to break down under this more complex scenario and as such this will conclude any further consideration of the Debiased method in this manuscript. 

\subsubsection{Correlation}

Now, lets consider some cases with higher levels of correlation. In Figure~\ref{Fig:correlation_structure}, the first row contains the coverages across 100 simulated datasets with n = p = 100 with $\beta \sim laplace(rate = 2)$ but with autoregressive correlation added. Note that for a moderate amount of correlation ($\rho = 0.4$), there is little impact of the coverage of the intervals (see Figure~\ref{Fig:laplace} for $\rho = 0$).  As we increase the correlation, however, we begin to see under coverage for larger values of n. That said, even with a significant amount of autoregressive correlation, we still observe a median coverage just under 70\%.

\begin{figure}[hbtp]
  \includegraphics[width=\linewidth]{correlation_structure}
  \caption{\label{Fig:correlation_structure} Caption goes here}
\end{figure}

Debiased, which admittedly was a naive attempt, has accumulated a number of drawbacks which bring in to question its stability and suggest it should be dropped from consideration along with Traditional and Posterior methods. The remainder of the manuscript focuses solely on the Hybrid bootstrap.

\logan{Allude to other future attempts to debais, they always come at a cost. Revisit after getting simulation fixed.}

\subsubsection{Distribution of Beta}

As mentioned in the previous section, the overall coverage rate is a trade off based on the value of $\lam$ and the true distribution of $\beta$, which is of course generally unknown. The plots below are from the same simulation setup as the previous section, but with different distributions for $\beta$. Focusing on $\lam_{CV}$, the coverage rate is similar regardless of the distribution of $\beta$, with $\beta \sim N(0, 1)$ producing coverage nearest to nominal.

In classical high dimension scenarios, it is often expected that the solution is sparse. It should be no surprise then that when the true data generating mechanism is more sparse than the laplace indicated by the respective $\lambda$ that this method will provide over greater coverage.

These plots also exemplify the effect of the magnitude of $\beta$ on coverage. The greatest separation is then $\lam = \lam_{max}$ and their coverage converges as $\lam \rightarrow \lam_{min}$. Coverage is of course higher for $\beta$ values smaller in magnitude. At values of $\beta$ that are classified as small in magnitude in this set of plots, their coverage is highest at $\lam_{max}$ and generally decreases as $\lam$ is reduced. On the other end, for $beta$ values largest in magnitude, we a monotonically increasing trend, with those moderate in size falling somewhere in the middle in their pattern depending on the distribution of $\beta$.

\begin{figure}[hbtp]
  \centering
  \includegraphics[width=.4\linewidth, height=.25\pdfpageheight]{distribution_table}
  \caption{\label{Fig:dist_beta} Caption goes here}
\end{figure}


\subsubsection{Epsilon Conundrum}\label{Sec:Epsilon}

Its natural to close this section by considering the performance on a scenario that was the main motivator of the Zero Sample method: the Epsilon conundrum. So how does Zero Sample Perform in this scenario? It avoids the downfall we observe with applying the Traditional bootstrap. Clearly, however, it also over covers due to the fact that the large majority of $\beta$s are near zero, but, again, this is to be expected and the important take away is that Zero-Sample provides viable intervals for values of $\beta$ near zero.

\begin{figure}[hbtp]
  \includegraphics[width=\linewidth]{beta_lambda_heatmap_laplace}
  \caption{\label{Fig:beta_lambda_heatmap_laplace} n = p = 100, where $\beta \sim Laplace(rate = 2)$ and $\X$ generated under independence structure. The red line represents the average CV value of $\lam$. The x-axis was truncated over the range of $\lam_{CV}$ and presented relative to each simulations $\lam_{max}$.}
\end{figure}

\subsubsection{Selection of \texorpdfstring{$\lambda$}{lambda} (and Estimation of \texorpdfstring{$\sigma^2$}{sigma squared})}

The reader may be wondering about what happens if the as the value of $\lambda$ is changed. The data in Figure~\ref{Fig:beta_lambda_heatmap_laplace} comes from a simulation where $\lambda$ was evenly distributed on the $\log_{10}$ scale from $\lam_{\max}$ to $\lam_{\min} = \lam_{max} * 0.001$. At each value, confidence intervals were obtained and coverage was recorded. This was repeated 100 times, then a gam was fit to provide a smooth estimate of the coverage rate. The relative coverage is defined as estimated coverage minus nominal coverage.

The pattern is exactly as we would intuitively expect. Depending on the value of $\lam$, values closer to zero see moderate amounts of over coverage whereas there is increasingly greater under coverage as $|\beta|$ increases. The value at where this transition occurs (and at which coverage of a specific value of $\beta$ = nominal) varies considerably over the range of $\lam$. At $\lam_{max}$, this transition occurs at a relatively small $|\beta|$. As the value of $\lam$ decreases from $\lam_{max}$ this transition occurs at increasingly large values of $\beta$ until all values of $\beta$ have an estimated coverage rate at or above that of nominal. To obtain a covariate rate near that of nominal, $\lam$ needs to be selected such that over and under coverage is balanced. In this scenario, $\lam_{CV}$ does a reasonable job at providing such a balance. This balance, of course, depends on the true distribution of $\beta$ (recall that here it is laplace). However, as we will see next, $\lam_{CV}$ generally does well regardless of the distribution of $\beta$.

At this point, it is also important to note that this performance is in spite of needing to provide estimate for the true values of $\lam$ and $\sigma^2$. In fact, we found that knowing these values actually produces worse performance, suggesting that for this method $\lam_{CV}$ and $\sigma^2(\lam_{CV})$ are reliable selections.

\subsubsection{Nominal Coverage}

\begin{figure}[hbtp]
  \includegraphics[width=\linewidth]{nominal_coverage}
  \caption{\label{Fig:nominal_coverage} Caption goes here}
\end{figure} 

Figure~\ref{Fig:nominal_coverage} is similar to Figure~\ref{Fig:laplace}, but it focuses only on Hybrid and gives simulation results for three values of n across three different nominal coverage rates. Since the method has a varying coverage rate based on the magnitude of $\beta$, it is important to consider different nominal coverages. Otherwise, it is conceivable that a method could perform well at one coverage rate but not another. However, that is not the case here. Regardless of the nominal coverage, the general pattern remains the same: the method over covers for smaller values of n but coverage converges to the nominal coverage rate relatively quickly. The only difference seen is the compression of this pattern for higher nominal coverage rate. Going forward, the nominal coverage will be set at $80\%$ to reduce compression so that any patterns present are easier to observe.

\begin{figure}[hbtp]
  \includegraphics[width=\linewidth]{zerosample2}
  \caption{\label{Fig:zerosample2} Caption goes here}
\end{figure}

\subsection{Comparison to Ridge Regression CIs}

We start of with a simple example and compare the intervals produced by Zero Sample with that of Ridge. In this example again n = p = 100, however, only one $\beta$ is non-zero. That is, $\beta_{A1} = 1$ and $\beta_{B1}, \beta_{N_1}, \ldots, \beta_{N98} = 0$. Additionally, the data is simulated such that $\rho(\beta_{A1}, \beta_{B1}) = .99$ but all of the N (noise) $\beta$s are uncorrelated with $\beta_{A1}, \beta_{B1}$, and each other.

100 datasets were generated in this manner and each method was applied and respective confidence intervals obtained.

\textbf{Figure A} depicts the results from the simulation with the plot on the right giving box plots for the lower (red) and upper (blue) bounds across the 100 datasets for 3 variables, A1, B1, and N1. On the right, Confidence Intervals for a randomly selected example from the 100 simulated datasets is displayed.

Focusing on A1 and B1, what we would hope to see is wide intervals. Although A1 truly is the signal variable, its high correlation with B1 should produce a large amount of uncertainty about which variable (if not both) contain signal. Ridge, however, fails in this respect in that the intervals are very narrow. On the other hand, Zero Sample displays the desired behavior. The uncertainty entangled in A1 and B1 is reflected in wider intervals. Additionally, intervals for A1 usually do not contain 0 whereas the intervals for B1 contains 0 over 40\% of the time. In general, there is a clear shift in the intervals for A1 compared to B1 suggesting that even with very high correlation, Zero Sample often attributed more of the signal to A1, which was clearly not the case with Ridge.

\begin{figure}[hbtp]
  \includegraphics[width=\linewidth]{highcorr}
  \caption{\label{Fig:highcorr} Caption goes here}
\end{figure}


\subsection{Comparison to Other CI Methods}

\logan{It would be interesting to get a better idea of if it is ``not zero'', how often does it catch? What is the impact on this on interpretaitons? May need to think how to do this in a case like the laplace... i.e. the true value of sigma2, n, xTx, to calculate the half width of an interval that the true value must be greater than to be considered nonzero?}

There are few other methods for obtaining intervals for the lasso. There are even fewer that provide R packages to provide comparisons with. Two that we were able to identify were Selective Inference (from \texttt{selectiveInference}) and Bootstrap Lasso Projection (from \texttt{hdi}). It should be noted, however, that neither method fits the criteria we were looking for of remaining faithful to the model fit.

Specifically, the Bootstrap Lasso Projection (BLP) is also known as the de-sparsified Lasso and by default the method reselects $\lambda$ for each bootstrap sample. Selective Inference (SI) doesn't directly correct for the bias introduced by penalization but this method does only provide intervals for variables with non-zero coefficients. With these points in mind, we compare the relative performance of the proposed Hybrid Bootstrap with these two alternatives.

\logan{Remove points that will be covered in results later}.

Before considering simulation results, there are a couple important limitations of each method that should be noted. For BLP, it should be noted that by with its default arguments, it is an order of magnitude slower than the Hybrid Bootstrap which itself is an order of magnitude slower than SI. Additionally, as implemented, there is no easy way to specify the $\lambda$ used which defaults to the 1-SE solution from \texttt{cv.glmnet}. SI is much faster, but also tended to be much more fragile in our testing. In general, SI does not work for the $p > n$ case (it can, but only if the number of features selected is less than n) and CIs are only produced for variables in the active set. Additionally, it is cumbersome to implement and infinite bounds were more common than not when applied to data where n was on the order of p. As will be emphasized later, in similar scenarios SI also failed to converge in some iterations of the simulation performed. 

\subsubsection{Similation}

The simulation results presented here are identical to the set up described in Section~\ref{Sec:Coverage}. In fact, the results for Hybrid are the same as used for the earlier figures just displayed differently. 

Referencing Figure~\ref{Fig:laplace_comparison}, both BLP and SI initially appear to perform strikingly well. Both have coverage very near that of nominal and lack the pattern seen with the methods proposed in this manuscript and instead provide rather consistent coverage regardless of the magnitude of $\beta$. However, \ref{Fig:laplace_other} tells a different story. First, although BLP provides average coverage rates the closest to $80\%$, there were a number of cases where the coverage dipped significantly. Potentially more concerning, with $n = 400$, the coverage noticeably drops below 80\%. Although the lasso thought of a model for high dimensional data, it is used across the entire spectrum of datasets, so it would be preferred to see convergence towards 80\% coverage.

The behavior for the Hybrid Bootstrap has been covered previously, specifically that it generally over covers when $n < p$ but has coverage very near nominal as n increases above p. Additionally, although not immune to under coverage, it performed more reliably than the other two method.

Before moving onto the results for SI, there are a couple numbers not in Figure~\ref{Fig:laplace_comparison} which are also important to consider and are provided in Table~\ref{Tab:selective_inference}. When n = 50, 20 of the 100 simulations errored out, with only 6 when n = 100, and none when n = 400. The last column gives the average percentage of variables that had confidence intervals of the simulation iterations that did not produce errors. It is this subset that is included in the coverage plot for SI. So, ignoring the times the method failed, even if on average the coverage look reasonable, the wide spread of coverages, including a staggering number below 50\% coverage is concerning. Although performance does improve, the fact that this behavior is not remedied even with larger n should also not be ignored.

\begin{table}[hb]
  \centering
  \begin{tabular}{cccc}
  \hline
  & \multicolumn{2}{c}{Simulations} & Variables \\
  n & \# Succeeded & \# Non-finite Median Width & \# Included on Average \\
  \hline
  50  & 80 & 18 & 18.5 \\
  100 & 94 & 12 & 30.4 \\
  400 & 100 & 3 & 70.1 \\
  \hline
  \end{tabular}
  \caption{Selective Inference Results}
  \label{Tab:selective_inference}
\end{table}

Staying with SI but directing our attention towards the interval widths, the concern surrounding this method only grows. Of the 80 that simulations that exceeded for n = 50, the median width of the intervals produced (from the variables included in the model) was infinite for 18 of the simulation. For n = 100, 12 of the 94 had infinite median widths. By n = 400, only 3 of the simulations had infinite median widths. However, even when the medians were finite, they were nearly always extremely wide, even for n = 400. This behavior was also observed when applying the method to real data sets which will be covered next. BLP and Hybrid on the other hand produce intervals which are more similar in width although BLP does tend to produce wider intervals with and with a bit more variability in the width.

The runtime of the three methods also differs considerably with SI being the fastest and BLP being by far the slowest. Additionally with BLP, there is an odd non-monotonic behavior which we looked into briefly but could not find an explanation for. This behavior occurred in multiple reruns of the same simulation. Hybrid and SI both have a monotonically increasing relationship with sample size, although it Hybrid is more affected by the increasing sample size. In our testing, speed was not a concern for SI, was noticeable for Hybrid, and prohibitive for BLP which will be returned to in Section~\ref{Sec:Scheetz2006}.

\begin{figure}[hbtp]
  \includegraphics[width=\linewidth]{laplace_comparison}
  \caption{\label{Fig:laplace_comparison} Caption goes here}
\end{figure}


\begin{figure}[hbtp]
  \includegraphics[width=\linewidth]{laplace_other}
  \caption{\label{Fig:laplace_other} Caption goes here}
\end{figure}


\subsubsection{Application to the World Health Organization study on acute respiratory illnesses (whoari)}

\logan{How to organize the real data analysis. Don't want sections 4 levels deep but also wanted to break up the analyses more clearly...}

The \texttt{whoari} dataset comes from the study ``Development of a clinical prediction model for an ordinal outcome: the World Health Organization multicentre study of clinical signs and etiological agents of pneumonia, sepis and meningitis in young infants'' written by \cite{Harrell1998}. The study considered a few acute illness in young infants across several countries, and the dataset used here is a subset of 816 infants who presented with pneumonia in the country Ethiopia. As alluded to in the title, collection of this data was done with the intention of building a prediction model to assess the severity of an infant presenting with a serious infection, which represents the main cause of mobility and mortality in infants under 3 months in these developing countries. Diagnosis of severity is a difficult task, and developing rules for grading the severity of disease are important for prompt delivery of treatment for those who need it while also avoiding unnecessarily and costly treatments where possible considering the limited resources in these developing countries. \dots

Of course, we can not ignore that the outcome considered here is ordinal (taking on a number from 1 - 5), however, for simplicity we treat the outcome as continuous and feel that the results are reasonable, at least for a comparison of the three methods under consideration.

With $N \approx 10p$, this dataset is not likely considered high dimensional, but sits on the edge of where classical methods and their resulting inferences may be questionable. Additionally, the use of a model that produces sparsity is beneficial \dots

Figure~\ref{Fig:comparison_data_whoari} provides the confidence intervals and corresponding point estimates (which are set to be the same across all three methods). It is important to emphasize that the range of the x-axis is changing across the three methods. The Hybrid produces the narrowest intervals and SelInf produces by far the widest. Despite the difference in widths, Hybrid and BLP share similar patterns, however, the conclusions that might be drawn could conceivably be quite varied. This may be observed by considering a common point of interest: whether or not an interval includes zero. Intervals from BLP only produce 3 intervals that do not contain zero, SelInf produces 13, and Hybrid produces 21.

This is not to say that Hybrid is more favorable because it produces more intervals where a given predictor might be deemed to be ``significant''. Indeed, we obviously do not know the truth, however, \logan{not sure how presumptuous I want to be here}.

It should be emphasized that SelInf selects (and provides intervals for) 37 of the 66 variables and only produces one interval here with an infinite bound and that is for \texttt{abb}. That said, there are a number of intervals that are unreasonably wide. In general, we believe for these results it would be difficult to provide a convincing interpretation for the intervals produced by SelInf.

\begin{figure}[hbtp]
  \includegraphics[width=\linewidth]{comparison_data}
  \caption{\label{Fig:comparison_data_whoari} Caption goes here}
\end{figure}

\subsubsection{Application to Gene expression in the mammalian eye (Scheetz2006)}\label{Sec:Scheetz2006}

We conclude the results section by considering the intervals produced by the Hybrid bootstrap applied to two datasets: \texttt{whoari} (World Health Organization study on acute respiratory illnesses) and \texttt{Scheetz2006} (Gene expression in the mammalian eye). Additionally we compare the intervals produced by the Hybrid bootstrap to those of BLP and SI.

It should be noted that these two datasets represent two extremes in terms of dimensionality. \texttt{whoari} contains 816 observations and 66 features. However, at the other end, \texttt{Scheetz2006} is certainly a high dimensional dataset, containing just 120 observations but with 18975 features.


In the case where p is large relative to n, SelInf's difficulties are amplified as seen in the simulation. However, applied to \texttt{Sheetz2006}, where $p > 100n$, the issues are unignorable. SelInf provides intervals for 66 of the 18975 features however, every single one of them has a lower or upper bound that is infinite. Additionally, for the bounds that are finite, they are all also extremely large compared to their respective point estimates and in comparison to the intervals produced by Hybrid and BLP. None of the intervals contains zero, either. Potentially even more odd is that of the 66 intervals, 62 of them were completely of the opposite sign as the corresponding estimate. 

Again, here we see that Hybrid and BLP produce similar intervals with the characteristic differences intact. Specifically, the intervals of Hybrid are generally narrower and pulled in closer to zero. From the examples provided in Figure~\ref{Fig:comparison_data_scheetz}, the intervals produced by BLP may appear more desirable. With this very high dimensional dataset, Hybrid produces intervals that are noticeably drawn in towards zero, likely due to the larger penalty selected in this setting. As such, a couple of the intervals produced by Hybrid exclude their corresponding point estimate. 

Despite these differences, depending on the perspective taken, there is not a large discrepancy for the variables deemed significant by the two methods. BLP produces 6 intervals which exclude zero, while Hybrid produces 3. Yes, this is twice as many, but in the grand scheme of nearly twenty-thousand variables, this is a relatively minor difference. Both methods have intervals not containing zero for \texttt{1389910\_at}, \texttt{1378319\_at}, and \texttt{1385395\_at}, while BLP produces three additional intervals which do not contain zero.

That said, we must be extremely clear that a user is not able to actually obtain these results from the implementation in \texttt{hdi}.

For the comparison of these three methods, we want to put a clear emphasis on how the intervals relate to the corresponding estimates from the lasso fit using a selected value of $\lambda$. This is not an issue for Hybrid or SI, but does require a slight adjustment to the default arguments for BLP. This slight adjustment is to set \texttt{boot.shortcut = TRUE}. From hdi's documentation, if \texttt{boot.shortcut = TRUE}, ``the lasso is not re-tuned for each bootstrap iteration, but it uses the tuning parameter computed on the original data instead.'' This in and of itself is not concerning, however, when \texttt{boot.shortcut = TRUE}, it is not clear how the value of $\lambda$ used is determined and the package provides no way for the user to set $\lambda$ themselves. Upon digging into the code, however, one can find that the $\lambda$ used corresponds to the 1SE solution from \texttt{cv.glmnet}. On the other hand, SI and our proposed method, Hybrid, allow for any value of $\lambda$ to be specified. For these two methods then, the $\lambda$ value used for the data analyses corresponds to the $\lambda$ which minimizes CVE.

\begin{figure}[hbtp]
  \includegraphics[width=\linewidth]{comparison_data_scheetz}
  \caption{\label{Fig:comparison_data_scheetz} Caption goes here}
\end{figure}

The 1SE $\lambda$ usually results in a value noticeably larger than the value which minimizes CVE. This is of particular importance because while it produces more sparsity, it also introduces a greater amount of bias. BLP is also called the ``desparsified Lasso.'' This seems counter intuitive to introduce more sparsity to subsequently de-sparsify. This also means that if one were to take the corresponding estimates for the $\lambda$ value selected, the discrepancy between the estimates and the resultant CIs are quite noticeable as seen in both analyses presented here. That said, it should not be ignored that BLP does provide its own estimates which do correspond well to the intervals provided. However, the connection to the the lasso is obscured in the process.

\section{Discussion}

\subsection{Space Requirements}

As implemented, a clear limitation of this method is that it takes a numeric matrix size $B \times p$. With $B = 1000$, the sample matrix gets large enough to cause memory concerns even when $p$ is on the order of $1e5$. For many datasets, this is likely not of concern. However, given that lasso is often used for datasets where $p$ is large, it is clearly not an edge case where $p$ is of this or larger order. One could reduce the size of the sample matrix by reducing the number of draws, but this is unsatisfactory and produces little additional leeway for what seems like a unacceptable sacrifice. This is an ongoing are of interest and a valuable areas of research for any high dimensional bootstrap methods. One solution would be using incremental quantile estimation such as the method introduced by \cite{Tierney1983}. An alternative option would be deriving a method with similar properties but which relies on samples statistics that are relatively straightforward to update over sequential samples (i.e. mean and variance).

\subsection{Remember the Name}

Since the main method suggested here falls somewhere between the Traditional Bootstrap and the Posterior Bootstrap, we propose the full name as the Posterior Adjusted Traditional Hybrid (PATH) Bootstrap.

\section*{Acknowledgments}

\section*{Appendix}

\subsection{True lambda / sigma}

\begin{figure}[hbtp]
  \includegraphics[width=\linewidth]{true_lambda}
  \caption{\label{Fig:true_lambda} Caption goes here}
\end{figure}

\section*{Supplement}

\subsection{Supplement A: Sampling from the Full Conditional Posterior}\label{Sup:A}

\logan{Discuss notation}

Recall, the lasso can be formulated as a Bayesian regression model with the prior $\p(\bb) = \prod_{j = 1}^{p} \frac{\gamma}{2}\exp(-\gamma \abs{\beta_j})$ for $\gamma > 0$ and with $\lam = \gamma \frac{\sigma^2}{n}$. We also saw the likelihood (written here in terms of the partial residuals) is proportional to $\exp(-\frac{n}{2\sigma^2} (\beta_j^2 - 2z_j\beta_j))$ where $z_{j} = \frac{1}{n} \x_{j}^{T}\r_{-j}$.  With this the form of the full conditional posterior can be worked out as follows:
\as{
\Rightarrow P(\beta_j | \r_{-j}) &\propto \exp(-\frac{n}{2\sigma^2} (\beta_j^2 - 2z_{j}\beta_j)) \frac{n \lambda}{2 \sigma^2} \exp(-\frac{n \lambda} {\sigma^2} \abs{\beta_j}) \\
&\propto \exp(-\frac{n}{2\sigma^2} (\beta_j^2 - 2 z_j\beta_j +  2 \lambda \abs{\beta_j})) \\
&= \exp(-\frac{n}{2\sigma^2} (\beta_j^2 - 2(z_j\beta_j - \lambda \abs{\beta_j}))) \\
&=
\begin{cases}
\exp(-\frac{n}{2\sigma^2} (\beta_j^2 - 2(z_j + \lambda)\beta_j)), \text{ if } \beta_j < 0, \\
\exp(-\frac{n}{2\sigma^2} (\beta_j^2 - 2(z_j - \lambda)\beta_j )), \text{ if } \beta_j \geq 0 \\
\end{cases} \\
&\propto
\begin{cases}
C_{-} \exp(-\frac{n}{2\sigma^2} (\beta_j - (z_j + \lambda))^2), \text{ if } \beta_j < 0, \\
C_{+} \exp(-\frac{n}{2\sigma^2} (\beta_j - (z_j - \lambda))^2), \text{ if } \beta_j \geq 0 \\
\end{cases}
}
where, $C_{-} = \exp(\frac{z_j \lambda n}{\sigma^2})$ and $C_{+} = \exp(-\frac{z_j \lambda n}{\sigma^2})$.

At this point, te reader likely notices that the piecewise defined full conditional posterior is made up of a kernel of two normal distributions. This can be leveraged and draws can be efficiently obtained from through a mapping onto the respective normal distributions. To define this mapping, it helps to introduce a concept and some notation. First, the use of ``tails'' in this supplement refers to the entirety the a distribution between zero and $\pm \infty$. That is, the lower tail is any part of the distribution below zero and the upper tail is any part greater than zero and $P(X \in lower \cup X \in upper) = 1$. Accordingly, we will let the tail probabilities in each of the two normals to transformed on to be denoted $Pr_{-}$ and $Pr_{+}$ respectively and the probability in each of the tails of the posterior, denoted $Post_{-}$ and $Post_{+}$ respectively. $Pr_{\pm}$ is trivial to compute with any statistical software. $Post_{\pm}$ is conceptually simple, although care must be taken to avoid numerical instability as n increases. Now, as noted,
\as{
P(\beta_j | \r_{-j})  & \propto
\begin{cases}
C_{-} Pr_{-}, \text{ if } \beta_j < 0, \\
C_{+} Pr_{+}, \text{ if } \beta_j \geq 0\\
\end{cases}
} which implies that $Post_- = \frac{C_{-} Pr_{-}}{C_{-} Pr_{-} + C_{+} Pr_{+}}$ and similarly for $Post_+$. However, to avoid numerical instability, or at least handle it properly when it is unavoidable, we need to work on the $\log$ scale. This works well for most of the problem, but computation of $Post_-$ and $Post_+$ need something a bit more since, for example, $\log(Post_-) = \log(C_{-}Pr_{-}) - \log(C_{-} Pr_{-} + C_{+} Pr_{+})$. That is, the denominator still must be computed then the $\log$ taken which does not allow operation on the $\log$ scale to fully address potential instability. Instead, $\log(Post_-)$ can be computed with $\log(C_-Pr_-) -  \log(C_+Pr_+) - \log(1 + \exp(\log(C_-Pr_-) -  \log(C_+Pr_+)))$. This still doesn't completely address the issue, however, if $\exp(\log(C_-Pr_-) -  \log(C_+Pr_+))$ is infinite then $C_-Pr_- >> C_+Pr_+$ and $\log(Post_-) \approx 0$.

With these values, we can compute quantiles by mapping the corresponding probabilities $p$ for the posterior onto the probabilities $p^*$ for the corresponding normals. Which normal the quantiles of interest ultimately come from is determined based on $Post_{\pm}$. For example, if $Post_{+} = 0.98$ and $p = 0.1$ the $p$ would be mapped onto the positive normal. As one more example, say $Post_{+} = 0.4$ and $p = 0.5$, then $p$ would be mapped onto the negative normal. The transformation to map a given probability from the posterior depends on which tail the quantile resides in on the posterior (equivalently which normal it is being mapped to, the positive or negative). This map is simply:

\as{
p^* &= p \times (Pr_{\pm} / Post_{\pm}) \\
}


Once the respective probabilities are mapped, one can simply use the inverses of the normal CDFs that the probabilities were mapped to. That being said, there is a nuance worth pointing out. When transforming the probabilities, the step to determine which tail the respective quantile comes from occurs first. With this, the probability should be adjusted so that it refers to the probability between the quantile of interest and the respective tail. After this, then the transformation can be applied. With that, obtaining draws from the full conditional posterior can be summarized as follows (written for a single $\beta$ for simplicity):

\begin{enumerate}
  \item Select $\lambda$, fit lasso and obtain estimates corresponding to $\lambda$, estimate $\sigma^2$
	\item Obtain the partial residuals, $\r_{-j}$, and compute $z_j$
	\item Compute $Pr_{-}$ = $\Phi(0, z_j + \lam, \frac{\sh^2}{n})$ and $Pr_{+}$ = $1 - \Phi(0, z_j - \lam, \frac{\sh^2}{n})$
	\item Compute $Post_-$ and $Post_+$ as detailed above
	\item Obtain the quantile $(q)$ corresponding to the given probability $(p)$ of interest:
  \begin{algorithmic}
    \If {$p \leq Post_{-}$}
      \State $q = \Phi^{-1}(p(Pr_{-} / Post_{-}), z_j + \lam, \frac{\sh^2}{n})$
    \Else
        \State $q = \Phi^{-1}(1 - (1 - p)(Pr_{+} / Post_{+}), z_j - \lam, \frac{\sh^2}{n})$
    \EndIf
  \end{algorithmic}
\end{enumerate}
  