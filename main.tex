\section{Introduction}

\pb{I suggest the organization: \\
  Lasso is great (p1 is fine) \\
  Inference is hard (p2 should mention some stuff like knockoff filter, gaussian mirror) \logan{Focus on selection uncertainty} \\
  \logan{Inference about individual variables has proven to be challenging?} \\
  What about bootstrapping? \\
  Main idea of our paper}

\pb{we probably also need some boilerplate stuff here: what is x, what is beta, we're using linear regression, etc.}

Since its introduction by \cite{Tibshirani1996}, the lasso (Least Absolute Shrinkage and Selection Operator) has been a popular model of choice, even in cases which would not necessarily be classified as high-dimensional. The lasso is a penalized regression approach and while the penalty is applicable to broader scenarios, in this manuscript, we put our attention on the implementation of lasso for linear regression which has the following objective function \logan{(not sure about this as final notation yet)}: $$Q(\bb|\X,\y,\lambda) = \frac{1}{2n}||\y - \X\bb||_2^2 + \lambda \sum_{j = 1}^p |\beta_j|,$$ where $\y$ is a length $n$ vector of independent continuous outcome variables, $\X$ is an $n \times p$ matrix of independent explanatory variables, $\bb$ is a length $p$ vector of regression coefficients, and $\lambda$ is a regularization parameter determining the amount of penalization. Note that the objective function involves the addition of the $L_1$ penalty, $\lambda||\bb||_1 = \lambda \sum_{j = 1}^p |\beta_j|$, to the squared error loss, which usually results in sparse estimates for the regression coefficients, depending on the choice of the regularization parameter $\lambda$. Its ability to both do variable selection and estimation lends itself to a particular ease of use. Additionally, in scenarios where both predictive accuracy and interpretability are desired, the lasso generally excels. This is particularly useful for models with a large number of covariates where the underlying model is believed to be sparse but where the exact subset of significant predictors is unknown. In such scenarios, the lasso helps to identify the most relevant variables leading to a more parsimonious and hence interpretable model. \cite{HTF2009} provides an overview of the lasso's performance in various settings, demonstrating its optimality in various examples in which the situations and results desired are similar to those previously mentioned. 

With that said, inference for the lasso has proven to be a difficult task. Although the $L_1$ penalty introduces sparsity in the estimates (i.e. selection), it is also responsible for inducing bias and complicating the sampling distribution of the estimators, which poses the primary obstacle to obtaining valid inference. Given this complexity, it should be no surprise that a number of angles have been taken, producing varying techniques towards inference. Focusing on a less granular level are methods such as those that control the false discovery rate (FDR). Examples of such approaches include the Covariance test proposed by \cite{Lockhart2014} and the Knockoff Filter with both a fixed and random (model-X) knockoffs, proposed by \cite{Candes2015} and \cite{Candes2018} respectively. \cite{Breheny2019} adjusts the idea of FDR and proposes a marginal approach. Another interesting method is the Gaussian Mirror introduced by \cite{Xing2023} in which pairs of ``mirror'' variables are constructed for each covariate through the addition of Gaussian noise and then used to construct a test statistic which can effectively be used to control FDR.

There has also been recent development on performing inference for individual features, often with the goal of producing confidence intervals. Here, the effect of bias is especially noticeable. Some methods, like the Low Dimension Projection Estimator (LDPE) proposed by \cite{ZhangZhang2014} focus on debiasing the original point estimates from a lasso fit to facilitate more traditional forms of inference. An approach which does not focus on debiasing, but instead aims to account for the uncertainty in model selection by conditioning on the selected model was introduced by \cite{LeeEtAl2016} and was appropriately named post-selection or selective inference. However, it should be noted that due to the fully conditional nature of selective inference that is conservative in high dimensions and only provides inference for covariates in the active set.

In scenarios such as this where the sampling distributions of estimators are complex, the bootstrap is often useful. \cite{Dezeure2017} extends the work of \cite{ZhangZhang2014} by proposing to bootstrap the de-biased lasso. Of course, this application of the bootstrap is not applied directly the the lasso itself, but instead, the de-biased (or de-sparsified) lasso, providing intervals that are debiased. The fact that the bootstrap is not applied to the lasso directly is for good reason as \cite{Chatterjee2010} showed that even if we had $\sqrt{n}$-consistency for the lasso (i.e. a small enough $\lambda$) that the bootstrap does not work, at least in the traditional sense. That said, the bootstrap distributions are generally similar to the posterior distribution if the penalty is translated into a corresponding prior and a Bayesian model is fit.

Thus, we take a different perspective and raise a new question to traditional wisdom: does the bootstrap not work or does it just require a shift in perspective on the properties of confidence intervals in high dimensional settings? We find that it's a combination of both. There does need to be a methodological fix, but also we need to rethink high dimensional confidence intervals (HDCIs). Accordingly, in this paper, we propose a slight modification to both the process of obtaining bootstrap draws and what is considered to be proper coverage for HDCIs, both of which borrow from Bayesian approaches. Section 2 examines the two underlying concepts in more detail and Section 3 introduces a few alternative methodological fixes. Then, Section 4 examines performance of the proposed methods across a number of simulations and includes a comparison to two previously mentioned HDCI alternatives, selective inference and the bootstrapped de-sparsified lasso. The results early in Section 4 also narrow down focus to one of the proposed methods, the Hybrid bootstrap. Lastly, in Section 5, we show the application of the Hybrid bootstrap to two data sets, one for acute respiratory illness and the other for gene expression data in mammalian eyes.

\section{Issues with Bootstrapping the lasso}
\label{Sec:Difficulties}

\pb{This is way jumping the gun. The point of this section is: why doesn't the bootstrap CI work (in the traditional sense)? Any discussion of ``faithful to the model fit'' has to come much later; it makes no sense to talk about it yet.\\
  \\
  I don't like ``bias tradeoff'' as a name for this issue. I feel like the issue is more with respect to: What coverage are we interested? Each interval has to have exactly 95\% coverage no matter what $\beta$ is? Or the \emph{collection} of intervals has to have 95\% coverage over a \emph{distribution of likely values of} $\beta$. Bias is kind of related to this, I guess, but the \emph{inherent} issue is not bias but the question of looking at a single interval in isolation or looking at a collection of intervals together.\\
  \\
  I think the theorem should go here -- it clearly illustrates what we're talking about.\\
  \\
  Also, I think we should cover epsilon conundrum first: it's a very straightforward problem, easily explained, and fixable. Let's tackle this first and the thorny philosophical issues second.}

We see the shortcomings of bootstrapping the lasso to construct CIs arising from two issues. We will refer to these issues as,

\begin{enumerate}
\item the Epsilon Conundrum (EC), and
\item Interval Coverage Consistency (ICC).
\end{enumerate}

The Epsilon Conundrum is a side effect of the desired behavior introduced by the $L_1$ penalty: sparsity. When bootstrapping the lasso model without additional adjustments, the EC rears its head in a disturbing manifestation. In a traditional setup some of the resulting confidence intervals will likely have one or both endpoints exactly equal to zero. Specifically, if a quantile based confidence interval is used, an interval of [0,0] will result if a given variable is included in the active set for less than $B(1 - \alpha)$ bootstrap replications. As the dimensionality of the problem grows, this becomes an increasing occurrence leading to a large majority of intervals possessing a length of zero (assuming $\lambda$ is not set to be some small value). When the true value of the coefficient is zero, the interval at least contains the truth. However, this issue is particularly troublesome when one considers what happens when the true value is not precisely zero, but rather small and near zero, as is likely the case in most reasonable scenarios. In fact, we might expect a large majority of variables to fall in this range in high dimensional settings. By just shifting the true value away from zero by $\eps$, an immediate drop in coverage would occur, hence the name: the Epsilon Conundrum. Figure~\ref{Fig:traditional}, shows that this is exactly what happens when a traditional bootstrapping approach is applied to the lasso. In this example, 100 datasets were generated, each with 94 coefficients randomly set to be about $\pm$ 3e-10 with the other 6 being relatively large in comparison. The plot provides 20 CIs as examples, corresponding point estimates, and the true values (red). Across all 100 datasets, the average coverage was 26.5\% which is far below the nominal coverage rate which was set to be 80\%. There are likely several conceivable way to address this issue. The novel approach we propose in Section~\ref{Sec:methods} to producing bootstrap based confidence intervals for the lasso provides one solution.

\begin{figure}[hbtp]
  \begin{center}
    \includegraphics[width=0.6\linewidth]{traditional}
    \caption{\label{Fig:traditional} 100 datasets were generated under independence with 94 of the coefficients having true values randomly assigned to be approximately $\pm$ 3e-10. The other 6 coefficients were larger in comparison ($\approx \pm 0.62, 0.31, 0.15$). The true values are unrounded and are set s.t. SNR = 1 under the restriction that $\sigma^2$ = 1. The plot provides a subset of 20 of the intervals produced by applying a traditional bootstrapping procedure to the lasso for a single dataset, randomly selected among the 100 generated. The points indicate point estimates from the lasso for the corresponding intervals and the red circles indicate the true values. The average coverage across all 100 datasets was 26.5\%. Nominal coverage was set to be 80\%.}
  \end{center}
  \end{figure}

Interval Coverage Consistency, on the other hand is less of an issue to be directly solved and more of a philosophical question. In short, ICC is concerned with if coverage is consistent with the nominal coverage rate for each interval or for the collection of intervals. In the classical frequentest sense, each interval must have exactly nominal coverage, regardless of the true value of $\beta$. That is for any value of $\beta$, $\int I(\beta \in \boldsymbol{A} | \boldsymbol{y}) p(\boldsymbol{y} | \beta)d\boldsymbol{y}$ = 1 - $\alpha$. In this sense, the coverage behavior of interest is more or less isolated for each individual interval. However, in the context of regression, we rarely consider a single covariate, but instead have many covariates with likely a range of true coefficient values. This brings up an alternative, more permissive, way to consider coverage. The alternative would be to consider the average coverage for the collection of intervals over a distribution of the likely values of $\beta$. This is the idea covered in the following theorem.

\begin{thm}
  \label{Thm:bcc}
  If the likelihood is correctly specified according to the true data generating mechanism, $p(\boldsymbol{y} | \bt)$, then credible sets obtained from $p(\bt|\boldsymbol{y})$ will have proper coverage when averaged over the distribution of likely values of $\theta$ if the prior distribution, $p(\bt)$, is correctly specified according to the distribution of $\theta$ values.
\end{thm}

\begin{proof}
Assume the likelihood and prior are correctly specified according the their respective distributions. By definition, a $100(1-\alpha)\%$ credible region for $\boldsymbol{\theta}$ is any set $\boldsymbol{A}$ s.t. $p(\bt \in \boldsymbol{A} | \boldsymbol{y}) \geq 1 - \alpha$. For a given $\bt_0$, the coverage probability can be defined as $\int I(\bt_0 \in \boldsymbol{A} | \boldsymbol{y}) p(\boldsymbol{y} | \bt_0)d\boldsymbol{y}$. Then, averaged over $p(\bt)$, the average coverage probability can be defined as

\as{
  \begin{aligned}
  \int \int I(\bt \in \boldsymbol{A} | \boldsymbol{y}) p(\boldsymbol{y} | \bt)d\boldsymbol{y}p(\bt)d\bt &= \int \int I(\bt \in \boldsymbol{A} | \boldsymbol{y}) p(\boldsymbol{y} | \bt)p(\bt)d\boldsymbol{y}d\bt \\
  &=  \int \int I(\bt \in \boldsymbol{A} | \boldsymbol{y}) p(\bt|\boldsymbol{y})p(\boldsymbol{y})d\boldsymbol{y}d\bt^* \\
  &=  \int \int I(\bt \in \boldsymbol{A} | \boldsymbol{y}) p(\bt|\boldsymbol{y})d\bt p(\boldsymbol{y})d\boldsymbol{y} \\
  &=  \int \int_{\boldsymbol{A}} p(\bt|\boldsymbol{y})d\bt p(\boldsymbol{y})d\boldsymbol{y} \\
  &\geq (1-\alpha)  \int p(\boldsymbol{y})d\boldsymbol{y} \\
  &= 1-\alpha
  \end{aligned}
}

*This step is only valid if the assumptions are met.
\end{proof}
The idea then being that to maintain proper coverage, individual intervals can deviate from the nominal coverage rate, but averaged over the distribution of likely values for $\beta$, the coverage must be greater than or equal to the nominal coverage rate. That is, the coverage rates for some values of $\beta$ may be higher than nominal coverage rates while others are lower, but this is fine as long as they are appropriately balanced. We argue that this is a reasonable perspective on coverage rates for confidence intervals arising from penalized regression models applied to high dimensional datasets. In such scenarios, p is usually large and the estimators are almost necessarily biased. p being large allows for more reliable average coverage while the effect of bias is a a driving force behind the need for a more inclusive definition of coverage.

Along with solving the issue of the Epsilon Conundrum, the Hybrid interval proposed in Section~\ref{Sec:methods} \logan{(is this okay? I think I put emphasis on hybrid a few times like this.)} of this manuscript has a Bayesian feel to it in the sense that it obeys the proposed collective definition of coverage. Alternatively, an attempt could be made to debias the intervals in order to meet the traditional frequentest definition of coverage. This proposition of a new definition of coverage is not to say that attempts at debiasing are unwarranted, indeed, meeting the traditional definition of coverage also means that this relaxed definition is satisfied. However, intervals that do not attempt to meet the rigid definition for coverage, using methods such as debiasing, are arguably more faithful to the model fit, an idea we will revisit later. 


\section{Lasso Bootstrap Confidence Intervals}\label{Sec:methods}

In this section, we present four alternative methods for obtaining bootstrap draws. Let $b \in \lbrace 1, \ldots, B \rbrace$ indicate the $b^{th}$ bootstrap draw.

\begin{itemize}
\item \textbf{Traditional bootstrap:} Simply takes the point estimate, $\hat{\beta}_j^b$,  for each $\beta_j$ for each respective bootstrap sample.
\item \textbf{Posterior bootstrap:} Leverages the Bayesian formulation of the model. For each bootstrap sample, a random draw from the full conditional posterior is obtained instead of taking the point estimate, which for the lasso is equivalent to the mode of the posterior.
\item \textbf{Hybrid bootstrap:} Takes a middle ground between the posterior bootstrap and the traditional bootstrap. The hybrid bootstrap only samples from the full conditional if the point estimate for a given $\beta_j$ is equal to zero for that bootstrap sample. Otherwise, takes the point estimate like the traditional bootstrap.
\item \textbf{Debiased bootstrap:} Uses the correlation between $\boldsymbol{x}^b_j$ and the partial residuals obtained from the model for each bootstrap sample as the bootstrap draw for variable j.
\end{itemize}

For each method, the main idea is given above. The remainder of this section presents their specific application to lasso-penalized linear regression. Section \ref{Sec:full-cond} defines and derives the full conditional distributions needed for the sampling in the posterior and hybrid methods. Explicit calculations for all four methods are then given in Section~\ref{Sec:implementation}.

\subsection{Full conditional distributions for the lasso}
\label{Sec:full-cond}

\pb{I think we should split off the algorithm/implementation into its own section}

In this section, we provide a high level derivation of the full conditional posterior distributions for lasso-penalized regression. Full details, including how to perform sampling, are provided in Supplement~\ref{Sup:A}.

As with other penalized regression models, the lasso can be formulated as a Bayesian regression model by setting an appropriate prior. This was addressed initially by \cite{Tibshirani1996} and covered more extensively by \cite{Park2008}. For the lasso, the corresponding prior is a Laplace distribution, also referred to as the double-exponential distribution: $$\p(\bb) = \prod_{j = 1}^{p} \frac{\gamma}{2}\exp(-\gamma \abs{\beta_j}), \gamma > 0.$$ With this prior, the lasso estimate, $\bbh(\lam)$, is the posterior mode for $\bb$ when $\lam = \gamma \frac{\sigma^2}{n}$.

The full conditional posterior for $\beta_j$ is defined as the distribution for $\beta_j$ conditional on $\bb_{-j}$, $\lambda$, and $\sigma^2$. Further discussion is given to selecting $\lambda$ and estimating $\sigma^2$ at the end of this section. Once $\lambda$ is selected, $\bb_{-j}$ is set equal to $\hat{\bb}_{-j}(\lambda)$ and $\r_{j}$, the partial residuals, are then defined as $\y - \X_{-j}\hat{\bb}_{-j}(\lambda)$. To reduce notational distractions, we only explicitly condition on $\r_{j}$ to denote full conditional distributions. A normal likelihood and Laplace prior are not conjugate. However, the full conditional posterior can be shown to be a mixture of a right and a left truncated normal where the truncation occurs at zero for right and left tails respectively. In this manuscript, we will assume that $\X$ has been standardized s.t. $\x_j^T\x_j = n$. Then, as shown in Supplement~\ref{Sup:A}, for $\beta_j$,

\as{
L(\beta_j | \r_{j}) &\propto \exp(-\frac{n}{2\sigma^2} (\beta_j^2 - 2z_j\beta_j)), \\
\text{ where } z_{j} &= \frac{1}{n} \x_{j}^{T}\r_{j} \text{, and} \\
P(\beta_j | \r_{j}) &\propto
\begin{cases}
C_{-} \exp(-\frac{n}{2\sigma^2} (\beta_j - (z_j + \lambda))^2), \text{ if } \beta_j < 0, \\
C_{+} \exp(-\frac{n}{2\sigma^2} (\beta_j - (z_j - \lambda))^2), \text{ if } \beta_j \geq 0 \\
\end{cases}
}

Where, $C_{-} = \exp(\frac{z_j \lambda n}{\sigma^2})$ and $C_{+} = \exp(-\frac{z_j \lambda n}{\sigma^2})$.

What makes this formulation attractive is that a mapping allows tail probabilities from the posterior to be translated to probabilities onto corresponding known normal distributions (i.e. $\N(z_j \pm \lambda, \frac{\sigma^2}{n})$). This allows for numerically stable and efficient sampling to be obtained from the full conditional posteriors.

It was mentioned previously that this solution corresponds to a particular value of $\lam$ and $\hat{\sigma}^2$. Our recommendation is to use cross validation (CV) to select $\lam$ and produce an estimate for $\sigma^2$ and to produce bootstrap confidence intervals corresponding to these values. Specifically when using CV to select $\lambda$ and estimate $\sigma^2$, we recommend using the value of $\lambda$ which minimizes the cross validation error (CVE) and to use CVE as the estimate for $\sigma^2$. It should be noted that producing an estimate for $\sigma^2$ in this manner implicitly depends on $\lam$, so a new estimate for $\sigma^2$ should be obtained if the value of $\lam$ is changed. Similarly, the partial residuals, calculated with $\hat{\bb}_{-j}(\lambda)$, are also specific to a given value of $\lambda$. As such, when referring to $\r_{j}$ and $\hat{\sigma}^2$ it will always be as a function of $\lam$ which we drop for notational convenience.

Note that we prefer the minimum CVE $\lam$ to the one standard error (1-SE) $\lambda$, which is the largest $\lambda$ value with a CVE within one standard error of the minimum CVE. The 1-SE solution is sometime preferred as it generally provides a more sparse solution. For inference, however, our preference for the minimum CVE $\lam$ is due to the fact that the 1-SE $\lambda$ inherently introduces more bias. As such, performance under the minimum CVE $\lambda$ will be focused on in the results and will be denoted $\lam_{\CV}$

\subsection{Implementation}
\label{Sec:implementation}

\pb{need to rework transition / intro}

With the full conditional posterior now defined for the linear regression lasso, we can provide the algorithm for implementing the four methods. Let $B$ represent the total number of bootstrap datasets generated and let $\boldsymbol{X}^b$ and $\boldsymbol{y}^b$ refer to the $b^{th}$ bootstrap sample. Similarly, let $\hat{\beta}^b_j$ refer to the estimate for $\beta_j$ from the lasso fit to $\boldsymbol{X}^b$ and $\boldsymbol{y}^b$ at a specified value of $\lam$. Then, the methods can be implemented as follows:

\begin{enumerate}
\item Perform CV using the original data to select $\lam$ and estimate $\hat{\sigma}^2$
\item For b $\in \lbrace 1, \ldots, B \rbrace$:
\begin{enumerate}
\item Obtain a pairs bootstrap sample, $\X^b$ and $\y^b$
\item Fit lasso with $\X^b$ and $\y^b$, obtain $\bbh^b$ and $\boldsymbol{z}^b$ corresponding to the selected value of $\lam$ in (a)
\item For j $\in \lbrace 1, \ldots, p \rbrace$:
	\begin{algorithmic}
	\Switch{method}
    \Case{traditional}
      \Assert{$q_j = \bh_j^b$}
    \EndCase
	  \Case{posterior}
    \Assert{
      $p^* = \Unif(0, 1)$ \\
      \hspace{1.95cm} $q_j = P^{-1}(p^*|\r_{j}^b)$
    }
    \EndCase
    \Case{hybrid}
      \Assertif{$\bh_j^b \neq 0$}{$q_j = \bh_j^b$}
      \Assertelse{
        $p^* = \Unif(0, 1)$ \\
        \hspace{1.8cm} $q_j = P^{-1}(p^*|\r_{j}^b)$
      }
    \EndCase
    \Case{debiased}
	    \Assert{$q_j = z_j^b$}
    \EndCase
	\EndSwitch 
	\end{algorithmic}
\item Save $\q$
\end{enumerate}
\item Combine all B $\q$ vectors to obtain a $B \times p$ matrix of bootstrap draws \pb{avoid R-specific language like ``row bind''}
\item For each $\beta_j$, compute the quantiles for $p_L = \alpha/2$ and $p_U = 1 - \alpha/2$ from the $j^{th}$ column of the draws to produce a final confidence interval estimate with significance level $\alpha$.
\end{enumerate}

\pb{short summary remarks?}

The steps defined above are presented primarily for explanatory purposes. They break out each of the four methods into individual cases for clarity, however, in practice, there is a large degree of overlap between methods. All four methods start with a lasso fit on each bootstrapped dataset which provides the draws for the traditional bootstrap. To obtain the full conditional posteriors which are needed for every variable for the posterior method, $z^b_j$ must first be computed which is the draw for debiased. If the draws are obtained for both traditional and posterior, then it is possible to subsequently compute what the draws would be for the hybrid bootstrap. Again, however, in practice this would not necessarily be the most efficient implementation unless one was interested in obtaining intervals from all four methods, but it does provide an illustration as to how the methods are related. Additionally, the draws can be computed more efficiently than one variable at a time and there are other complexities which are outside the scope of this manuscript. The hybrid method is implemented in the R package \texttt{ncvreg} and the source code can be examined for these additional nuances.

\pb{We should briefly mention that we're using pairs bootstrap and quantile intervals as this is a common way of applying the bootstrap. Plenty of attention has been paid to various bootstrap implementations (cite bootstrap book); our focus here is on the differences between the traditional bootstrap and its posterior/hybrid/debiased variants.}

Furthermore, there are various ways to perform the bootstrap and subsequently obtain confidence intervals. A large body of research exists on various bootstrap implementations, and \cite{Efron1994} in \textit{An Introduction to the Bootstrap} provides an accessible overview on the core body of knowledge. In this manuscript we use the pairs bootstrap and quantile intervals as this is one of the common ways of obtaining bootstrap confidence intervals and requires few assumptions. That said, our focus in this manuscript is on the differences between the traditional bootstrap and the proposed variants.

\section{Results}
\label{Sec:results}

We begin by examining the coverage behavior of the proposed bootstrap methods in what might be considered the ``ideal'' scenario, where the assumptions of Theorem~\ref{Thm:bcc} are met (Section~\ref{Sec:Coverage}). We then examine how this accuracy is affected by a number of violations to those assumptions to assess each method's robustness (Section~\ref{Sec:robustness}). Finally, we compare the proposed confidence interval methods to other confidence interval approaches for penalized regression that have been proposed in the literature. \logan{At this point results are really focused on just Hybrid.}

Unless otherwise noted, the nominal coverage rate in all of these experiments is 80\%.

We compare the proposed CI methods by showing the coverage of each, both overall and as the magnitude of $\beta$ changes, then present results on the width and bias of the intervals as a way of gaining insight into why the methods result in over or under coverage at various magnitudes of $\beta$.

\subsection{Coverage}\label{Sec:Coverage}

As suggested by Theorem~\ref{Thm:bcc}, there is reason to expect that the proposed methods (if they behave like Bayesian intervals) have proper average coverage when the penalty (prior) matches the true distribution of the coefficients. For a lasso model, this occurs when each $\beta_j$ independently follows a Laplace (double exponential) distribution. For the simulations in this section, we scaled the coefficients after drawing them so that $\bb^T\bb = 1$. With $\sigma^2=1$ and independent features, this results in a signal-to-noise ratio (SNR) of 1.

We generated 100 independent data sets; for each data set, $B = 1000$ bootstrap iterations were drawn for each of the four methods described in Section~\ref{Sec:methods}. Each dataset was simulated as follows. $\X$ was generated independently from a $N(0, 1)$ with $n = p = 100$. Then, $\y$ was generated as $\y = \X\bb + \bvep$, where $\veps_i \iid N(0, 1)$. The results are shown in Figure~\ref{Fig:laplace}, where the dotted lines represent the average coverage for each method across all coefficients, while the solid lines are smoothed estimates of coverage as a function of $|\beta|$. The black line indicates the nominal coverage rate, which is set to be 80\%.

\begin{figure}[hbtp]
  \begin{center}
  \includegraphics[width=0.65\linewidth]{laplace}
  \caption{\label{Fig:laplace} Results are from the simulation described in Section~\ref{Sec:Coverage}. The fitted curves are from Binomial GAMMs (one for each method) fit with coverage being modeled as a smooth function of $|\beta|$ and with a random intercept on dataset to account for deviations in coverage specific to a given randomly generated dataset. The data used for modeling contains a row for each variable per simulated dataset. The dashed lines represent the average for each method across all 100 independently generated datasets and the solid black line indicates the nominal coverage rate. The shaded distribution in the background depicts the positive tail of the distribution the $\beta$s were drawn from, a Laplace.}
  \end{center}
\end{figure}

\pb{NO -- START WITH THE MOST IMPORTANT THING, NOT A MINOR TANGENT. Before describing the observed coverage behavior, we draw brief attention to Figure~\ref{Fig:laplace} as a depiction that Hybrid is a mixture of Posterior and Traditional, being more like Posterior when $\beta$ is small in magnitude and increasingly more like Traditional as $\beta$ increases in magnitude.}

Figure~\ref{Fig:laplace} shows that, as is generally the case when applied to the lasso, the traditional bootstrap coverage is far below the nominal 80\%. At the other extreme, the average coverage of the posterior bootstrap was above 80\%. Meanwhile, the hybrid and debiased bootstrap methods had coverage fairly close to the nominal 80\%.

\pb{combine and shorten these two paragraphs}

To understand the coverage behaviors further, it helps to consider what happens when $\abs{\beta}$ is small and when it is large. The traditional bootstrap has low coverage both near and far-from zero. Debiased also under covers both near and far-from zero but to a much lesser extent and, of the methods considered here, generally sticks closest to nominal coverage regardless of the magnitude of $\beta$.  Posterior and Hybrid, on the other hand, display a novel decreasing trend with high coverage rates high for values of $\beta$ near zero and lower coverage rates for values of $\beta$ larger in magnitude. This pattern is typical for these methods and is to be expected from intervals arising from procedures that do not attempt to debias. The introduction of bias through penalization, a feature of the lasso, can largely explain the pattern observed. In fact, the effect of penalization on the width and bias of the intervals can explain the coverage patterns seen for all four methods, which is where we now turn our attention to.

\pb{we have too many figures. can we combine the two figures in this section to a 2x3 fig? or perhaps get rid of the ``interval bias'' and ``towards - away'' figures?}

\begin{figure}[hbtp]
  \begin{center}
  \includegraphics[width=0.65\linewidth]{laplace_width_bias}
  \caption{\label{Fig:laplace_width_bias} Results are from the simulation described in Section~\ref{Sec:Coverage}. The fitted curves are from GAMMs (one for each method-feature combination) fit with each respective feature modeled as a smooth function of $|\beta|$. The data used for modeling contains a row for each variable per simulated dataset. Additionally, the models contain a random intercept on dataset to account for deviations in the feature specific to a given randomly generated dataset. Miss away is defined as having both interval endpoints being larger in magnitude and of the same sign as the truth. A miss towards is defined as having at least one end point between the truth and zero. A Type 3 error is defined as having both interval endpoints be a different sign than the truth, unless the both endpoints are zero in which case it is defined as a miss towards zero. Black lines at P(Event) = 0.1 in the bottom two plots indicate the optimal value.}
  \end{center}
\end{figure}

\logan{Still trying to decide if I want T3 alone, or just put it in a supplement?}

The fitted curves in Figure~\ref{Fig:laplace_width_bias} are constructed in the same manner as for Figure~\ref{Fig:laplace}, albeit on the respective features of interest. This plot displays interval width (upper left) and three measures of interval bias (bottom, upper right). The top plot of Figure~\ref{Fig:laplace_width_bias} shows that the interval width tends to increase as $\beta$ increases in magnitude. This behavior is related to the proportion of times a variable is selected to be in the model. Consider for a moment the draws for the traditional bootstrap. When a given $\beta$ is near zero, its estimate will often be shrunk to zero which results in less variable bootstrap draws than for a variable which is always selected. That is, a variable with a larger corresponding true value of $\beta$ will have a wider range of plausible estimates for each bootstrap draw than for a variable with a true value of $\beta$ nearer to zero, all else equal. This effect is most notable for the Traditional bootstrap, as it is the only method which will result in a draw exactly equal to zero when $\bh^b_j = 0$. The three other methods all introduce some additional variability in this scenario. This explains why Traditional produces much narrower intervals, especially for values of $\beta$ smaller in magnitude, and largely explains the drastic under coverage observed previously. This is a manifestation of the Epsilon Conundrum as Traditional has a tendency to produce the intervals with endpoints equal to zero for $\beta$s near zero. The plot of widths also helps display the underlying mechanism of over coverage observed for Posterior. We can see that compared to the other methods, the intervals for Posterior are significantly wider, regardless of the magnitude of $\beta$. The resulting over coverage suggests then that the intervals are too wide. Lastly note that, compared to the other three methods, the Hybrid bootstrap produces fairly consistent width confidence intervals, regardless of the magnitude of $\beta$.

\logan{Need to clean up, I started using miss towards zero to include Type 3 as short hand but then added Type 3 back in. Need to readjust... but in a way that isn't even more cumbersom.}

The bottom plots depict the probability that an interval misses the truth either towards or as a Type 3 error (left) or away from zero (right), as a function of the magnitude of $\beta$. If the entire interval was of a different sign than that of the true value (unless both endpoints were 0), we defined this as a Type 3 error, which is also presented alone in the top right plot. The black lines at 0.1 on both bottom plots indicate that for an unbiased method with proper coverage, we would expect the intervals to miss equally in either direction with proportion $\alpha / 2 = 0.1$. For the most part, as a result of bias, we see that the intervals miss towards zero which accordingly increases as the magnitude of $\beta$ increases (for all but traditional which suffers regardless of the magnitude of $\beta$). Furthermore, Hybrid and Posterior show a similar pattern of rate of missing towards zero. The pattern is sigmoid in shape with near zero rates of miss towards zero for small values of $\beta$ and increasingly rates of missing toward zero (with some leveling off) as $\beta$ increases in magnitude. Although Hybrid does tend to miss towards zero more than Posterior, this is mostly a result of decreased width. Alternatively, when $|\beta|$ is near zero, debiased has very close to the ideal miss rate, however, for moderately sized values of $\beta$, this higher tendency to miss towards zero compared to Hybrid and Posterior translates into a less than ideal behavior for a method that is supposedly debiased. It isn't until $\beta$ is larger in magnitude that it appears coverage is truly favorable in terms of a lower miss towards zero rate compared to the other methods. Regardless, although debiased does reduce the rate of intervals missing toward zero for larger magnitude $\beta$s, it is unable to correct all the bias, still missing towards zero at a rater higher than ideal. Similarly indicating unresolved bias, all methods miss away from zero with rates less than 0.1, with Debias really being the only method with notable rates of misses away from zero. \logan{Up until this point ``miss towards'' also includes T3.} Additionally, as set up, the probability that a method misses towards zero by definition converges to zero as $\beta \rightarrow 0$ (unless the method can produce bounds that are exactly equal to zero, like traditional). Given that the probability of a miss towards zero goes to zero as $\beta \rightarrow 0$, a Type 3 error near 0.1 for $\beta$ near zero is not necessarily a bad thing. Clearly, traditional has issues, but otherwise, although slightly higher than desired, the Type 3 error rate for Debiased isn't overly concerning and neither Hybrid or Posterior produce any significant rates of Type 3 errors.

\logan{A decent amount of repeated ideas in above and below paragraphs, I can probably reduce further.}

Now that we have a deeper understanding of the intervals' behaviors, we can use them to explain the trends in coverages seen in Figure~\ref{Fig:laplace}. Traditional has the most amount of bias towards zero regardless of the magnitude of $\beta$, and produces narrow intervals. As a result, the Traditional bootstrap under covers regardless of the magnitude of $\beta$. On the opposite end of the extreme, the Posterior bootstrap tends to produce intervals that over cover, mainly due to having intervals that are in general too wide. However, the over coverage does not occur across the range of $\beta$ values. In this sense, the Posterior and the Hybrid share a common pattern. This pattern is explainable through the rate of misses towards zero and the respective widths of the intervals (misses away / T3 errors are negligible for both methods). For values of $\beta$ near zero, the effect of the lasso penalty is minimal while the width is only minorly decreased leading to coverage levels near 1. However, as $|\beta|$, and the average effect of the lasso penalty, increases, this eventually leads to lower coverage for values of $\beta$ larger in magnitude, as width does not increase accordingly (nor would we necessarily want it to) leading to a high rate of misses towards zero. Although they share the same coverage pattern, the main difference between Hybrid and Posterior is that Hybrid produces narrower intervals, as it only samples from the Full Conditional Posterior (the driver of the increased width for Posterior) when $\bh = 0$. This leads to rates of coverage nearer to that of nominal. Lastly, debiased, like the other intervals can have its coverage tendencies explained through misses towards zero and Type 3 errors. Although it is the only of the four methods to have any significant rate of missing away from zero, it is still unable to correct for the bias introduced by penalization leading to under coverage across the range of values of $|\beta|$ and in general to average coverage rates below nominal. 

We hope the reader is exceedingly convinced that the Traditional bootstrap is a poor choice. Additionally, the posterior isn't bad, it just generally provides intervals significantly wider than that of the Hybrid bootstrap. As will be demonstrated throughout the remainder of this manuscript, when the Hybrid departs from nominal coverage, it generally does so in the direction of over-covering. Thus, in these scenarios, the wider intervals of the Posterior are strictly worse than those produced by the Hybrid. The remaining two methods, Hybrid and Debiased, both produce coverage levels that are reasonably near that of nominal. As such, we will give both further consideration. As just noted, for the simple case considered in the first simulation, Debiased does tend to produce coverage near nominal. However, Debiased, approaches nominal (with increasing sample size) from below as the uncorrected bias for larger magnitude $\beta$s slowly diminishes with increasing sample size. Hybrid, on the other hand, tends to approach nominal from above rather than below. This behavior is rather stable and extends to more complex data generating mechanisms, something that we will see does not hold true for Debiased in Section~\ref{Sec:robustness}.

\subsection{Robustness}
\label{Sec:robustness}

This section explores a number of scenarios to help understand the behavior of the Hybrid bootstrap. It begins with a look at the coverage behavior when there is correlation among the predictors and provides a comparison to the behavior of Debiased. The results in this section also give the first depiction of how the behavior changes with increasing sample size. As mentioned previously, and as we will see in Section~\ref{Sec:correlation}, Debiased begins to break down with this introduction of correlation and as such this will conclude further consideration of the Debiased method. Next, we consider how Hybrid performs under various distributions of $\beta$ and follow this up with a brief revisit to the Epsilon Conundrum. This section is rounded out with a look at how the coverage behavior changes with changes in the $\lambda$ used before finishing off with a demonstration that the behavior is consistent across different choices of nominal coverage rates.

\subsubsection{Correlation}
\label{Sec:correlation}

Now, we consider the behavior of both Debiased and Hybrid where the covariates are generated under increasing levels of autoregressive correlation. Other than the addition of correlation, the set up of the simulation for the results displayed in Figure~\ref{Fig:correlation_structure} is the same as for the first simulation described. The boxplots provide the coverages across the 100 simulated datasets for each value of n. The results for Hybrid and Debiased are in the top and bottom row respectively with the amount of correlation starting at $\rho = 0.4$ in the first column and increasing to $0.6$ in the second column and $0.8$ in the third. 

As previously alluded to, even at the lower end of correlations introduced, Debiased noticeably under covers even when $n = 4p = 400$. This is only exacerbated with increasing correlation. This, along with previously observed behavior, points to a lack of robustness for the Debiased bootstrap method. Hybrid, however, fares better in the presence of correlation. It takes the highest amount of autoregressive correlation ($\rho = 0.8$) to produce coverage that is noticeably below nominal coverage. For a moderate amount of correlation ($\rho = 0.4$), there is little impact on the coverage of the intervals (see Figure~\ref{Fig:laplace_comparison} for $\rho = 0$ as a comparison). 

\begin{figure}[hbtp]
  \begin{center}
  \includegraphics[width=0.6\linewidth]{correlation_structure}
  \caption{\label{Fig:correlation_structure} This figure presents results for the simulation described in Section~\ref{Sec:correlation}. The boxplots are for the coverage rates across 100 simulated datasets across two methods, Hybrid and Debiased, and across three different levels of autoregressive correlation among the covariates, $\rho = 0.4, 0.6, \text{ and, } 0.8$. For this simulation, p = 100, and the results for each combination of method and level of correlation are presented across three different sample sizes, n = $\frac{1}{2}$p, p, 4p. The horizontal black line provides reference for the 80\% nominal coverage rate.}
  \end{center}
\end{figure}

Debiased, which admittedly was a naive attempt, has accumulated a number of drawbacks which bring in to question its stability and suggest it should be dropped from further consideration. This hints at an idea that will be explored further in Section~\ref{Sec:Comparison}, debiasing comes at a cost. This cost manifests differently depending on the implementation, but there is one. In Section~\ref{Sec:Comparison}, comparison of Hybrid is done to two alternative methods, both of which attempt to provide constant coverage regardless of the magnitude of $\beta$. Although the cost for one is far more noticeable than it is for the other, both have more variable coverage rates and wider intervals. Often, bias is spoken about in a hushed tone, but we want to emphasize that bias isn't always a bad thing. 

The remainder of the manuscript focuses solely on the Hybrid bootstrap.

\subsubsection{Distribution of Beta}\label{Sec:Distribution}

Now that we have seen that the Hybrid Bootstrap does relatively well in the presence of correlation, we turn our attention to performance under various distributions of $\beta$. Given that we have previously observed that the coverage for a given $\beta$ depends on its magnitude, it may be expected that the coverage will vary wildly depending on how $\bb$ is distributed. Table~\ref{Tab:dist_beta} shows the results of $\bb$ generated under 7 alternative distributions. The first column of the table gives a depiction of the distributions and details of these distributions are provided in the notes for the table. The simulation set up was again the same as the initial simulation discussed, but with the respective generating mechanisms for $\bb$. While it is certainly the case that the distribution affects the coverage, the underlying patterns remain the same. That is, for smaller values of n, Hybrid over covers, but, as n increases, coverage approaches the nominal coverage rate. Additionally, while distributions with most of their mass near zero result in over coverage, the opposite behavior is not observed. That is, even when most of the density is concentrated away from zero, as with the Beta distribution, the desired coverage properties remain intact. Lastly, note, that between n = 400 and n = 1000, there is little change in the coverage rates for distributions that were already near 80\% coverage, and for the others (T, Sparse 1, Sparse 2), although slow, coverage still appears to be decreasing towards nominal.

\begin{table}[hbtp]
  \centering
  \input{tab/distribution_table}
  \caption{\label{Tab:dist_beta} Results are from the simulation described in Section~\ref{Sec:Distribution}. The nominal coverage rate is 80\%. The setup is the same as the previous simulation, except with $\bb$ being generated under 7 alternative (to the laplace) distributions and the addition of a fourth sample size, n = 1000. To maintain the specified SNR of 1, $\bb$ is normalized. Prior to normalization, Sparse 1 had $\bb_{1-10} = \pm(0.5, 0.5, 0.5, 1, 2)$ and $\bb_{11-100} = 0$. Sparse 2 had $\bb_{1-30} \sim N(0, 1)$ with the rest equal to zero.  Sparse 2 had $\bb_{1-50} \sim N(0, 1)$ with the rest equal to zero. All distributions of $\bb$ were centered at zero. For normal, laplace, and uniform, after normalization, the original scale is arbitrary. For the T distribution, df was set to 3 and for the Beta distribution it was generated from Beta(0.1, 0.1) - 0.5. Note that the distribution for Sparse 1 is fixed and that for Sparse 1 and 2 that, like for the Normal, the choice of scale is arbitrary.}
\end{table}

\subsubsection{Epsilon Conundrum}\label{Sec:Epsilon}


Its natural to now consider the performance of the hybrid bootstrap in the scenario that was the main motivator of the method: the Epsilon Conundrum. The simulation set up used to assess the Epsilon Conundrum is essentially just another distribution of $\bb$ values, although one that can prove difficult for methods which fail to take proper precautions. The setup for this simulation was described in Section~\ref{Sec:Difficulties}. As seen in Figure~\ref{Fig:zerosample2}, the Hybrid bootstrap CIs avoid the downfall we previously observed in Figure~\ref{Fig:traditional} when applying the Traditional bootstrap to the same simulation set up. Although only a subset of the intervals are displayed in Figure~\ref{Fig:zerosample2}, it is evident that the intervals avoid a precipitous drop in coverage for values where $\beta_j$ is near zero by producing intervals that are sufficiently wide (and with endpoints not equal to zero), the main effect of sampling from the full conditional posterior when $\hat{\beta}_j = 0$. On the contrary, Hybrid does, however, over cover on average due to the fact that a large majority of $\beta$s are near zero (recalling the tendency of Hybrid to over cover near zero). But, again, this is expected behavior from the Hybrid bootstrap CIs and the important take away of this section is that hybrid bootstrap provides viable intervals for values of $\beta$ near zero.

\logan{Not sure yet what, if anything, to say about the large of values of beta not being covered. That the coverage rate is essentially that of the proportion of near zero values of beta... or if it is okay to leave as is given the results in Section~\ref{Sec:Distribution}?}

\logan{I did also look into what happens if we set sigma2 = 1 (true value) here, which is smaller than was is usually estimated by CV and the results hardly change. Usuaully the intervals are centered enough on zero s.t. a small reduction in the width doesn't majorly impact the coverage.}

\begin{figure}[hbtp]
  \begin{center}
  \includegraphics[width=0.6\linewidth]{zerosample2}
  \caption{\label{Fig:zerosample2} 100 datasets were generated with n = p = 100 with 94 of the coefficients having true values randomly assigned to be approximately $\pm$ 3e-10. The other 6 coefficients were larger in comparison ($\approx \pm 0.62, 0.31, 0.15$). The true values are unrounded and were generated s.t. $\bb^T\bb = 1$. The plot provides an example of 20 of the intervals produced by applying the Hybrid bootstrapping procedure to the lasso for a single dataset, randomly selected among the 100 generated. The points indicate point estimates from the lasso for the corresponding intervals and the red circles indicate the true values. The average coverage across all 100 datasets was 93.6\%. Nominal coverage was set to be 80\%. See Figure~\ref{Fig:traditional} for a comparison to how the Traditional method performs.}
  \end{center}
\end{figure}


\subsubsection{Selection of \texorpdfstring{$\lambda$}{lambda}} \label{Sec:lambda}

Since CV is non-deterministic, $\lambda_{\CV}$ likely takes on a range of values depending on the seed set, so there maybe be question about what happens if the value of $\lambda$ is changed. The data in Figure~\ref{Fig:beta_lambda_heatmap_laplace} comes from a simulation that addresses just that. The data generating mechanism for this simulation is identical to that for Section~\ref{Sec:Coverage}. In this simulation, however, $\lambda$ was evenly distributed on the $\log_{10}$ scale from $\lam_{\max}$ to $\lam_{\min} = \lam_{\max} * 0.001$. At each value, confidence intervals were obtained and coverage was recorded. This was repeated 100 times, then a gam was fit to provide a smooth estimate of the coverage rate by $\lambda$ and $|\beta|$. Relative coverage is defined here as the estimated coverage rate minus the nominal coverage rate. The range of $\lambda$ values displayed in the plot represents the range of $\lambda_{\CV}$ across the 100 simulations, relative to $\lam_{\max}$. The red line indicates the average $\lambda_{\CV}$ while the blue line represents the value of $\lambda$ which provided coverage closest to that of nominal.

\begin{figure}[hbtp]
  \begin{center}
  \includegraphics[width=0.6\linewidth]{beta_lambda_heatmap_laplace}
  \caption{\label{Fig:beta_lambda_heatmap_laplace} The heatmap displays results for the simulation described in Section~\ref{Sec:lambda}. 100 datasets were generated as with previous simulations with n = p = 100, with $\beta \sim Laplace$ and $\X$ generated under independence from a $N(0,1)$. Then, the Hybrid bootstrap was fit across a range of $\lam$ values and confidence intervals and their coverages were obtained and then used to fit a gam to estimate the coverage as a smooth function of the $|\beta|$ and $\lam$. The x-axis was truncated over the range of $\lam_{\CV}$s and is presented relative to each simulations $\lam_{\max}$. The red line represents the relative average CV selected value of $\lam$. The blue line indicates the relative $\lam$ value that provided coverage closest to the 80\% nominal coverage rate.}
  \end{center}
\end{figure}

Depending on the value of $\lam$, $\beta$s closer to zero see moderate amounts of over coverage which decreases, eventually leading to under coverage, as the magnitude of $\beta$ increases. The $|\beta|$ where this transition occurs (and at which coverage of a specific value equals nominal) varies considerably over the range of $\lam$. At $\lam_{\max}$, this transition occurs at a relatively small $|\beta|$. As the value of $\lam$ decreases from $\lam_{\max}$ this transition occurs at increasingly large values of $\beta$ until all values of $\beta$ have an estimated coverage rate at or above that of nominal. To obtain an average coverage rate near that of nominal, $\lam$ needs to be selected such that over coverage for smaller $|\beta|$s and under coverage for larger $|\beta|$s is balanced. The blue line serves as a general representation of where this tradeoff is met. In this scenario, and in general, $\lam_{\CV}$ does a reasonable job at providing such a balance across the distribution of $\beta$ values. However, it should be no surprise by this point that the average $\lam_{\CV}$ (the red line) is to the right of the relative $\lambda$ value that provided closest to nominal coverage (the blue line), as we have seen previously that the $\lambda$ which minimizes CVE tends to produce over coverage when n = p. That is, as $\lam$ is decreased from the blue line, the average coverage increases above nominal as a greater proportion of the distribution of $\bb$ experiences over coverage.

It should be reemphasized that, as implemented, the estimate of $\sigma^2$ depends on the choice of $\lambda$. That said, by definition, the estimates are increasing in either direction of $\lambda_{\CV}$. So, the general pattern previously described is likely more due to the direct effects of $\lambda$ as opposed to an indirect effect of the estimate of $\sigma^2$, at least over the range of $\lam_{\CV}$ and as estimated using CVE. However, the over coverage at $\lam_{\CV}$ is more of a result of the over estimate of $\sigma^2$, see Supplement~\ref{Sup:B}. Briefly, setting $\lam$ equal to its true value (which we know since $\beta \sim Laplace$), while still estimating $\sigma^2$ using CVE but just at the true value of $\lam$, leads to coverage very similar to that when $\lam$ is selected using CV, likely because $\lam_{\CV}$ was generally near the true value of $\lam$ in the simulations considered. In contrast, if $\sigma^2$ is also set to its true value, coverage was very near nominal regardless of the sample size. Thus, alternative methods to estimating $\sigma^2$ could be considered to reduce the degree of over coverage, a topic which is further explored in the Discussion. \logan{Still need to add this assuming we agree this is something we want to allocate space for.}

Finally, the balance of over and under coverage, of course, depends on the true distribution of $\bb$ (recall that here it is Laplace). However, as we saw in Section~\ref{Sec:Distribution}, $\lam_{\CV}$ generally does well regardless of the distribution of $\beta$, with average coverage behavior being similar across various distributions of $\beta$, so a similar pattern would be expected for other distributions.

\subsection{Comparison to Ridge Regression CIs}\label{Sec:Ridge}

We now turn attention to a comparison of the intervals produced by Hybrid to those produced by Ridge regression. In this example again n = p = 100, however, only one $\beta$ is non-zero. That is, $\beta_{A_1} = 1$ and $\beta_{B_1}, \beta_{N_1}, \ldots, \beta_{N_{98}} = 0$. Additionally, the data are simulated such that $\rho(\beta_{A_1}, \beta_{B_1}) = .99$ but all of the N (noise) $\beta$s are uncorrelated with $\beta_{A_1}, \beta_{B_1}$, and each other.

100 datasets were generated in this manner and each method was applied and respective confidence intervals obtained.

Figure~\ref{Fig:highcorr} depicts the results from the simulation with the plots on the right giving box plots for the lower (red) and upper (blue) bounds across the 100 datasets for 3 variables, $A_1$, $B_1$, and $N_1$. On the right, confidence intervals for the first 20 variables for a randomly selected example from the 100 simulated datasets is displayed.

Focusing on $A_1$ and $B_1$, what we would hope to see is wide intervals. Although $A_1$ truly is the signal variable, its high correlation with $B_1$ should produce a large amount of uncertainty about which variable (if not both) contain signal. Ridge, however, fails in this respect in that the intervals are narrow, with little distinguishable difference in width between $A_1$ and $B_1$ and the noise variables. On the other hand, Hybrid displays more desirable behavior. The uncertainty entangled in $A_1$ and $B_1$ is reflected with wider intervals. Additionally, intervals for $A_1$ usually do not contain 0 whereas the intervals for $B_1$ contains 0 over 40\% of the time. Ridge, on the other hand, produces no intervals that contain zero for $B_1$ and on the flip side hardly ever produce intervals with upper bounds above $0.5$ for $A_1$. In general, there is a clear shift in the intervals for $A_1$ compared to $B_1$ suggesting that even with very high correlation, the Hybrid bootstrap often attributed more of the signal to $A_1$, which is not the case with Ridge.

\begin{figure}[hbtp]
  \begin{center}
  \includegraphics[width=0.6\linewidth]{highcorr}
  \caption{\label{Fig:highcorr} Figure provides results for simulation described in Section~\ref{Sec:Ridge}. The right plots show a single example of a intervals produced by Ridge (top) and the Hybrid bootstrap (bottom) from one (randomly selected) of the 100 datasets for the first 20 variables. The left plot summarizes the resulting CIs for the variables $A_1$, $B_1$, and $N_1$ across the 100 simulations. The blue box plots provide the distributions for the upper bounds and the red boxplots for the lower bounds. Note, nominal coverage for this simulation was 80\%.}
  \end{center}
\end{figure}

\subsection{Comparison to Other HDCI Methods}\label{Sec:Comparison}

There are few other methods for obtaining intervals for the lasso and even fewer with implementation in companion R packages to allow for easy usage and comparison. Two that we were able to identify were Selective Inference (SelInf) introduced by \cite{LeeEtAl2016} and implemented in \texttt{selectiveInference} and Bootstrap Lasso Projection (BLP) introduced by \cite{ZhangZhang2014} and implemented in \texttt{hdi}.

Neither of these method fits the criteria, remaining faithful to the model fit, that was in mind when working on a new confidence interval producing method for this manuscript. Specifically, the Bootstrap Lasso Projection is also known as the de-sparsified Lasso and by default the method reselects $\lambda$ for each bootstrap sample, making it ambiguous how the results relate to a specific lasso model. Selective Inference doesn't directly correct for the bias introduced by penalization but this method does only provide intervals for variables with non-zero coefficients. Additionally, as we will see in Section~\ref{Sec:RDA}, the intervals produced are often questionable in relation to their corresponding point estimates, especially when p is greater than n.

\subsubsection{Similation Study}

\begin{figure}[hbtp]
  \begin{center}
  \includegraphics[width=0.65\linewidth]{laplace_comparison}
  \caption{\label{Fig:laplace_comparison} Results are from the simulation described in Section~\ref{Sec:Comparison} and identical to that of Section~\ref{Sec:Coverage}, except with different methods as a comparison for Hybrid. The fitted curves are from Binomial GAMMs (one for each method) fit with coverage being modeled as a smooth function of $|\beta|$ and with a random intercept on dataset to account for deviations in coverage specific to a given randomly generated dataset. The data used for modeling contains a row for each variable per simulated dataset. The dashed lines represent the average for each method across all 100 independently generated datasets and the solid black line indicates the nominal coverage rate. The shaded distribution in the background depicts the positive tail of the distribution the $\beta$s were drawn from, a Laplace.}
  \end{center}
\end{figure}

This simulation study compares the performance of the methods using their default parameters. With that said, as implemented in \texttt{hdi}, there is no way to specify the $\lambda$ used for BLP, which defaults to the 1-SE solution from \texttt{cv.glmnet} (and, again, which is re-selected for each bootstrap draw). For the simulation study, we use this ``out of the box'' setup, however, it was unsatisfactory for the comparisons in the subsequent data analyses. For the data analyses, we forked the \texttt{hdi} repository and adjusted it to allow for the specification of $\lambda$. Specifically, to allow for the use of the $\lambda$ which minimizes CVE. We did attempt to use this version in the simulation study, but this highlighted a limitation for BLP which made such usage infeasible under the desired setup. Specifically, BLP only works when the number of non-zero coefficients is less than n, something that is more likely to occur for the 1-SE $\lambda$ than for the $\lambda$ which minimizes CVE. As the simulation is set up, this proved prohibitive as more often than not when n was small relative to p, the method resulted in an error. Although it would be ideal to have the option to compare the methods under the same value of $\lambda$, there is something to be said for comparing the performance of the methods as closely as possible to what a user would experience if they started using one of the methods with its default arguments. However, it must be emphasized that this means the $\lambda$ values used for BLP vary greatly from those used for SelInf and Hybrid\footnote{\texttt{cv.glmnet} and \texttt{cv.ncvreg} generally select a similar value for $\lambda$. \texttt{cv.glmnet} was used for SelInf since \texttt{cv.glmnet} is what is used in examples for the \texttt{selectiveInference} package and \texttt{cv.ncvreg} was used for Hybrid since the Hybrid bootstrap is implemented in \texttt{ncvreg}.}, which both use a single $\lambda$ for each dataset with the $\lambda$ used being the value which minimizes CVE from \texttt{cv.glmnet} and \texttt{cv.ncvreg}, respectively, applied to the original dataset. HDI, on the other hand, has B $\lambda$ values (not returned) chosen as the 1-SE $\lambda$ from \texttt{cv.glmnet} applied to each of the B bootstrap draws from the original dataset.

\begin{figure}[hbtp]
  \begin{center}
  \includegraphics[width=0.65\linewidth]{laplace_other}
  \caption{\label{Fig:laplace_other} Results are from the simulation described in Section~\ref{Sec:Comparison}. Each plot provides corresponding results for each of BLP, Hybrid, and SelfInf for three different sample sizes. The top plot provides boxplots of coverages, the middle plot provides boxplots of the median widths, and the bottom plot provides boxplots of the run times, all across the 100 simulated datasets. Note that the plots for median width and for run time both have y-axes on the $\log_10$ scale.}
  \end{center}
\end{figure}

The simulation results presented here are from a set up identical to that described in Section~\ref{Sec:Coverage}. In fact, the results for Hybrid are the same as used for the earlier figures, just displayed differently. Referencing Figure~\ref{Fig:laplace_comparison}, both BLP and SelInf initially appear to perform strikingly well. Both have coverage near that of nominal and lack the coverage pattern seen with the Hybrid method, instead providing consistent coverage regardless of the magnitude of $\beta$. However, Figure~\ref{Fig:laplace_other} tells a different story. First, although BLP provides average coverage rates the closest to $80\%$, there were a number of cases where the coverage dipped significantly. Potentially more concerning is that when $n = 400$, the coverage noticeably drops below 80\%. Although the lasso is generally thought of as a model for high dimensional data, it is used across the entire spectrum of datasets, so it would be preferred to see convergence towards the nominal rate of coverage. The behavior for the Hybrid Bootstrap has been covered previously, specifically that it generally over covers when $n < p$ but has coverage near nominal as n increases above p. Additionally, although not immune to under coverage, it performed more reliably than the other two methods. Before moving onto the results for SelInf, there are a couple numbers not in Figure~\ref{Fig:laplace_comparison} which are also important to consider and which are provided in Table~\ref{Tab:selective_inference}. The first column gives the average number of variables included on average across the simulations (that succeeded) which also represents the number of variables on average that confidence intervals were provided for. The second column indicates that when n = 50, 20 of the 100 simulations errored out, with only 6 erroring out when n = 100, and none when n = 400. It is this subset (represented by the first and second columns) that is included in the coverage plot for SelInf. Although on average SelInf's coverage is near nominal, the coverages for individual simulations are sporadic, ranging from 0 to 1. The fact that SelInf does not produce intervals for all variables helps explain the sporadic coverage behavior, since its coverage values are averaged over fewer variables. Accordingly, there are also a considerable number of iterations that had coverage below 50\%, an issue which is not remedied even with larger values of n. So, even if on average SelInf looks good, the underlying behavior is much less desirable. This is similar to the conclusion drawn for BLP, and, if we think back to earlier in the manuscript, Debiased as well. The pattern that runs through BLP, SelInf, and Debiased is that debiasing comes at the expense of stability for these methods.

\begin{table}[hb]
  \centering
  \begin{tabular}{ccccc}
  \hline
  & Variables & \multicolumn{3}{c}{Simulations} \\
  \cmidrule(lr){2-2}\cmidrule(lr){3-5}
  n & \# Included on Average & \# Succeeded & \# Non-finite Median Width & \# Any Non-finite Width \\
  \hline
  50 & 18.5 & 80 & 18 & 40 \\
  100 & 30.4 & 94 & 12 & 63 \\
  400 & 70.1 & 100 & 3 & 55 \\
  \hline
  \end{tabular}
  \caption{This table provides additional information on the results for SelInf in the simulation described in Section~\ref{Sec:Comparison}. The first column indicated the average number of variables included (non-zero) in the lasso model at a given sample size. Note, this column also applies to BLP and Hybrid. The second column indicates the number of simulations that actually succeeded. A majority of the errors occur due to too large of a $\lambda$ value being selected using CV (causing all coefficients to equal zero), however, a failure to satisfy the polyhedral constraint also was a source of some errors. The third and fourth column indicate the number of simulations, among those that succeeded, that had a non-finite median width or any interval with an infinite width, respectively.}
  \label{Tab:selective_inference}
\end{table}

Staying with SelInf but directing our attention towards the interval widths, the concern surrounding SelInf only grows. Referring to column 3 of Table~\ref{Tab:selective_inference}, of the 80 simulations that suceeded for n = 50, the median width of the intervals produced (from the variables included in the model, column 1) was infinite for 18 of the simulations. For n = 100, 12 of the 94 simulations had infinite median widths. By n = 400, at which point one would hope this issue was eliminated, still 3 of the 100 simulations had infinite median widths. Column 4 gives the number of simulations with at least one interval with an infinite width. Additionally, we can see in the second plot of Figure~\ref{Fig:laplace_other}, which excludes the simulations with infinite median widths, even when the medians were finite, they were nearly always extremely wide, even for n = 400. This behavior was also observed when applying the method to real data sets which will be covered in Section~\ref{Sec:RDA}. BLP and Hybrid on the other hand produce intervals which are more similar in width although BLP does tend to produce wider intervals and with a bit more variability. As expected, the interval widths for three methods decrease with sample size.

The third plot of Figure~\ref{Fig:laplace_other} provides boxplots of the run times for each of the methods. The runtime differs considerably between the methods with SelInf being the fastest and BLP being by far the slowest with about an order of magnitude difference separating the methods respectively. Additionally with BLP, there is an odd non-monotonic behavior which we looked into briefly but could not find an explanation for. This behavior occurred in all reruns of the same simulation. Hybrid and SelInf both have a monotonically increasing relationship with sample size, although Hybrid is more affected by the increasing sample size. In our testing, speed was not a concern for SelInf, was noticeable for Hybrid, and prohibitive for BLP which will be returned to in Section~\ref{Sec:Scheetz2006}. BLP is particularly slow with its default arguments because of the reselection of $\lambda$ (using CV) for each iteration of the bootstrap.

\logan{I could see the reader questioning... well if you overcame a similar issue (just noted how many errors occured) for SelInf... why not do the same for BLP? The issue for BLP however is far worse because an error can potentially arrise in any bootstrap iteration whereas SelInf has to work just once, for the original dataset. That is, BLP fails much more miserably.}

\logan{I did start to track down the error for BLP and it seems to be due to a NaN showing up where they weren't expecting and it not being handled properly. However, I think it would take a considerable amount of time to find the root of the error and potentially fix it... which I could do but not sure if it is worth the effort.}

\section{Real Data Analysis}\label{Sec:RDA}

We conclude the results section by considering the intervals produced by the Hybrid bootstrap applied to two datasets: \texttt{whoari} (World Health Organization study on acute respiratory illnesses) and \texttt{Scheetz2006} (Gene expression in the mammalian eye). These two datasets sit on opposite ends of the spectrum in terms of dimensionality. \texttt{whoari} contains 816 observations and 66 features while \texttt{Scheetz2006} contains just 120 observations but with 18975 features, clearly making it a high dimensional dataset.

In this section, we also compare the intervals produced by the Hybrid bootstrap to those of BLP and SelInf. In this comparison, we wanted the results for the real data analysis to correspond directly back to a single set of point estimates for all three methods in order to put a clear emphasis on how the intervals relate to the corresponding estimates from the lasso fit using a selected value of $\lambda$. The $\lambda$ of interest is the value which minimizes CVE, selected here using \texttt{cv.glmnet}. This is not an issue for Hybrid or SelInf, which both allow $\lambda$ to be specified, but does require adjustments for BLP\footnote{The unmodified versions for BLP are provided in a supplement}. Since \texttt{hdi} was not set up with the flexibility to allow for the specification of $\lambda$, we forked the \texttt{hdi} repo and made the necessary modifications. Recall, without making the adjustment the option is fixed to be the 1-SE solution from \texttt{cv.glmnet} (although this is not indicated in \texttt{hdi}'s documentation). The second adjustment relative to the ``out of the box'' set up is supported in the arguments for BLP. This adjustment is to set \texttt{boot.shortcut = TRUE}. From hdi's documentation, if \texttt{boot.shortcut = TRUE}, ``the lasso is not re-tuned for each bootstrap iteration, but it uses the tuning parameter computed on the original data instead.'' With the first modification, then, BLP has the same behavior as Hybrid and SelInf. Lastly, it should not be ignored that BLP does provide its own estimates. However, the connection to the the original lasso fit is obscured, so for the purpose of this comparison, we use the the same point estimates across the three methods.

\subsection{World Health Organization study on acute respiratory illnesses (whoari)}\label{Sec:whoari}

\begin{figure}[hbtp]
  \begin{center}
  \includegraphics[width=0.6\linewidth]{comparison_data}
  \caption{\label{Fig:comparison_data_whoari} Confidence intervals produced by three different methods for all 66 variables in the \texttt{whoari} dataset described in Section~\ref{Sec:whoari}}
  \end{center}
\end{figure}

The \texttt{whoari} dataset comes from the study ``Development of a clinical prediction model for an ordinal outcome: the World Health Organization multicentre study of clinical signs and etiological agents of pneumonia, sepis and meningitis in young infants'' written by \cite{Harrell1998}. The study considered a few acute illness in young infants across several countries, and the dataset used here is a subset of 816 infants who presented with pneumonia in the country Ethiopia. As alluded to in the title, collection of this data was done with the intention of building a prediction model to assess the severity of an infant presenting with a serious infection, which represents the main cause of mobility and mortality in these developing countries for infants under 3 months of age. Diagnosis of severity is a difficult task, and developing rules for grading the severity of disease is important for prompt delivery of treatment for those who need it while also avoiding unnecessarily and costly treatments where possible. The outcome considered here is ordinal (taking on a number from 1 - 5), however, for simplicity we treat the outcome as continuous and feel that the results are reasonable, at least for a comparison of the three methods under consideration. The variables collected contain information on vital signs, family history, and clinical observations and represent a range of datatypes from binary to ordinal to continuous. With $N \approx 10p$, this dataset is not necessarily high dimensional, but sits on the edge of where classical methods and their resulting inferences may be questionable. Additionally, the use of a model that produces sparsity is beneficial both for interpretation and for ultimately determining factors for assessment in practice, where obtaining predictions from a model may be prohibitive.

Figure~\ref{Fig:comparison_data_whoari} provides the confidence intervals from each of the three methods along with corresponding point estimates. Recall, the point estimates are the same across all three methods and come from the lasso fit on the original data with $\lambda$ selected using \texttt{cv.glmnet}. It is important to emphasize that the range of the x-axis is different for each of the plots corresponding to the three methods. The Hybrid produces the narrowest intervals and SelInf produces by far the widest. Despite the difference in widths, Hybrid and BLP share similar patterns, however, the conclusions that might be drawn could conceivably be quite varied. This may most easily be observed by considering a common point of interest: whether or not an interval includes zero. BLP produces 3 intervals that do not contain zero, SelInf produces 13 that do not contain zero, and Hybrid produces 21 that do not contain zero. SelInf producing 13 intervals not containing zero is only part of the picture, since it is also important to note SelInf selects (and provides intervals for) 37 of the 66 variables and only produces one interval here with an infinite bound (for \texttt{abb}). That said, there are a number of intervals that are unreasonably wide and we believe it would be difficult to provide a convincing interpretation for the intervals produced by SelInf. The comparison between Hybrid and HDI is more interesting since, visually, they appear similar. The patterns observed here are similar to those in the preceding simulation study. As previously mentioned, the intervals from Hybrid are narrower on average. Additionally, while Hybrid's intervals, relative to the point estimates, tend to be skewed towards zero or symmetric, BLP's intervals are often skewed away from zero, likely as a result of debiasing. It appears then, that BLP's higher coverage at larger values of $\beta$, as seen in the simulation study, is likely due to a combination of increased width and the skewness induced by debiasing.  

\subsection{Gene expression in the mammalian eye (Scheetz2006)}\label{Sec:Scheetz2006}

\begin{figure}[hbtp]
  \begin{center}
  \includegraphics[width=0.6\linewidth]{comparison_data_scheetz}
  \caption{\label{Fig:comparison_data_scheetz} Confidence intervals produced by three different methods for the 20 variables with the largest absolute point estimates in the \texttt{Scheetz2006} dataset described in Section~\ref{Sec:Scheetz2006}}
  \end{center}
\end{figure}


The \texttt{Scheetz2006} data was obtained from the study ``Regulation of gene expression in the mammalian eye and its relevance to eye disease'', written by \cite{Scheetz2006}. This study involved measuring the RNA levels from the eyes of 120 rats. Of 31000 different probes used, 18976 were detected at a sufficient level to be considered ``expressed''. For this analysis we treat one of the genes, Trim32, as the outcome since it is known to be linked to the genetic disorder Bardet-Biedl Syndrome (BBS). The remaining 18975 genes are used as covariates with the goal of determining other genes which may have expression correlated with Trim32 and thus may also contribute to BBS. 

In the simulation study, we saw that when p is large relative to n, SelInf's difficulties are amplified. Applied to \texttt{Sheetz2006}, where $p > 100n$, the issues are unignorable. SelInf provides intervals for 66 of the 18975 features however, every single one of them has a lower or upper bound that is infinite. Additionally, for the bounds that are finite, they are all also extremely large compared to their respective point estimates and in comparison to the intervals produced by Hybrid and BLP. Additionally, none of the intervals contain zero. Potentially even more odd is that of the 66 intervals, 62 of them were completely of the opposite sign as the corresponding estimate. 

Like with \texttt{whoari}, Hybrid and BLP produce similar intervals, however with their characteristic differences more prominent due to the dimensionality of the dataset. As such, the intervals of Hybrid are narrower, and, from the examples provided in Figure~\ref{Fig:comparison_data_scheetz}, the intervals produced by BLP may appear more desirable due to their relation with the corresponding point estimates. With this very high dimensional dataset, Hybrid produces intervals that are noticeably drawn in towards zero, likely due to a larger being penalty selected in this setting. As such, a couple of the intervals produced by Hybrid exclude their corresponding point estimate. 

Despite these differences, depending on the perspective taken, there is not a large discrepancy for the variables deemed significant by the two methods. BLP produces 6 intervals which exclude zero, while Hybrid produces 3. This is twice as many, but in the grand scheme of nearly twenty-thousand variables, this is a relatively minor difference. Both methods have intervals not containing zero for \texttt{1389910\_at}, \texttt{1378319\_at}, and \texttt{1385395\_at}, while BLP produces intervals which do not contain zero for three additional genes.

That said, we emphasize again that a user is not able to obtain these results from the implementation in \texttt{hdi}. Additionally, even with \texttt{boot.shortcut = TRUE} (where $\lambda$ is not reselected for each bootstrap iteration), on a MacBook Pro with 16 GB of RAM and an Apple M1 Pro chip, BLP took over 6 hours to run. \logan{Should update with run on HPC / with more specific time details.}

\section{Discussion}

\logan{currently serving as a catch for things removed... but maybe not entirely eliminated.}

Estimation of $\sigma^2$. \logan{I think this could be something interesting to explore further and then make reference to the Reid 2016 paper. Specifically smaller estimates would decrease overcoverage near zero, and have less effects away from zero. It could also be interesting to re-explore posterior but with an alternative method for posterior where the estimates of sigma2 are adjusted for varios values of beta. In attempts to provide a consistent coverage level in a way alternative to debiasing. The intervals would have to be pretty wide for larger betas... Not saying this should even be mentioned but is something I ahve thought about.} 

Debiasing.

Guidance on interpreting intervals. Important to note shifted towards zero, causing under coverage.

\subsection{Intuition behind the hybrid bootstrap}

Given that the hybrid bootstrap is the main focus of this manuscript, it will be worth while to briefly consider the intuition driving this proposed sampling method. Recall, potentially the most striking issue with the traditional bootstrap is that it can produce intervals which are singleton at 0. This occurs when $\hat{\beta}_j^b = 0$ for at least $B * (1 - \alpha)$ draws. To address this issue, then, adjustment is only needed when $\hat{\beta}_j^{b} = 0$. 

Now, we consider the behavior of the Hybrid interval from a logical perspective. For a moment, hold p constant. As n increases and we let $\lambda$ get selected by CV, this method will converge to that of applying the traditional bootstrap to a classical linear regression model. This is the case because with increasing n, $\lambda_{CV} \rightarrow 0$. On the other hand, as n decreases $\lambda_{CV} \rightarrow \lambda_{max}$ and $\hat{\boldsymbol{\beta}}\rightarrow \boldsymbol{0}$. On this end of the extreme, bootstrap samples are then drawn from the marginal posterior. The coverage behavior is well understood for the bootstrap applied to linear regression as n increases. Thus, by only sampling from the posterior when $\hat{\beta}_j^b = 0$, we minimize the addition of unnecessary variability as n increases. The behavior on the other end of the extreme is relatively less understood compared to the asymptotics of the traditional bootstrap. However, we argue that leveraging the posterior lends itself to reasonable behavior especially when the alternative would be confidence intervals which are all identical to zero. Such intervals, one could argue, would necessarily have 0\% coverage. Conversely, as we will see, with smaller values of n, the Hybrid tends to lead us to wider confidence intervals which over rather than under cover relative to nominal.

\subsection{Space Requirements}

As implemented, a clear limitation of this method is that it takes a numeric matrix size $B \times p$. With $B = 1000$, the sample matrix gets large enough to cause memory concerns even when $p$ is on the order of $1e5$. For many datasets, this is likely not of concern. However, given that lasso is often used for datasets where $p$ is large, it is clearly not an edge case where $p$ is of this or larger order. One could reduce the size of the sample matrix by reducing the number of draws, but this is unsatisfactory and produces little additional leeway for what seems like a unacceptable sacrifice. This is an ongoing are of interest and a valuable areas of research for any high dimensional bootstrap methods. One solution would be using incremental quantile estimation such as the method introduced by \cite{Tierney1983}. An alternative option would be deriving a method with similar properties but which relies on samples statistics that are relatively straightforward to update over sequential samples (i.e. mean and variance).

\subsection{Remember the Name}

Since the main method suggested here falls somewhere between the Traditional Bootstrap and the Posterior Bootstrap, we propose the full name as the Posterior Adjusted Traditional Hybrid (PATH) Bootstrap.

\newpage

\section*{Supplement}

\subsection{Supplement A: Sampling from the Full Conditional Posterior}\label{Sup:A}

\logan{Discuss notation}

The lasso can be formulated as a Bayesian regression model with the prior $\p(\bb) = \prod_{j = 1}^{p} \frac{\gamma}{2}\exp(-\gamma \abs{\beta_j})$ for $\gamma > 0$ and with $\lam = \gamma \frac{\sigma^2}{n}$. With $\gamma$ written in terms of $\lambda$, $\sigma^2$, and $n$, then the prior on $\beta_j$, conditional on $\bb_{-j}$, is proportional to $\exp(-\frac{n \lambda} {\sigma^2} \abs{\beta_j})$. Similarly, the full conditional likelihood can be simplified. Instead of conditioning on $\y$, $\X$, and $\bb_{-j}$ in what follows, we simply condition on $\r_{-j} = \y - \X_{-j}\bb_{-j}$ and implicitly condition on $\x_j$. Assuming $\X$ has been standardized s.t. $\x_j^T\x_j = n$,

\as{
  \begin{aligned}
  L(\beta_j|\r_{-j}, \lambda, \sigma^2) &\propto \exp(-\frac{1}{2\sigma^2} ||\y - \X\bb||_2^2) \\
  &= \exp(-\frac{1}{2\sigma^2} ||(\y - \X_{-j} \bb_{-j}) - \x_{j} \beta_{j}||_2^2) \\
  &= \exp(-\frac{1}{2\sigma^2} ||\r_{-j} - \x_{j} \beta_{j}||_2^2) \\
  &\propto \exp(-\frac{1}{2\sigma^2}( - 2\x_{j}^T\r_{-j} \beta_{j} + \x_{j}^T\x_{j} \beta_{j}^2)) \\
  &= \exp(-\frac{1}{2\sigma^2}( - 2 n z_{j} \beta_{j} + n \beta_{j}^2)) \\
  &= \exp(-\frac{n}{2\sigma^2}(\beta_{j}^2 - 2 z_{j} \beta_{j})), \\
  \end{aligned}
}

where $z_{j} = \frac{1}{n} \x_{j}^{T}\r_{-j}$.

With this the form of the full conditional posterior can be worked out as follows:
\as{
\Rightarrow P(\beta_j | \r_{-j}) &\propto \exp(-\frac{n}{2\sigma^2} (\beta_j^2 - 2z_{j}\beta_j)) \exp(-\frac{n \lambda} {\sigma^2} \abs{\beta_j}) \\
&= \exp(-\frac{n}{2\sigma^2} (\beta_j^2 - 2 z_j\beta_j +  2 \lambda \abs{\beta_j})) \\
&= \exp(-\frac{n}{2\sigma^2} (\beta_j^2 - 2(z_j\beta_j - \lambda \abs{\beta_j}))) \\
&=
\begin{cases}
\exp(-\frac{n}{2\sigma^2} (\beta_j^2 - 2(z_j + \lambda)\beta_j)), \text{ if } \beta_j < 0, \\
\exp(-\frac{n}{2\sigma^2} (\beta_j^2 - 2(z_j - \lambda)\beta_j )), \text{ if } \beta_j \geq 0 \\
\end{cases} \\
&\propto
\begin{cases}
C_{-} \exp(-\frac{n}{2\sigma^2} (\beta_j - (z_j + \lambda))^2), \text{ if } \beta_j < 0, \\
C_{+} \exp(-\frac{n}{2\sigma^2} (\beta_j - (z_j - \lambda))^2), \text{ if } \beta_j \geq 0 \\
\end{cases}
}
where, $C_{-} = \exp(\frac{z_j \lambda n}{\sigma^2})$ and $C_{+} = \exp(-\frac{z_j \lambda n}{\sigma^2})$.

At this point, te reader likely notices that the piecewise defined full conditional posterior is made up of a kernel of two normal distributions. This can be leveraged and draws can be efficiently obtained from through a mapping onto the respective normal distributions. To define this mapping, it helps to introduce a concept and some notation. First, the use of ``tails'' in this supplement refers to the entirety the a distribution between zero and $\pm \infty$. That is, the lower tail is any part of the distribution below zero and the upper tail is any part greater than zero and $P(X \in lower \cup X \in upper) = 1$. Accordingly, we will let the tail probabilities in each of the two normals to transformed on to be denoted $Pr_{-}$ and $Pr_{+}$ respectively and the probability in each of the tails of the posterior, denoted $Post_{-}$ and $Post_{+}$ respectively. $Pr_{\pm}$ is trivial to compute with any statistical software. $Post_{\pm}$ is conceptually simple, although care must be taken to avoid numerical instability as n increases. Now, as noted,
\as{
P(\beta_j | \r_{-j})  & \propto
\begin{cases}
C_{-} Pr_{-}, \text{ if } \beta_j < 0, \\
C_{+} Pr_{+}, \text{ if } \beta_j \geq 0\\
\end{cases}
} which implies that $Post_- = \frac{C_{-} Pr_{-}}{C_{-} Pr_{-} + C_{+} Pr_{+}}$ and similarly for $Post_+$. However, to avoid numerical instability, or at least handle it properly when it is unavoidable, we need to work on the $\log$ scale. This works well for most of the problem, but computation of $Post_-$ and $Post_+$ need something a bit more since, for example, $\log(Post_-) = \log(C_{-}Pr_{-}) - \log(C_{-} Pr_{-} + C_{+} Pr_{+})$. That is, the denominator still must be computed then the $\log$ taken which does not allow operation on the $\log$ scale to fully address potential instability. Instead, $\log(Post_-)$ can be computed with $\log(C_-Pr_-) -  \log(C_+Pr_+) - \log(1 + \exp(\log(C_-Pr_-) -  \log(C_+Pr_+)))$. This still doesn't completely address the issue, however, if $\exp(\log(C_-Pr_-) -  \log(C_+Pr_+))$ is infinite then $C_-Pr_- >> C_+Pr_+$ and $\log(Post_-) \approx 0$.

With these values, we can compute quantiles by mapping the corresponding probabilities $p$ for the posterior onto the probabilities $p^*$ for the corresponding normals. Which normal the quantiles of interest ultimately come from is determined based on $Post_{\pm}$. For example, if $Post_{+} = 0.98$ and $p = 0.1$ the $p$ would be mapped onto the positive normal. As one more example, say $Post_{+} = 0.4$ and $p = 0.5$, then $p$ would be mapped onto the negative normal. The transformation to map a given probability from the posterior depends on which tail the quantile resides in on the posterior (equivalently which normal it is being mapped to, the positive or negative). This map is simply:

\as{
p^* &= p \times (Pr_{\pm} / Post_{\pm}) \\
}


Once the respective probabilities are mapped, one can simply use the inverses of the normal CDFs that the probabilities were mapped to. That being said, there is a nuance worth pointing out. When transforming the probabilities, the step to determine which tail the respective quantile comes from occurs first. With this, the probability should be adjusted so that it refers to the probability between the quantile of interest and the respective tail. After this, then the transformation can be applied. With that, obtaining draws from the full conditional posterior can be summarized as follows (written for a single $\beta$ for simplicity):

\begin{enumerate}
  \item Select $\lambda$, fit lasso and obtain estimates corresponding to $\lambda$, estimate $\sigma^2$
	\item Obtain the partial residuals, $\r_{-j}$, and compute $z_j$
	\item Compute $Pr_{-}$ = $\Phi(0, z_j + \lam, \frac{\sh^2}{n})$ and $Pr_{+}$ = $1 - \Phi(0, z_j - \lam, \frac{\sh^2}{n})$
	\item Compute $Post_-$ and $Post_+$ as detailed above
	\item Obtain the quantile $(q)$ corresponding to the given probability $(p)$ of interest:
  \begin{algorithmic}
    \If {$p \leq Post_{-}$}
      \State $q = \Phi^{-1}(p(Pr_{-} / Post_{-}), z_j + \lam, \frac{\sh^2}{n})$
    \Else
        \State $q = \Phi^{-1}(1 - (1 - p)(Pr_{+} / Post_{+}), z_j - \lam, \frac{\sh^2}{n})$
    \EndIf
  \end{algorithmic}
\end{enumerate}

\newpage

\subsection{Supplement B: Behavior under true lambda}\label{Sup:B}

This simulation considers the behavior of the Hybrid intervals if $\lam$ is set to its true value and, additionally, if $\sigma^2$ is also set to its true value, in this case, one. The simulation set up here is identical to the one described in Section~\ref{Sec:Coverage}. Recall that, like the rest of the simulation in the manuscript, the coefficients where generated s.t. $\bb^T\bb = 1$ so that with $\sigma^2=1$ and independent features, a signal-to-noise ratio (SNR) of 1 is achieved. Also recall that the simulation in Section~\ref{Sec:Coverage} generated $\beta$ from a Laplace distribution. Thus, this implies a true Laplace rate of $\lam=\sqrt{2p}$, since $\sum_{j=1}^p\Ex(\beta_j^2|\lam=\sqrt{2p}) = 1$. So, the only difference in the simulation here is that $\lambda$ is set to be this value instead of being selected via cross validation. 

\begin{figure}[hbtp]
  \begin{center}
  \includegraphics[width=0.65\linewidth]{true_lambda}
  \caption{\label{Fig:true_lambda} The results displayed are from a simulation with the same data generating mechanism as described in Section~\ref{Sec:Coverage} but with three different samples sizes. The smooth curves are also constructed in the same manner and provide estimates for coverage as a smooth function of $\beta$. The dashed horizontal lines indicated the average coverage across all 100 simulations. The left plot shows the results where the true value of $\lambda$ was used in the Hybrid bootstrap and the right provides results where both $\lam$ and $\sigma^2$ were set at their true values. The black line indicates the 80\% nominal coverage rate.}
  \end{center}
\end{figure}

Figure~\ref{Fig:true_lambda} shows the results of the simulation described with $\lambda$ set to the true value (left) and with both $\lambda$ and $\sigma^2$ set to their true values (right), see the right plot in Figure~\ref{Fig:nominal_coverage} as a comparison for $\lambda_{\CV}$ and $\hat{\sigma}^2(\lambda_{\CV})$. When just the true value of $\lam$ is known, there is slightly more over coverage (generally $\lam_{\CV}$ is larger than the true value of $\lam$), but otherwise the results are generally comparable to when $\lambda$ is selected via CV, assuming $\hat{\sigma}^2$ is set equal to the minimum CVE. This, like Figure~\ref{Fig:beta_lambda_heatmap_laplace}, suggests that $\lam_{\CV}$ generally provides a reasonable selection of $\lam$. However, the differences are more notable when $\sigma^2$ is also set to its true value of one. All the coverages are near nominal, with slight under coverage for larger values of n. This suggests that using CVE to estimate $\sigma^2$, which generally over estimates variability, especially when $n < p$, is responsible for the over coverage observed throughout the results for the Hybrid bootstrap. However, in absence of knowing the true value of $\sigma^2$, over estimating it and maintain coverage above that of nominal is preferred to the alternative. 

\logan{Now that I come back to this, I am remembering why I added the true value of lambda to the heatmap... wondering if it is worth representing it here with the additional line? Specifically that $\lambda_{\CV}$ is in between the value which provides nominal coverage and the truth.}

\newpage

\subsection{Supplement C: Unmodified BLP Real Data Analysis}\label{Sup:C}

\begin{figure}[hbtp]
  \begin{center}
  \includegraphics[width=0.6\linewidth]{comparison_data_original}
  \caption{\label{Fig:comparison_data_whoari_original} Confidence intervals produced by unmodified BLP for all 66 variables in the \texttt{whoari} dataset described in Section~\ref{Sec:whoari}}
  \end{center}
\end{figure}

\begin{figure}[hbtp]
  \begin{center}
  \includegraphics[width=0.6\linewidth]{comparison_data_scheetz_original}
  \caption{\label{Fig:comparison_data_scheetz_original} Confidence intervals produced by unmodified BLP for the 20 variables with the largest absolute point estimates in the \texttt{Scheetz2006} dataset described in Section~\ref{Sec:Scheetz2006}}
  \end{center}
\end{figure}

\newpage

\subsection{Supplement D: Nominal Coverage}\label{Sup:D}

\begin{figure}[hbtp]
  \begin{center}
  \includegraphics[width=0.6\linewidth]{nominal_coverage}
  \caption{\label{Fig:nominal_coverage}  The results displayed are from a simulation with the same data generating mechanism as described in Section~\ref{Sec:Coverage} but with three different samples sizes across three different nominal coverage rates, indicated by the horizontal black lines. The smooth curves are also constructed in the same manner and provide estimates for coverage as a smooth function of $\beta$. The dashed horizontal lines indicated the average coverage across all 100 simulations}
  \end{center}
\end{figure} 

Figure~\ref{Fig:nominal_coverage} is similar to Figure~\ref{Fig:laplace}, but focuses only on Hybrid and gives simulation results for three values of n across three different nominal coverage rates. Since the Hybrid bootstrap method has a varying coverage rates depending on the magnitude of $\beta$, it is important to consider different nominal coverages. Otherwise, it is conceivable that a method could perform well at one coverage rate but not another. However, that is not the case here. Regardless of the nominal coverage, the general pattern remains the same: the method over covers for smaller values of n but coverage converges to the nominal coverage rate relatively quickly. The only difference seen is the compression of this pattern for higher nominal coverage rates.

\newpage