\section{Introduction}

The objective function for lasso-penalized linear regression \citep{Tibshirani1996} is 
$$Q(\bb|\X,\y,\lambda) = \frac{1}{2n}\norm{\y - \X\bb}_2^2 + \lambda\norm{\bb}_1,$$
where $\y$ is a length $n$ vector of independent outcomes, $\X$ is an $n \times p$ matrix of features, $\bb$ is a length $p$ vector of regression coefficients, and $\lambda$ is a regularization parameter controlling the amount of penalization. Note that the objective function involves the addition of the $L_1$ penalty, $\lambda\norm{\bb}_1 = \lambda \sum_{j = 1}^p |\beta_j|$, to the squared error loss. This typically results in sparse estimates for some of the regression coefficients (i.e., $\bh_j = 0$) depending on the choice of the regularization parameter $\lambda$. Its ability to carry out both variable selection and estimation is particularly attractive, especially in scenarios where both predictive accuracy and interpretability are important. The model performs particularly well in cases where the number of features is large and the underlying model is sparse \citep{HTF2009}, but has become popular in a wide variety of settings.

Nevertheless, inference for the lasso has proven challenging. By introducing both sparsity and shrinkage, the $L_1$ penalty greatly complicates the sampling distribution of the estimators. This complexity has given rise to a wide variety of inferential approaches. The majority of these approaches have focused on controlling the false discovery rate (FDR) of the selected features. Examples include the Covariance test \citep{Lockhart2014}, the Knockoff Filter \citep{Candes2015,Candes2018}, the marginal FDR \citep{Breheny2019}, and the Gaussian mirror \citep{Xing2023}.

There have also been various proposals for constructing confidence intervals (CIs), although the shrinkage/bias introduced by the $L_1$ penalty introduces a number of challenges here. Several methods \citep{ZhangZhang2014,Javanmard2014} focus on ``debiasing'' the original point estimates from a lasso fit to facilitate more traditional forms of inference. An alternative approach, which accounts for the uncertainty in model selection by conditioning on the selected model is known as selective inference \citep{LeeEtAl2016}, although it is worth noting that this approach only produces intervals for variables that were selected by the model.

The bootstrap is often a natural choice for handling complex sampling distributions. However, \cite{Chatterjee2010} demonstrated that when applied to lasso estimators, the bootstrap is inconsistent -- even if the lasso model itself is $\sqrt{n}$-consistent. For this reason, efforts to bootstrap lasso models have focused instead on bootstrapping de-biased (or de-sparsified) versions of the lasso \citep{Dezeure2017}.

In this manuscript, we take up the question of whether bootstrapping lasso estimators is perhaps more useful than the statistical community is giving it credit for. In particular: does the bootstrap not work at all, or does it merely not work \emph{in the classical sense}? And if it doesn't work, can the bootstrap be modified so that it does? We find that the answer is a combination of both. Yes, there does need to be a methodological adjustment to fix the bootstrap, but also a change in perspective. Bootstrapping biased lasso estimators may not yield correct coverage in the classical sense for each individual coefficient, but we show that it \emph{does} result in approximately correct overall, or average coverage. There is no objectively correct perspective, but we discuss the differences between them and hope the reader finds the debate illuminating. For the sake of simplicity, we focus on lasso-penalized linear regression, but most of the discussion is relevant to any sparse penalty and loss.

Section 2 examines the underlying concepts in more detail and Section 3 introduces a methodological fix. Then Section 4 examines the performance of the proposed method across a number of simulations and includes a comparison to two previously mentioned HDCI alternatives, selective inference and the bootstrapped de-sparsified lasso. Lastly, in Section 5, we show the application of the proposed method to two data sets, one for acute respiratory illness and the other for gene expression data in mammalian eyes.

\section{Issues with Bootstrapping the lasso}
\label{Sec:Difficulties}

There are two fundamental reasons that the bootstrap fails to be consistent when applied to lasso models. One of these issues requires an adjustment to the bootstrap; the other requires a change in perspective. To facilitate discussion, we will refer to these issues as

\begin{itemize}
\item Epsilon Conundrum (EC), and
\item Individual vs Average Coverage (IAC)
\end{itemize}

The Epsilon Conundrum is a side effect of the sparsity introduced by the $L_1$ penalty. Because the lasso model is sparse, many of the bootstrap draws will be exactly zero. If we then construct bootstrap percentile intervals, there is a substantial probability that one or both of the endpoints will be exactly equal to zero
\pb{; to be precise, this will happen any time a feature is included in the active set for less than $B(1 - \alpha)$ bootstrap replications. Should we include this here? It's kind of useful, but at the same time, we need to define both B and alpha, which is starting to seem like a tangent}
If the true value of the coefficient is zero, this does not pose a problem. However, if the true value is not exactly zero, but simply very small (say, a distance $\eps$ from zero), the coverage of the bootstrap interval is nowhere close to its nominal value.

Figure~\ref{Fig:traditional} shows that this is exactly what happens when a traditional bootstrapping approach is applied to the lasso. In this example, 100 datasets were generated, each with 94 coefficients randomly set to be about $\pm 3 \times 10^{-10}$ with the other 6 being relatively large in comparison. The plot illustrates 20 CIs along with their corresponding point estimates (filled black circle) and true values (open red circle), as applied to one of the datasets. Across all 100 datasets, the average coverage was 26.7\%, far below the nominal coverage rate of 80\%. In Section~\ref{Sec:methods}, we propose a method for addressing this issue, \pb{which produces the results on the right side of Figure~\ref{Fig:traditional}; can we combine these results? I want to see them side-by-side}.

\begin{figure}[hbtp]
  \begin{center}
    \includegraphics[width=0.6\linewidth]{ec_lasso_traditional}
    \caption{\label{Fig:traditional} 100 datasets were generated under independence with 94 of the coefficients having true values randomly assigned to be approximately $\pm$ 3e-10. The other 6 coefficients were larger in comparison ($\approx \pm 0.62, 0.31, 0.15$). The true values are unrounded and are set s.t. SNR = 1 under the restriction that $\sigma^2$ = 1. The plot provides a subset of 20 of the intervals produced by applying a traditional bootstrapping procedure to the lasso for a single dataset, randomly selected among the 100 generated. The points indicate point estimates from the lasso for the corresponding intervals and the red circles indicate the true values. The average coverage across all 100 datasets was 26.7\%. Nominal coverage was set to be 80\%. \pb{Combine with hybrid, on right?}}
  \end{center}
  \end{figure}

Interval Coverage Consistency, on the other hand is less of an issue to be directly solved and more of a philosophical question. In short, ICC is concerned with if coverage is consistent with the nominal coverage rate for each interval or for the collection of intervals. In the classical frequentest sense, each interval must have exactly nominal coverage, regardless of the true value of $\beta$. That is for any value of $\beta$, $\int I(\beta \in \boldsymbol{A} | \boldsymbol{y}) p(\boldsymbol{y} | \beta)d\boldsymbol{y}$ = 1 - $\alpha$. In this sense, the coverage behavior of interest is more or less isolated for each individual interval. However, in the context of regression, we rarely consider a single covariate, but instead have many covariates with likely a range of true coefficient values. This brings up an alternative, more permissive, way to consider coverage. The alternative would be to consider the average coverage for the collection of intervals over a distribution of the likely values of $\beta$. This is the idea covered in the following theorem.

\begin{thm}
  \label{Thm:bcc}
  If the likelihood is correctly specified according to the true data generating mechanism, $p(\boldsymbol{y} | \bt)$, then credible sets obtained from $p(\bt|\boldsymbol{y})$ will have proper coverage when averaged over the distribution of likely values of $\theta$ if the prior distribution, $p(\bt)$, is correctly specified according to the distribution of $\theta$ values.
\end{thm}

\begin{proof}
Assume the likelihood and prior are correctly specified according the their respective distributions. By definition, a $100(1-\alpha)\%$ credible region for $\boldsymbol{\theta}$ is any set $\boldsymbol{A}$ s.t. $p(\bt \in \boldsymbol{A} | \boldsymbol{y}) \geq 1 - \alpha$. For a given $\bt_0$, the coverage probability can be defined as $\int I(\bt_0 \in \boldsymbol{A} | \boldsymbol{y}) p(\boldsymbol{y} | \bt_0)d\boldsymbol{y}$. Then, averaged over $p(\bt)$, the average coverage probability can be defined as

\as{
  \begin{aligned}
  \int \int I(\bt \in \boldsymbol{A} | \boldsymbol{y}) p(\boldsymbol{y} | \bt)d\boldsymbol{y}p(\bt)d\bt &= \int \int I(\bt \in \boldsymbol{A} | \boldsymbol{y}) p(\boldsymbol{y} | \bt)p(\bt)d\boldsymbol{y}d\bt \\
  &=  \int \int I(\bt \in \boldsymbol{A} | \boldsymbol{y}) p(\bt|\boldsymbol{y})p(\boldsymbol{y})d\boldsymbol{y}d\bt^* \\
  &=  \int \int I(\bt \in \boldsymbol{A} | \boldsymbol{y}) p(\bt|\boldsymbol{y})d\bt p(\boldsymbol{y})d\boldsymbol{y} \\
  &=  \int \int_{\boldsymbol{A}} p(\bt|\boldsymbol{y})d\bt p(\boldsymbol{y})d\boldsymbol{y} \\
  &\geq (1-\alpha)  \int p(\boldsymbol{y})d\boldsymbol{y} \\
  &= 1-\alpha
  \end{aligned}
}

*This step is only valid if the assumptions are met.
\end{proof}
The idea then being that to maintain proper coverage, individual intervals can deviate from the nominal coverage rate, but averaged over the distribution of likely values for $\beta$, the coverage must be greater than or equal to the nominal coverage rate. That is, the coverage rates for some values of $\beta$ may be higher than nominal coverage rates while others are lower, but this is fine as long as they are appropriately balanced. We argue that this is a reasonable perspective on coverage rates for confidence intervals arising from penalized regression models applied to high dimensional datasets. In such scenarios, p is usually large and the estimators are almost necessarily biased. p being large allows for more reliable average coverage while the effect of bias is a a driving force behind the need for a more inclusive definition of coverage.

Along with solving the issue of the Epsilon Conundrum, the intervals proposed in Section~\ref{Sec:methods} of this manuscript have a Bayesian feel to them in the sense that they obey the proposed collective definition of coverage. Alternatively, an attempt could be made to debias the intervals in order to meet the traditional frequentest definition of coverage. This proposition of a new definition of coverage is not to say that attempts at debiasing are unwarranted, indeed, meeting the traditional definition of coverage also means that this relaxed definition is satisfied. However, intervals that do not attempt to meet the rigid definition for coverage, using methods such as debiasing, are arguably more faithful to the model fit, an idea we will revisit later. 


\section{Lasso Bootstrap Confidence Intervals}\label{Sec:methods}

In this section, we present three alternative methods for obtaining bootstrap draws. Let $b \in \lbrace 1, \ldots, B \rbrace$ indicate the $b^{th}$ bootstrap draw.

\begin{itemize}
\item \textbf{Traditional bootstrap:} Simply takes the point estimate, $\hat{\beta}_j^b$,  for each $\beta_j$ for each respective bootstrap sample.
\item \textbf{Posterior bootstrap:} Leverages the Bayesian formulation of the model. For each bootstrap sample, a random draw from the full conditional posterior is obtained instead of taking the point estimate, which for the lasso is equivalent to the mode of the posterior.
\item \textbf{Hybrid bootstrap:} Takes a middle ground between the posterior bootstrap and the traditional bootstrap. The hybrid bootstrap only samples from the full conditional if the point estimate for a given $\beta_j$ is equal to zero for that bootstrap sample. Otherwise, takes the point estimate like the traditional bootstrap.
\end{itemize}

For each method, the main idea is given above. The remainder of this section presents their specific application to lasso-penalized linear regression. Section \ref{Sec:full-cond} defines and derives the full conditional distributions needed for the sampling in the posterior and hybrid methods. Explicit calculations for all  methods are then given in Section~\ref{Sec:implementation}.

\subsection{Full conditional distributions for the lasso}
\label{Sec:full-cond}

In this section, we provide a high level derivation of the full conditional posterior distributions for lasso-penalized regression. Full details, including how to perform sampling, are provided in Supplement~\ref{Sup:A}.

As with other penalized regression models, the lasso can be formulated as a Bayesian regression model by setting an appropriate prior. This was addressed initially by \cite{Tibshirani1996} and covered more extensively by \cite{Park2008}. For the lasso, the corresponding prior is a Laplace distribution, also referred to as the double-exponential distribution: $$\p(\bb) = \prod_{j = 1}^{p} \frac{\gamma}{2}\exp(-\gamma \abs{\beta_j}), \gamma > 0.$$ With this prior, the lasso estimate, $\bbh(\lam)$, is the posterior mode for $\bb$ when $\lam = \gamma \frac{\sigma^2}{n}$.

The full conditional posterior for $\beta_j$ is defined as the distribution for $\beta_j$ conditional on $\bb_{-j}$, $\lambda$, and $\sigma^2$. Further discussion is given to selecting $\lambda$ and estimating $\sigma^2$ at the end of this section. Once $\lambda$ is selected, $\bb_{-j}$ is set equal to $\hat{\bb}_{-j}(\lambda)$ and $\r_{j}$, the partial residuals, are then defined as $\y - \X_{-j}\hat{\bb}_{-j}(\lambda)$. To reduce notational distractions, we only explicitly condition on $\r_{j}$ to denote full conditional distributions. A normal likelihood and Laplace prior are not conjugate. However, the full conditional posterior can be shown to be a mixture of right and left truncated normals where the truncation occurs at zero for right and left tails respectively. In this manuscript, we will assume that $\X$ has been standardized s.t. $\x_j^T\x_j = n$. Then, as shown in Supplement~\ref{Sup:A}, for $\beta_j$,

\as{
L(\beta_j | \r_{j}) &\propto \exp(-\frac{n}{2\sigma^2} (\beta_j^2 - 2z_j\beta_j)), \\
\text{ where } z_{j} &= \frac{1}{n} \x_{j}^{T}\r_{j} \text{, and} \\
P(\beta_j | \r_{j}) &\propto
\begin{cases}
C_{-} \exp(-\frac{n}{2\sigma^2} (\beta_j - (z_j + \lambda))^2), \text{ if } \beta_j < 0, \\
C_{+} \exp(-\frac{n}{2\sigma^2} (\beta_j - (z_j - \lambda))^2), \text{ if } \beta_j \geq 0 \\
\end{cases}
}

Where, $C_{-} = \exp(\frac{z_j \lambda n}{\sigma^2})$ and $C_{+} = \exp(-\frac{z_j \lambda n}{\sigma^2})$.

What makes this formulation attractive is that a mapping allows tail probabilities from the posterior to be translated to probabilities onto corresponding known normal distributions (i.e. $\N(z_j \pm \lambda, \frac{\sigma^2}{n})$). This allows for numerically stable and efficient sampling to be obtained from the full conditional posteriors.

It was mentioned previously that this solution corresponds to a particular value of $\lam$ and $\hat{\sigma}^2$. Our recommendation is to use cross validation (CV) to select $\lam$ and produce an estimate for $\sigma^2$ and to produce bootstrap confidence intervals corresponding to these values. Specifically when using CV to select $\lambda$ and estimate $\sigma^2$, we recommend using the value of $\lambda$ which minimizes the cross validation error (CVE) and to use CVE as the estimate for $\sigma^2$. We will denote this $\lambda$ as $\lam_{\CV}$. It should be noted that producing an estimate for $\sigma^2$ in this manner implicitly depends on $\lam$, so a new estimate for $\sigma^2$ should be obtained if the value of $\lam$ is changed. Similarly, the partial residuals, calculated with $\hat{\bb}_{-j}(\lambda)$, are also specific to a given value of $\lambda$. As such, when referring to $\r_{j}$ and $\hat{\sigma}^2$ it will always be as a function of $\lam$ which we drop for notational convenience. Our recommendation for using the $\lambda$ value which minimizes CVE is because this generally serves as a good estimate of the true value of $\lambda$ when it is known, a point which  will be covered later in Section~\ref{Sec:Coverage}.

\subsection{Implementation}
\label{Sec:implementation}

With the full conditional posterior now defined for the linear regression lasso, we can provide the algorithm for implementing the three methods. Let $B$ represent the total number of bootstrap datasets generated and let $\boldsymbol{\X}^b$ and $\boldsymbol{\y}^{b}$ refer to the $b^{th}$ bootstrap sample. Similarly, let $\hat{\beta}^b_j$ refer to the estimate for $\beta_j$ from the lasso fit to $\boldsymbol{\X}^b$ and $\boldsymbol{\y}^b$ at a specified value of $\lam$. Then, the methods can be implemented as follows:

\begin{enumerate}
\item Perform CV using the original data to select $\lam$ and estimate $\sigma^2$
\item For b $\in \lbrace 1, \ldots, B \rbrace$:
\begin{enumerate}
\item Obtain a pairs bootstrap sample, $\X^b$ and $\y^b$
\item Fit lasso with $\X^b$ and $\y^b$, obtain $\bbh^b$ and $\boldsymbol{z}^b$ corresponding to the selected value of $\lam$ in (a)
\item For j $\in \lbrace 1, \ldots, p \rbrace$:
	\begin{algorithmic}
	\Switch{method}
    \Case{traditional}
      \Assert{$q_j = \bh_j^b$}
    \EndCase
	  \Case{posterior}
    \Assert{
      $p^* = \Unif(0, 1)$ \\
      \hspace{1.95cm} $q_j = P^{-1}(p^*|\r_{j}^b)$
    }
    \EndCase
    \Case{hybrid}
      \Assertif{$\bh_j^b \neq 0$}{$q_j = \bh_j^b$}
      \Assertelse{
        $p^* = \Unif(0, 1)$ \\
        \hspace{1.8cm} $q_j = P^{-1}(p^*|\r_{j}^b)$
      }
    \EndCase
	\EndSwitch 
	\end{algorithmic}
\item Save $\q$
\end{enumerate}
\item Combine all B $\q$ vectors to obtain a $B \times p$ matrix of bootstrap draws
\item For each $\beta_j$, compute the quantiles for $p_L = \alpha/2$ and $p_U = 1 - \alpha/2$ from the $j^{th}$ column of the draws to produce a final confidence interval estimate with significance level $\alpha$
\end{enumerate}

\pb{\sout{short summary remarks?}} \logan{Would be good to review, make sure remarks are what you had in mind}

The steps defined above are presented primarily for explanatory purposes. They break out each of the three methods into individual cases for clarity, however, in practice, there is a large degree of overlap between methods. All three methods start with a lasso fit on each bootstrapped dataset which provides the draws for the traditional bootstrap. If the draws are obtained for both traditional and posterior, then it is possible to subsequently compute what the draws would be for the hybrid bootstrap. Additionally, the draws can be computed more efficiently than one variable at a time and there are other complexities which are outside the scope of this manuscript. These methods are implemented in the R package \texttt{ncvreg} and the source code can be examined for these additional details.

Furthermore, there are various ways to perform the bootstrap and subsequently obtain confidence intervals. A large body of research exists on various bootstrap implementations, and \cite{Efron1994} in \textit{An Introduction to the Bootstrap} provides an accessible overview on the core body of knowledge. In this manuscript, we use the pairs bootstrap and quantile intervals as this is one of the common ways of obtaining bootstrap confidence intervals and requires few assumptions. That said, our focus in this manuscript is on the differences between the traditional bootstrap and the proposed variants.

\section{Results}
\label{Sec:results}

We begin by examining the coverage behavior of the proposed bootstrap methods in what might be considered the ``ideal'' scenario, where the assumptions of Theorem~\ref{Thm:bcc} are met (Section~\ref{Sec:Coverage}). We then examine how this accuracy is affected by a number of violations to those assumptions to assess each method's robustness (Section~\ref{Sec:robustness}). Finally, we compare the proposed confidence interval methods to other confidence interval approaches for penalized regression that have been proposed in the literature (Sections~\ref{Sec:Ridge}~and~\ref{Sec:Comparison}).

Unless otherwise noted, the nominal coverage rate in all of these experiments is 80\%.

\subsection{Coverage}\label{Sec:Coverage}

We compare the proposed CI methods by showing the coverage of each, both overall and as the magnitude of $\beta$ changes, then present results on the width and bias of the intervals as a way of gaining insight into why the methods result in over or under coverage at various magnitudes of $\beta$.

As suggested by Theorem~\ref{Thm:bcc}, there is reason to expect that the proposed methods have proper average coverage (since they behave like Bayesian intervals) when the penalty (prior) matches the true distribution of the coefficients. For a lasso model, this occurs when each $\beta_j$ independently follows a Laplace (double exponential) distribution. For the simulations in this section, we scaled the coefficients after drawing them so that $\bb^T\bb = 1$. With $\sigma^2=1$ and independent features, this results in a signal-to-noise ratio (SNR) of 1.

We generated 100 independent data sets; for each data set, $B = 1000$ bootstrap iterations were drawn for each of the three methods described in Section~\ref{Sec:methods}. Each dataset was simulated as follows. $\X$ was generated independently from a $N(0, 1)$ with $n = p = 100$. Then, $\y$ was generated as $\y = \X\bb + \bvep$, where $\veps_i \iid N(0, 1)$. The results are shown in Figure~\ref{Fig:laplace}, where the dotted lines represent the average coverage for each method across all coefficients, while the solid lines are smoothed estimates of coverage as a function of $|\beta|$. The black line indicates the nominal coverage rate, which is set to be 80\%.

\begin{figure}[hbtp]
  \begin{center}
  \includegraphics[width=0.65\linewidth]{laplace}
  \caption{\label{Fig:laplace} Results are from the simulation described in Section~\ref{Sec:Coverage}. The fitted curves are from Binomial GAMs fit with coverage being modeled as a smooth function of $|\beta|$. The dashed lines represent the average for each method across all 100 independently generated datasets and the solid black line indicates the nominal coverage rate. The shaded distribution in the background depicts the positive tail of the distribution the $\beta$s were drawn from, a Laplace.}
  \end{center}
\end{figure}

Figure~\ref{Fig:laplace} shows that, as is generally the case when applied to the lasso, the traditional bootstrap coverage is far below the nominal 80\%. At the other extreme, the average coverage of the posterior bootstrap is well above 80\%. Meanwhile, the Hybrid bootstrap method had coverage fairly close to the nominal 80\%.

To understand the coverage behaviors further, it helps to consider what happens when $\abs{\beta}$ is small and when it is large. The traditional bootstrap has low coverage both near and far-from zero. Posterior and Hybrid, on the other hand, display a novel decreasing trend with high coverage rates for values of $\beta$ near zero and lower coverage rates for values of $\beta$ larger in magnitude. This pattern is typical for these methods and is to be expected from intervals arising from procedures that do not attempt to debias. The effect of penalization, a feature of the lasso, can largely explain the patterns observed by understanding the impact on the width and bias of intervals produced, which is where we now turn our attention.

\begin{figure}[hbtp]
  \begin{center}
  \includegraphics[width=0.65\linewidth]{laplace_width_bias}
  \caption{\label{Fig:laplace_width_bias} Results are from the simulation described in Section~\ref{Sec:Coverage}. The fitted curves are from GAMs fit with each respective feature modeled as a smooth function of $|\beta|$. Miss away is defined as having both interval endpoints being larger in magnitude and of the same sign as the truth. A miss towards is defined as having at least one end point between the truth and zero. A Type 3 error is defined as having both interval endpoints be a different sign than the truth, unless the both endpoints are zero in which case it is defined as a miss towards zero.}
  \end{center}
\end{figure}

The curves in Figure~\ref{Fig:laplace_width_bias} are constructed in the same manner as for Figure~\ref{Fig:laplace}, albeit on the respective features of interest. This plot displays interval width (upper left) and three measures of interval bias (bottom, upper right). The top left plot of Figure~\ref{Fig:laplace_width_bias} shows that the interval width tends to increase as $\beta$ increases in magnitude. This behavior is related to the proportion of times a variable is selected to be in the model. Consider for a moment the draws for the traditional bootstrap. When a given $\beta$ is near zero, its estimate will often be shrunk to zero which results in less variable bootstrap draws than for a variable which is always selected. That is, a variable with a larger corresponding true value of $\beta$ will have a wider range of plausible estimates for each bootstrap draw than for a variable with a true value of $\beta$ nearer to zero, all else equal. This effect is most notable for the Traditional bootstrap, as it is the only method which will result in a draw exactly equal to zero when $\bh^b_j = 0$. The proposed methods introduce additional variability in this scenario. This explains why Traditional produces much narrower intervals, especially for values of $\beta$ smaller in magnitude, and largely explains the drastic under coverage observed previously. This is a manifestation of the Epsilon Conundrum as Traditional has a tendency to produce intervals with endpoints equal to zero for $\beta$s near zero. The plot of widths also helps display the underlying mechanism of over coverage observed for Posterior. We can see that compared to the other methods, the intervals for Posterior are significantly wider, regardless of the magnitude of $\beta$. The resulting over coverage suggests then that the intervals are wider than necessary. Lastly note that, compared to the other two methods, the Hybrid bootstrap produces fairly consistent width confidence intervals, regardless of the magnitude of $\beta$.

The bottom plots depict the probability that an interval misses the truth either towards zero (left) or away from zero (right), as a function of the magnitude of $\beta$. If the entire interval was of a different sign than that of the true value (unless both endpoints were 0), we defined this as a Type 3 error, which is presented in the top right plot. Note that these depictions of bias are also functions of interval width as much as interval location. For the most part, as a result of bias, we see that the proposed intervals miss toward zero which accordingly increases as the magnitude of $\beta$ increases. Furthermore, we see the sigmoid shaped pattern for Hybrid and Posterior which is the driver of the observed coverage patterns previously observed. Although Hybrid does tend to miss toward zero more than Posterior, this is mostly a result of decreased width. Similarly as a result of penalization, all methods miss away from zero to a negligible extent. Additionally, it is important to note that as set up, the probability that a method misses towards zero by definition converges to zero as $\beta \rightarrow 0$ (unless the method can produce bounds that are exactly equal to zero, like Traditional). Given that the probability of a miss towards zero goes to zero as $\beta \rightarrow 0$, a Type 3 error near $\alpha / 2$ for $\beta$ near zero would be reasonable, however, the high rates for traditional is concerning. Now that we have a deeper understanding of the intervals' behaviors, we can see that the behavior of the proposed methods, Hybrid and Posterior, can be explained by their tendency to miss towards zero and their respective widths. That is, although they share the same coverage pattern, the main difference between Hybrid and Posterior is that Hybrid produces narrower intervals, as it only samples from the Full Conditional Posterior (the driver of the increased width for Posterior) when $\bh_j^b = 0$. This pattern occurs, because for values of $\beta$ near zero, the effect of the lasso penalty is minimal (in fact, it encourages values near the truth) while the widths are only minorly narrower (relative to large magnitudes of $\beta$) leading to coverage levels near 1. However, as $|\beta|$, and the average effect of the lasso penalty, increases, this eventually leads to lower coverage for values of $\beta$ larger in magnitude, as width does not increase accordingly (nor would we necessarily want it to) leading to a high rate of misses towards zero. The sporadic nature of traditional can largely be understood through penalization induced bias towards zero and through its overly narrow intervals, especially when $\beta$ is small.

We hope the reader is exceedingly convinced that the Traditional bootstrap is a poor choice. Additionally, the posterior isn't bad, it just generally provides intervals significantly wider than that of the Hybrid bootstrap. As will be demonstrated throughout the remainder of this manuscript, when the Hybrid departs from nominal coverage, it generally does so in the direction of over-covering. Thus, in these scenarios, the wider intervals of the Posterior are strictly worse than those produced by the Hybrid. \logan{Could reference supplement of intuition behind hybrid to explain why if kept.} Hybrid produces coverage levels that are reasonably near that of nominal. Note that one can reasonably anticipate how the Posterior bootstrap would behave based on the results for Hybrid. As such, will now focus our attention to how the Hybrid method fares under alternative scenarios. 

\subsection{Robustness}
\label{Sec:robustness}

This section explores a number of scenarios to help understand the behavior of the proposed bootstrapping methods, focusing on the Hybrid method. It begins with a look at the coverage behavior when there is correlation among the predictors. The results in this section also give the first depiction of how the behavior changes with increasing sample size. Next, we consider how Hybrid performs under various distributions of $\beta$ and then follow this up with a brief revisit to the Epsilon Conundrum. This section is rounded out with a look at how the coverage behavior changes with changes in the $\lambda$ used.

\subsubsection{Correlation}
\label{Sec:correlation}

Now, we consider the behavior of the Hybrid bootstrap where the covariates are generated under increasing levels of autoregressive correlation. Other than the addition of correlation, the set up of the simulation for the results displayed in Figure~\ref{Fig:correlation_structure} is the same as for the first simulation described. The boxplots provide the coverages across the 100 simulated datasets for each value of n. The amount of correlation starts at $\rho = 0.4$ in the left plot and increases to $0.6$ in the middle plot and $0.8$ in the right. The coverage behavior remains largely intact for increasing levels of correlation, with the main effect being a slight shift downward in coverage. For greater amounts of correlation the shift downward in coverage is a direct result of increased bias due to the correlation between covariates. So, in general, Hybrid fares well in the presence of correlation as it generally behaves conservatively for smaller sample sizes where the presence of correlation is more of a concern. It takes the highest amount of autoregressive correlation ($\rho = 0.8$) to produce coverage that is noticeably below nominal. For a moderate amount of correlation ($\rho = 0.4$), there is little impact on the coverage of the intervals (see Figure~\ref{Fig:laplace_comparison} for $\rho = 0$ as a comparison).

\begin{figure}[hbtp]
  \begin{center}
  \includegraphics[width=0.6\linewidth]{correlation_structure}
  \caption{\label{Fig:correlation_structure} This figure presents results for the simulation described in Section~\ref{Sec:correlation}. The boxplots are for the coverage rates across 100 simulated datasets for the Hybrid method and across three different levels of autoregressive correlation among the covariates, $\rho = 0.4, 0.6, \text{ and, } 0.8$. For this simulation, p = 100, and the results for each combination of method and level of correlation are presented across three different sample sizes, n = $\frac{1}{2}$p, p, 4p. The horizontal black line provides reference for the 80\% nominal coverage rate.}
  \end{center}
\end{figure}

\subsubsection{Distribution of Beta}\label{Sec:Distribution}

Now that we have seen that the proposed methods do relatively well in the presence of correlation, we turn our attention to performance under various distributions of $\beta$. Given that we have previously observed that the coverage for a given $\beta$ depends on its magnitude, it may be expected that the coverage will vary wildly depending on how $\bb$ is distributed. Table~\ref{Tab:dist_beta} shows the results of $\bb$ generated under 7 alternative distributions. The first column of the table gives a depiction of the distributions and details of these distributions are provided in the notes for the table. The simulation set up was again the same as the initial simulation discussed, but with the respective generating mechanisms for $\bb$. While it is certainly the case that the distribution affects the coverage, the underlying patterns remain the same. That is, for smaller values of n, Hybrid over covers, but, as n increases, coverage approaches the nominal coverage rate. Additionally, while distributions with most of their mass near zero result in higher coverage, the opposite behavior is not observed. That is, even when most of the density is concentrated away from zero, as with the Beta distribution, the desired coverage properties largely remain intact. Lastly, note, that between n = 400 and n = 1000, there is little change in the coverage rates for distributions that were already near 80\% coverage, and for the others (T, Sparse 1, Sparse 2), although slow, coverage still trends towards nominal.

\begin{table}[hbtp]
  \centering
  \input{tab/distribution_table}
  \caption{\label{Tab:dist_beta} Results are from the simulation described in Section~\ref{Sec:Distribution}. The nominal coverage rate is 80\%. The setup is the same as the previous simulations, except with $\bb$ being generated under 7 alternative (to the laplace) distributions and the addition of a fourth sample size, n = 1000. To maintain the specified SNR of 1, $\bb$ is normalized. Prior to normalization, Sparse 1 had $\bb_{1-10} = \pm(0.5, 0.5, 0.5, 1, 2)$ and $\bb_{11-100} = 0$. Sparse 2 had $\bb_{1-30} \sim N(0, 1)$ with the rest equal to zero.  Sparse 3 had $\bb_{1-50} \sim N(0, 1)$ with the rest equal to zero. All distributions of $\bb$ were centered at zero. For normal, laplace, and uniform, after normalization, the original scale is arbitrary. For the T distribution, df was set to 3 and the Beta distribution was generated from Beta(0.1, 0.1) - 0.5. Note that the distribution for Sparse 1 is fixed and that for Sparse 1 and 2 that, like for the Normal, the choice of scale for the non-zero coefficients is arbitrary.}
\end{table}

\subsubsection{Epsilon Conundrum}\label{Sec:Epsilon}


Its natural to now consider the performance of the hybrid method in the scenario that was the main motivator of the method: the Epsilon Conundrum. The simulation set up used to assess the Epsilon Conundrum is essentially just another distribution of $\bb$ values, although one that can prove difficult for methods which fail to take proper precautions. The setup for this simulation was described in Section~\ref{Sec:Difficulties}. As seen in Figure~\ref{Fig:zerosample2}, the Hybrid CIs avoid the downfall we previously observed in Figure~\ref{Fig:traditional} when applying the Traditional bootstrap to the same simulation set up. Although only a subset of the intervals are displayed in Figure~\ref{Fig:zerosample2}, it is evident that the intervals avoid a precipitous drop in coverage for values where $\beta_j$ is near zero by producing intervals that are sufficiently wide and without endpoints not equal to zero. Recall, the Hybrid approaches this issue by sampling from the full conditional posterior when $\hat{\beta}_j^b = 0$. Contrary to Traditional, Hybrid does over cover on average due to the fact that a large majority of $\beta$s are near zero (recalling the tendency of Hybrid to over cover near zero). 

\logan{I added a rough pass at the "issue" of larger values of beta not being covered in the discussion... on to chapter 2!!!}

\begin{figure}[hbtp]
  \begin{center}
  \includegraphics[width=0.6\linewidth]{ec_lasso_hybrid}
  \caption{\label{Fig:zerosample2} 100 datasets were generated with n = p = 100 with 94 of the coefficients having true values randomly assigned to be approximately $\pm$ 3e-10. The other 6 coefficients were larger in comparison ($\approx \pm 0.62, 0.31, 0.15$). The true values are unrounded and were generated s.t. $\bb^T\bb = 1$. The plot provides an example of 20 of the intervals produced by applying the Hybrid bootstrapping procedure to the lasso for a single dataset, randomly selected among the 100 generated. The points indicate point estimates from the lasso for the corresponding intervals and the red circles indicate the true values. The average coverage across all 100 datasets was 93.6\%. Nominal coverage was set to be 80\%. See Figure~\ref{Fig:traditional} for a comparison to how the traditional bootstrap method performs.}
  \end{center}
\end{figure}


\subsubsection{Selection of \texorpdfstring{$\lambda$}{lambda}} \label{Sec:lambda}

Since CV is non-deterministic, $\lambda_{\CV}$ likely takes on a range of values depending on the seed set, so there maybe be question about what happens if the value of $\lambda$ is changed. The data in Figure~\ref{Fig:beta_lambda_heatmap_laplace} comes from a simulation that addresses just that. The data generating mechanism for this simulation is identical to that for Section~\ref{Sec:Coverage}. In this simulation, however, $\lambda$ was evenly distributed on the $\log_{10}$ scale from $\lam_{\max}$ to $\lam_{\min} = \lam_{\max} * 0.001$. At each value, confidence intervals were obtained and coverage was recorded. Note that at each value of $\lam$, $\sigma^2$ was re-estimated using the CVE at that value of $\lam$. This was repeated 100 times, then a gam was fit to provide a smooth estimate of the coverage rate by $\lambda$ and $|\beta|$. Relative coverage is defined here as the estimated coverage rate minus the nominal coverage rate. The x-axis for $\lambda$ is presented relative to $\lam_{\max}$ and the black lines delineate the range of $\lam_{\CV}$ over the 100 simulations. The red line indicates the average $\lambda_{\CV}$ while the blue line represents the value of $\lambda$ which provided coverage closest to that of nominal.

\begin{figure}[hbtp]
  \begin{center}
  \includegraphics[width=0.6\linewidth]{beta_lambda_heatmap_laplace.png}
  \caption{\label{Fig:beta_lambda_heatmap_laplace} The heatmap displays results for the simulation described in Section~\ref{Sec:lambda}. 100 datasets were generated as with previous simulations with n = p = 100, with $\beta \sim Laplace$ and $\X$ generated under independence from a $N(0,1)$. Then, each method was fit across a range of $\lam$ values and confidence intervals and their coverages were obtained and then used to fit a gam to estimate the coverage as a smooth function of the $|\beta|$ and $\lam$. The x-axis for $\lambda$ is presented relative to $\lam_{\max}$ and the black lines delineate the range of $\lam_{\CV}$ over the 100 simulations. The red line represents the relative average CV selected value of $\lam$. The blue line indicates the relative $\lam$ value that provided coverage closest to the 80\% nominal coverage rate.}
  \end{center}
\end{figure}

\logan{Should we add back the true lambda value??}

We will focus on the range of $\lam_{\CV}$ as this is where the most interesting pattern occurs and which falls under our recommendation for selecting $\lam$. Depending on the value of $\lam$, $\beta$s closer to zero see moderate amounts of over coverage which decreases, eventually leading to under coverage, as the magnitude of $\beta$ increases. The $|\beta|$ where this transition occurs (and at which coverage of a specific value equals nominal) varies considerably over the range of $\lam$. At $\lam_{\max}$, this transition occurs at a relatively small $|\beta|$. As the value of $\lam$ decreases from $\lam_{\max}$ this transition occurs at increasingly large values of $\beta$ until all values of $\beta$ have an estimated coverage rate at or above that of nominal. To obtain an average coverage rate near that of nominal, $\lam$ needs to be selected such that over coverage for smaller $|\beta|$s and under coverage for larger $|\beta|$s is balanced. The blue line serves as a general representation of where this tradeoff is met. In this scenario, and in general, $\lam_{\CV}$ does a reasonable job at providing such a balance across the distribution of $\beta$ values. However, it should be no surprise by this point that the average $\lam_{\CV}$ (the red line) is to the right of the relative $\lambda$ value that provided closest to nominal coverage (the blue line), as we have seen previously that the $\lambda$ which minimizes CVE tends to produce over coverage when n = p. That is, as $\lam$ is decreased from the blue line, the average coverage increases above nominal as a greater proportion of the distribution of $\bb$ experiences over coverage.

It should be reemphasized that, as implemented, the estimate of $\sigma^2$ depends on the choice of $\lambda$. That said, by definition, the estimates are increasing in either direction of $\lambda_{\CV}$. So, the general pattern previously described is likely more due to the direct effects of $\lambda$ as opposed to an indirect effect of the estimate of $\sigma^2$, at least over the range of $\lam_{\CV}$ and as estimated using CVE. However, the over coverage at $\lam_{\CV}$ is more of a result of the over estimate of $\sigma^2$, see Supplement~\ref{Sup:B}. Briefly, setting $\lam$ equal to its true value (which we know since $\beta \sim Laplace$), while still estimating $\sigma^2$ using CVE but just at the true value of $\lam$, leads to coverage very similar to that when $\lam$ is selected using CV, likely because $\lam_{\CV}$ was generally near the true value of $\lam$ in the simulations considered. In contrast, if $\sigma^2$ is also set to its true value, coverage was very near nominal regardless of the sample size. Thus, alternative methods to estimating $\sigma^2$ could be considered to reduce the degree of over coverage, a topic which is further explored in the Discussion.

Finally, the balance of over and under coverage, of course, depends on the true distribution of $\bb$ (recall that here it is Laplace). However, as we saw in Section~\ref{Sec:Distribution}, $\lam_{\CV}$ generally does well regardless of the distribution of $\beta$, with average coverage behavior being similar across various distributions of $\beta$, so a similar pattern would be expected for other distributions.

\subsection{Comparison to Ridge Regression CIs}\label{Sec:Ridge}

We now turn attention to a comparison of the intervals produced by Hybrid to those produced by Ridge regression. In this example again n = p = 100, however, only one $\beta$ is non-zero. That is, $\beta_{A_1} = 1$ and $\beta_{B_1}, \beta_{N_1}, \ldots, \beta_{N_{98}} = 0$. Additionally, the data are simulated such that $\rho(\beta_{A_1}, \beta_{B_1}) = .99$ but all of the N (noise) $\beta$s are uncorrelated with $\beta_{A_1}, \beta_{B_1}$, and each other.

100 datasets were generated in this manner and each method was applied and respective confidence intervals obtained.

Figure~\ref{Fig:highcorr} depicts the results from the simulation with the plots on the left giving box plots for the lower (red) and upper (blue) bounds across the 100 datasets for 3 variables, $A_1$, $B_1$, and $N_1$. On the right, confidence intervals for the first 20 variables for a randomly selected example from the 100 simulated datasets is displayed.

Focusing on $A_1$ and $B_1$, what we would hope to see is wide intervals centered somewhere near 0.5. Although $A_1$ truly is the signal variable, its high correlation with $B_1$ should produce a large amount of uncertainty about which variable (if not both) contain signal. Ridge, however, fails in this respect in that the intervals are narrow, with little distinguishable difference in width between $A_1$ and $B_1$ and the noise variables. On the other hand, Hybrid displays more desirable behavior. The uncertainty entangled in $A_1$ and $B_1$ is reflected with wider intervals. Additionally, intervals for $A_1$ usually do not contain 0 whereas the intervals for $B_1$ contains 0 over 40\% of the time. Ridge, on the other hand, produces no intervals that contain zero for $B_1$ and on the flip side hardly ever produce intervals with upper bounds above $0.5$ for $A_1$. Additionally for hybrid, overall, there is a clear shift in the intervals for $A_1$ compared to $B_1$ suggesting that even with very high correlation, the Hybrid bootstrap often attributed more of the signal to $A_1$, which is not the case with Ridge.

\begin{figure}[hbtp]
  \begin{center}
  \includegraphics[width=0.6\linewidth]{highcorr}
  \caption{\label{Fig:highcorr} Provides results for simulation described in Section~\ref{Sec:Ridge}. The right plots show a single example of a intervals produced by Ridge (top), Elastic Net (middle) and the Hybrid bootstrap (bottom) from one (randomly selected) of the 100 datasets for the first 20 variables. The left plot summarizes the resulting CIs for the variables $A_1$, $B_1$, and $N_1$ across the 100 simulations. The blue box plots provide the distributions for the upper bounds and the red boxplots for the lower bounds. Note, nominal coverage for this simulation was 80\%.}
  \end{center}
\end{figure}

Additionally, between the results for Ridge and the results for lasso are results for Hybrid intervals for Elastic Net with $\alpha = 0.8$. These intervals are obtained by using a data augmentation method that allows Elastic net to be fit using the algorithms for fitting lasso. The main point of including this is a demonstration that the methods implemented in \texttt{ncvreg} are not strictly limited to the lasso penalty.

\subsection{Comparison to Other HDCI Methods}\label{Sec:Comparison}

There are few other methods for obtaining intervals for the lasso and even fewer with implementation in companion R packages to allow for easy usage and comparison. Two that we were able to identify were Selective Inference (SelInf) introduced by \cite{LeeEtAl2016} and implemented in \texttt{selectiveInference} and Bootstrap Lasso Projection (BLP) introduced by \cite{Dezeure2017} and implemented in \texttt{hdi}.

Note that both of these methods have markedly different behavior than the Hybrid method proposed here and also from each other. This can be understood by reviewing the methodological reasoning behind the these two alternatives even without getting into any results. To review, BLP proposed by \cite{Dezeure2017} proposed a method for bootstrapping the Low Dimension Projection Estimator (LDPE) proposed by \cite{ZhangZhang2014}. LDPE provides a method to debias the original point estimates from a lasso fit to facilitate more traditional forms of inference. LDPE is also known as the de-biased or de-sparsified Lasso. The process of debiasing is involved and makes it ambiguous how the results relate to a specific lasso model. Alternatively, SelInf aims to account for the uncertainty in model selection by conditioning on the selected model. SelInf doesn't directly correct for the bias introduced by penalization but because of its formulation does only provide intervals for variables selected and generally provides consistent coverage regardless of the magnitude of $\beta$, something we would expect from a debiased method. As a result and as we will see in Section~\ref{Sec:RDA}, the intervals produced by these alternatives are often questionable in relation to their corresponding point estimates, especially when p is greater than n, which leads to an idea we will expand on that the Hybrid intervals are valuable in the sense that they remain more faithful to the lasso model fit.

\subsubsection{Similation Study}

\begin{figure}[hbtp]
  \begin{center}
  \includegraphics[width=0.65\linewidth]{laplace_comparison}
  \caption{\label{Fig:laplace_comparison} Results are from the simulation described in Section~\ref{Sec:Comparison} and identical in setup to that of Section~\ref{Sec:Coverage}. The fitted curves are from Binomial GAMs fit with coverage being modeled as a smooth function of $|\beta|$. The dashed lines represent the average for each method across all 100 independently generated datasets and the solid black line indicates the nominal coverage rate. The shaded distribution in the background depicts the positive tail of the distribution the $\beta$s were drawn from, a Laplace.}
  \end{center}
\end{figure}

This simulation study compares the performance of the methods using their default parameters. With that said, as implemented in \texttt{hdi}, there is no way to specify the $\lambda$ used for BLP, which defaults to the 1-SE solution from \texttt{cv.glmnet}. Additionally, by default, $\lam$ is also re-selected for each bootstrap draw. For the simulation study, we use this ``out of the box'' setup, however, it was unsatisfactory for the comparisons in the subsequent data analyses. For the data analyses, we forked the \texttt{hdi} repository and adjusted it to allow for the specification of $\lambda$. Specifically, to allow for the use of the $\lambda$ which minimizes CVE. We did attempt to use this version in the simulation study, but this highlighted a limitation for BLP which made such usage infeasible under the desired setup. Specifically, BLP only works when the number of non-zero coefficients is less than (not equal to) n, something that is more likely to occur for the 1-SE $\lambda$ than for the $\lambda$ which minimizes CVE. As the simulation is set up, this proved prohibitive as more often than not when n was small relative to p, the method resulted in an error. Although it would be ideal to have the option to compare the methods under the same value of $\lambda$, there is something to be said for comparing the performance of the methods as closely as possible to what a user would experience if they started using one of the methods with its default arguments. However, it must be emphasized that this means the $\lambda$ values used for BLP vary greatly from those used for SelInf and Hybrid, which both use a single $\lambda$ for each dataset with the $\lambda$ used being the value which minimizes CVE from \texttt{cv.glmnet} and \texttt{cv.ncvreg}, respectively, applied to the original dataset\footnote{\texttt{cv.glmnet} and \texttt{cv.ncvreg} generally select a similar value for $\lambda$. \texttt{cv.glmnet} was used for SelInf since \texttt{cv.glmnet} is what is used in examples for the \texttt{selectiveInference} package and \texttt{cv.ncvreg} was used for Hybrid since the Hybrid bootstrap is implemented in \texttt{ncvreg}.}. HDI, on the other hand, has B $\lambda$ values (not returned) chosen as the 1-SE $\lambda$ from \texttt{cv.glmnet} applied to each of the B bootstrap draws from the original dataset.

\begin{figure}[hbtp]
  \begin{center}
  \includegraphics[width=0.65\linewidth]{laplace_other}
  \caption{\label{Fig:laplace_other} Results are from the simulation described in Section~\ref{Sec:Comparison}. Each plot provides corresponding results for each of BLP, Hybrid, and SelfInf for three different sample sizes. The top plot provides boxplots of coverages, the middle plot provides boxplots of the median widths, and the bottom plot provides boxplots of the run times, all across the 100 simulated datasets. Note that the plots for median width and for run time both have y-axes on the $\log_{10}$ scale.}
  \end{center}
\end{figure}

The simulation results presented here are from a set up identical to that described in Section~\ref{Sec:Coverage}. In fact, the results for Hybrid are the same as those used for the earlier figures, just displayed differently. Referencing Figure~\ref{Fig:laplace_comparison}, both BLP and SelInf initially appear to perform strikingly well. Both have coverage near that of nominal and lack the coverage pattern seen with the Hybrid method, instead providing consistent coverage regardless of the magnitude of $\beta$. However, Figure~\ref{Fig:laplace_other} tells a different story. First, although BLP provides average coverage rates the closest to $80\%$, there were a number of cases where the coverage dipped significantly. Potentially more concerning is that when $n = 400$, the coverage noticeably drops below 80\%. Although the lasso is generally thought of as a model for high dimensional data, it is used across the entire spectrum of datasets, so it would be preferred to see convergence towards the nominal rate of coverage. The behavior for the Hybrid Bootstrap has been covered previously, specifically that it generally over covers when $n < p$ but has coverage near nominal as n increases above p. Additionally, although not immune to under coverage, it performed more reliably than the other two methods. Before moving onto the results for SelInf, there are a couple numbers not in Figure~\ref{Fig:laplace_comparison} which are also important to consider and which are provided in Table~\ref{Tab:selective_inference}. The first column gives the average number of variables included on average across the simulations (that succeeded) which also represents the number of variables on average that confidence intervals were provided for. The second column indicates that when n = 50, 20 of the 100 simulations errored out, with only 6 erroring out when n = 100, and none when n = 400. It is this subset (represented by the first and second columns) that is included in the coverage plot for SelInf. Although on average SelInf's coverage is near nominal, the coverages for individual simulations are sporadic, ranging from 0 to 1. The fact that SelInf does not produce intervals for all variables also helps explain the sporadic coverage behavior, since its coverage values are averaged over fewer variables. Accordingly, there are also a considerable number of iterations that had coverage below 50\%, an issue which is not remedied even with larger values of n. So, even if on average SelInf looks good, the underlying behavior is much less desirable.

\begin{table}[hb]
  \centering
  \begin{tabular}{ccccc}
  \hline
  & Variables & \multicolumn{3}{c}{Simulations} \\
  \cmidrule(lr){2-2}\cmidrule(lr){3-5}
  n & \# Included on Average & \# Succeeded & \# Non-finite Median Width & \# Any Non-finite Width \\
  \hline
  50 & 18.5 & 80 & 18 & 40 \\
  100 & 30.4 & 94 & 12 & 63 \\
  400 & 70.1 & 100 & 3 & 55 \\
  \hline
  \end{tabular}
  \caption{Additional information on the results for SelInf in the simulation described in Section~\ref{Sec:Comparison}. The first column indicates the average number of variables included (non-zero) in the lasso model at a given sample size. Note, this column also applies to BLP and Hybrid. The second column indicates the number of simulations that actually succeeded. A majority of the errors occur due to too large of a $\lambda$ value being selected using CV (causing all coefficients to equal zero), however, a failure to satisfy the polyhedral constraint also was a source of some errors. The third and fourth column indicate the number of simulations, among those that succeeded, that had a non-finite median width or any interval with an infinite width, respectively.}
  \label{Tab:selective_inference}
\end{table}

Staying with SelInf but directing our attention towards the interval widths, the concern surrounding SelInf only grows. Referring to column 3 of Table~\ref{Tab:selective_inference}, of the 80 simulations that suceeded for n = 50, the median width of the intervals produced (from the variables included in the model, column 1) was infinite for 18 of the simulations. For n = 100, 12 of the 94 simulations had infinite median widths. By n = 400, at which point one would hope this issue was eliminated, still 3 of the 100 simulations had infinite median widths. Column 4 gives the number of simulations with at least one interval with an infinite width. Additionally, we can see in the second plot of Figure~\ref{Fig:laplace_other}, which excludes the simulations with infinite median widths, even when the medians were finite, they were nearly always extremely wide, even for n = 400 (note the y-axis is on the log scale). This behavior was also observed when applying the method to real data sets which will be covered in Section~\ref{Sec:RDA}. BLP and Hybrid on the other hand produce intervals which are more similar in width although BLP does tend to produce wider intervals and with a bit more variability. As expected, the interval widths for three methods decrease with sample size.

The third plot of Figure~\ref{Fig:laplace_other} provides boxplots of the run times for each of the methods. The runtime differs considerably between the methods with SelInf being the fastest and BLP being by far the slowest with about an order of magnitude difference separating the methods respectively (note the y-axis is on the log scale). Additionally with BLP, there is an odd non-monotonic behavior which we looked into briefly but could not find an explanation for. This behavior occurred in all reruns of the same simulation. Hybrid and SelInf both have a monotonically increasing relationship with sample size, although Hybrid is more affected. In our testing, speed was not a concern for SelInf, was noticeable for Hybrid, and prohibitive for BLP which will be returned to in Section~\ref{Sec:Scheetz2006}. BLP is particularly slow with its default arguments because of the reselection of $\lambda$ (using CV) for each iteration of the bootstrap.

\logan{I could see the reader questioning... well if you overcame a similar issue (just noted how many errors occured) for SelInf... why not do the same for BLP? The issue for BLP however is far worse because an error can potentially arrise in any bootstrap iteration whereas SelInf has to work just once, for the original dataset. That is, BLP fails much more miserably.}

\logan{I did start to track down the error for BLP and it seems to be due to a NaN showing up where they weren't expecting and it not being handled properly. However, I think it would take a considerable amount of time to find the root of the error and potentially fix it... which I could do but not sure if it is worth the effort.}

\section{Real Data Analysis}\label{Sec:RDA}

We conclude the results section by considering the intervals produced by the Hybrid and Debiased bootstrap applied to two datasets: \texttt{whoari} (World Health Organization study on acute respiratory illnesses) and \texttt{Scheetz2006} (Gene expression in the mammalian eye). These two datasets sit on opposite ends of the spectrum in terms of dimensionality. \texttt{whoari} contains 816 observations and 66 features while \texttt{Scheetz2006} contains just 120 observations but with 18975 features.

In this section, we also compare the intervals produced by the proposed methods to those of BLP and SelInf. In this comparison, we wanted the results for the real data analysis to correspond directly back to a single set of point estimates for all three methods in order to put a clear emphasis on how the intervals relate to the corresponding estimates from the lasso fit using a selected value of $\lambda$. The $\lambda$ of interest is the value which minimizes CVE, selected here using \texttt{cv.glmnet}. As previously discussed, this is not an issue for Hybrid or SelInf, which both allow $\lambda$ to be specified, but does require adjustments for BLP\footnote{The unmodified results for BLP are provided in a Supplement~\ref{Sup:C}}. Since \texttt{hdi} was not set up with the flexibility to allow for the specification of $\lambda$, we forked the \texttt{hdi} repo and made the necessary modifications. Recall, without making the adjustment, the option is fixed to be the 1-SE solution from \texttt{cv.glmnet} (although this is not indicated in \texttt{hdi}'s documentation). The second adjustment relative to the ``out of the box'' set up is supported in the arguments for BLP. This adjustment is to set \texttt{boot.shortcut = TRUE}. From hdi's documentation, if \texttt{boot.shortcut = TRUE}, ``the lasso is not re-tuned for each bootstrap iteration, but it uses the tuning parameter computed on the original data instead.'' With these modifications, then, BLP has the same behavior as the other methods. Lastly, it should not be ignored that BLP does provide its own estimates. However, the connection to the the original lasso fit is obscured, so for the purpose of this comparison, we use the the same point estimates across the three methods.

\subsection{World Health Organization study on acute respiratory illnesses (whoari)}\label{Sec:whoari}

\begin{figure}[hbtp]
  \begin{center}
  \includegraphics[width=0.6\linewidth]{comparison_data}
  \caption{\label{Fig:comparison_data_whoari} Confidence intervals produced by three different methods for all 66 variables in the \texttt{whoari} dataset described in Section~\ref{Sec:whoari}}
  \end{center}
\end{figure}

The \texttt{whoari} dataset comes from the study ``Development of a clinical prediction model for an ordinal outcome: the World Health Organization multicentre study of clinical signs and etiological agents of pneumonia, sepis and meningitis in young infants'' written by \cite{Harrell1998}. The study considered a few acute illness in young infants across several countries, and the dataset used here is a subset of 816 infants who presented with pneumonia in the country Ethiopia. As alluded to in the title, collection of this data was done with the intention of building a prediction model to assess the severity of an infant presenting with a serious infection, which represents the main cause of morbidity and mortality in these developing countries for infants under 3 months of age. Diagnosis of severity is a difficult task, and developing rules for grading the severity of disease is important for prompt delivery of treatment for those who need it while also avoiding unnecessarily and costly treatments where possible. The outcome considered here is ordinal (taking on a number from 1 - 5), however, for simplicity we treat the outcome as continuous and feel that the results are reasonable, at least for a comparison of the three methods under consideration. The variables collected contain information on vital signs, family history, and clinical observations and represent a range of data types from binary to ordinal to continuous. With $N \approx 10p$, this dataset is not necessarily high dimensional, but sits on the edge of where classical methods and their resulting inferences may be questionable. Additionally, the use of a model that produces sparsity is beneficial both for interpretation and for ultimately determining factors for assessment in practice, where obtaining predictions from a model may be prohibitive.

Figure~\ref{Fig:comparison_data_whoari} provides the confidence intervals from each of the three methods along with corresponding point estimates. Recall, the point estimates are the same across all three methods and come from the lasso fit on the original data with $\lambda$ selected using \texttt{cv.glmnet}. It is important to emphasize that the range of the x-axis is different for each of the plots corresponding to the three methods. The Hybrid produces the narrowest intervals and SelInf produces by far the widest. Despite the difference in widths, Hybrid and BLP share similar patterns, however, the conclusions that might be drawn could conceivably be quite varied. This may most easily be observed by considering a common point of interest: whether or not an interval includes zero. BLP produces 3 intervals that do not contain zero, SelInf produces 13 that do not contain zero, and Hybrid produces 21 that do not contain zero. SelInf producing 13 intervals not containing zero is only part of the picture, since it is also important to note SelInf selects (and provides intervals for) 37 of the 66 variables and only produces one interval here with an infinite bound (for \texttt{abb}). That said, there are a number of intervals that are unreasonably wide and we believe it would be difficult to provide a convincing interpretation for the intervals produced by SelInf. The comparison between Hybrid and HDI is more interesting since, visually, they appear similar. The patterns observed here are similar to those in the preceding simulation study. As previously mentioned, the intervals from Hybrid are narrower on average. Additionally, while Hybrid's intervals, relative to the point estimates, tend to be skewed towards zero or symmetric, BLP's intervals are often skewed away from zero as a result of debiasing. It appears then, that BLP's higher coverage at larger values of $\beta$, as seen in the simulation study, is likely due to a combination of increased width and the skewness induced by debiasing.  

\subsection{Gene expression in the mammalian eye (Scheetz2006)}\label{Sec:Scheetz2006}

\begin{figure}[hbtp]
  \begin{center}
  \includegraphics[width=0.7\linewidth]{comparison_data_scheetz}
  \caption{\label{Fig:comparison_data_scheetz} Confidence intervals produced by three different methods for the 20 variables with the largest absolute point estimates in the \texttt{Scheetz2006} dataset described in Section~\ref{Sec:Scheetz2006}}
  \end{center}
\end{figure}


The \texttt{Scheetz2006} data was obtained from the study ``Regulation of gene expression in the mammalian eye and its relevance to eye disease'', written by \cite{Scheetz2006}. This study involved measuring the RNA levels from the eyes of 120 rats. Of 31000 different probes used, 18976 were detected at a sufficient level to be considered ``expressed''. For this analysis we treat one of the genes, Trim32, as the outcome since it is known to be linked to the genetic disorder Bardet-Biedl Syndrome (BBS). The remaining 18975 genes are used as covariates with the goal of determining other genes which may have expression correlated with Trim32 and thus may also contribute to BBS. 

In the simulation study, we saw that when p is large relative to n, SelInf's difficulties are amplified. Applied to \texttt{Sheetz2006}, where $p > 100n$, the issues are unignorable. SelInf provides intervals for 66 of the 18975 features however, every single one of them has a lower or upper bound that is infinite. Additionally, for the bounds that are finite, they are all also extremely large compared to their respective point estimates and in comparison to the intervals produced by Hybrid and BLP. Additionally, none of the intervals contain zero. Potentially even more odd is that of the 66 intervals, 62 of them were completely of the opposite sign as the corresponding estimate. 

Like with \texttt{whoari}, Hybrid and BLP produce similar intervals, however with their characteristic differences more prominent due to the dimensionality of the dataset. As such, the intervals of Hybrid are more drawn towards zero. Alternatively, however, the intervals for BLP often do not contain the lasso point estimate or barely do, a point that will be considered further in the discussion.

Despite these differences, depending on the perspective taken, there is not a large discrepancy for the variables deemed significant by the two methods. BLP produces 6 intervals which exclude zero, while Hybrid produces 3. This is twice as many, but in the grand scheme of nearly twenty-thousand variables, this is a relatively minor difference. Both methods have intervals not containing zero for \texttt{1389910\_at}, \texttt{1378319\_at}, and \texttt{1385395\_at}, while BLP produces intervals which do not contain zero for three additional genes.

That said, we emphasize again that a user is not able to obtain these results from the implementation in \texttt{hdi}. Additionally, even with \texttt{boot.shortcut = TRUE} (where $\lambda$ is not reselected for each bootstrap iteration), on a MacBook Pro with 16 GB of RAM and an Apple M1 Pro chip, BLP took over 6 hours to run. This is not all that surprising, however, since LDPE is already computationally expensive, and while bootstrapping provides additional benefits, this only adds to the computational burden. That said, for large p, the expense of bootstrapping is still overshadowed by the p lasso regressions needed for $X_j$ on $\X_{-j}$. \logan{Should update with run on HPC / with more specific time details.}

\section{Discussion}

Since the main method suggested here falls somewhere between the Traditional Bootstrap and the Posterior Bootstrap, we propose the full name as the Posterior Adjusted Traditional Hybrid (PATH) Bootstrap.

The under coverage of coefficients for larger values of $\beta$, admittedly the coefficients that are often of most interest, may at first glance appear to be a major flaw of the bootstrap alternative proposed here. However, there are two observations we believe suggest otherwise. 

The first redirects attention back to Theorem~\ref{Thm:bcc}. The coverage properties of the bootstrapped lasso intervals here mimic closely the behavior of the intervals produced by a corresponding Bayesian analysis. Average coverage, weighted by the likely distribution of $\beta$ values is maintained near that of nominal rather than focusing on long run coverage rates at all values of $\beta$. Would one really dismiss a Bayesian analysis on this same account? \logan{I am wondering if spending some time here elaborating on a 2x2 table of selection uncertainty vs. significance on one side and overall and single on the other would be convincing. I.e. Hybrid falls in that overall significance category? Obviously would put into much more elegant terms.}

The second observation is that this behavior is directly related to the properties of the lasso. If there is issue with the coverage of larger coefficients, then we might suggest that the lasso is not the model the practitioner is looking for rather than dismiss the intervals themselves. Alternatively, one might consider other penalities which reduce the bias introduced. One alternative would be to consider the same proposed bootstrap methods but applied to the Minimax Concave Penalty (MCP). The MCP penalty closely resembles that of the lasso near zero but eventually levels out to a constant penalty for larger values of $\beta$ unlike the lasso which applies a penalty proportional to the magnitude of $\beta$. Further exploration of the behavior under alternative penalties is outside the scope of this manuscript, but something that warrants further consideration, especially in contrast to methods that explicitly attempt to debias during inference.

This leads us back to the observations seen in Section~\ref{Sec:Scheetz2006}. The remedy, admittedly a natural one, to this issue would be to provide intervals that are debiased. However, as we saw with BLP, this can lead to intervals which coincide with the original lasso estimates. To be fair, this was not something that the authors of BLP were attempting to do. BLP is essentially producing an alternative model to the lasso, the de-biased or de-sparsified lasso. This should be emphasized. The intervals produced by BLP (and by other debiasing methods) are producing inference on $\beta$ but not inference for a specific lasso fit. This is why earlier in the manuscript we alluded that methods like Hybrid that take an alternative perspective on coverage produce intervals that are faithful to a specific lasso model.

A fair question might be how to interpret intervals which do not have constant coverage. However, we believe the best advice would to be to treat them no differently than any other interval. In such cases, the interpretation will be conservative. If the true value of $\beta$ is small, the coverage is likely much higher than that of nominal. On the other hand, if $\beta$ is large, the coverage is likely below nominal, but the interval likely understates the effect.

The estimate for $\sigma^2$ used throughout this manuscript was based on the CVE. Alternative estimates would affect the resulting width of the Posterior and PATH bootstrap intervals which certainly would impact coverage. We do not explore the impact of alternative methods for estimating $\sigma^2$ here, but \cite{Reid2016} provides a comprehensive overview for estimating error variance for the lasso. Given that CVE generally over estimates $\sigma^2$, an estimate with less bias may result in more desirable coverage behavior for smaller sample sizes. Regardless, we have found that using CVE provides a level of robustness as it does a good job indicating the uncertainty around the estimates.

Further work could extend or propose alternative methods that are more closely calibrated to the nominal coverage rates at any value of lambda.

\section{Removed}

\subsection{Intuition behind the hybrid bootstrap}

Given that the hybrid bootstrap is the main focus of this manuscript, it will be worth while to briefly consider the intuition driving this proposed sampling method. Recall, potentially the most striking issue with the traditional bootstrap is that it can produce intervals which are singleton at 0. This occurs when $\hat{\beta}_j^b = 0$ for at least $B * (1 - \alpha)$ draws. To address this issue, then, adjustment is only needed when $\hat{\beta}_j^{b} = 0$. 

Now, we consider the behavior of the Hybrid interval from a logical perspective. For a moment, hold p constant. As n increases and we let $\lambda$ get selected by CV, this method will converge to that of applying the traditional bootstrap to a classical linear regression model. This is the case because with increasing n, $\lambda_{CV} \rightarrow 0$. On the other hand, as n decreases $\lambda_{CV} \rightarrow \lambda_{max}$ and $\hat{\boldsymbol{\beta}}\rightarrow \boldsymbol{0}$. On this end of the extreme, bootstrap samples are then drawn from the marginal posterior. The coverage behavior is well understood for the bootstrap applied to linear regression as n increases. Thus, by only sampling from the posterior when $\hat{\beta}_j^b = 0$, we minimize the addition of unnecessary variability as n increases. The behavior on the other end of the extreme is relatively less understood compared to the asymptotics of the traditional bootstrap. However, we argue that leveraging the posterior lends itself to reasonable behavior especially when the alternative would be confidence intervals which are all identical to zero. Such intervals, one could argue, would necessarily have 0\% coverage. Conversely, as we will see, with smaller values of n, the Hybrid tends to lead us to wider confidence intervals which over rather than under cover relative to nominal.

\subsection{Space Requirements}

As implemented, a clear limitation of this method is that it takes a numeric matrix size $B \times p$. With $B = 1000$, the sample matrix gets large enough to cause memory concerns even when $p$ is on the order of $1e5$. For many datasets, this is likely not of concern. However, given that lasso is often used for datasets where $p$ is large, it is clearly not an edge case where $p$ is of this or larger order. One could reduce the size of the sample matrix by reducing the number of draws, but this is unsatisfactory and produces little additional leeway for what seems like a unacceptable sacrifice. This is an ongoing are of interest and a valuable areas of research for any high dimensional bootstrap methods. One solution would be using incremental quantile estimation such as the method introduced by \cite{Tierney1983}. An alternative option would be deriving a method with similar properties but which relies on samples statistics that are relatively straightforward to update over sequential samples (i.e. mean and variance).
