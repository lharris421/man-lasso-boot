\section{Introduction}

Since its unveiling by \cite{Tibshirani1996}, the Lasso (Least Absolute Shrinkage and Selection Operator) has been a popular model of choice, even in cases which would not necessarily be classified as high-dimensional. Its ability to both do variable selection and estimation lends itself to a particular ease. Additionally, in scenarios where both predictive accuracy and interpretability are desired, the Lasso excels. This is particularly useful for models with a large number of covariates and the underlying model is believed to be sparse but where the exact subset of significant predictors is unknown. In such scenarios, the Lasso helps to identify the most relevant variables leading to a more parsimonious and hence interpretable model. \cite{HTF2009} provide an overview of the Lasso's performance in various settings, demonstrating its optimality in various examples in which the situations and results desired are similar to those previously mentioned. With that said, inference for the lasso has proven to be a difficult task, especially when one desires to obtain proper confidence intervals.

The challenges of constructing confidence intervals have lead to various alternative approaches in the literature. For example, the concept of post-selection inference as discussed in \cite{LeeEtAl2016}, which aims to account for the uncertainty in model selection by conditioning on the selected model or \cite{ZhangZhang2014} which focuses on a desparsified approach to correct the bias of the lasso and facilitate more traditional forms of inference. Some focus has been placed on using the bootstrap to produce confidence intervals, however, the conventional opinion since \cite{Chatterjee2010} is that the traditional use of bootstrapping to produce confidence intervals for penalized regression is suboptimal. The suboptimality arises directly due to the inherent bias and sparsity of estimates produced by soft thresholding.

Thus, we take a different perspective and raise a new question to conventional wisdom: does the bootstrap not work or do we need to rethink the properties of confidence intervals in high dimensional settings? We find that it's a combination of both. There does need to be a methodological fix, but also we need to rethink high dimensional confidence intervals (HDCIs).

\section{Difficulties of Confidence Intervals for lasso}\label{Sec:Difficulties}

We see the difficulty of constructing HDCIs arising from two problems when proposing to use bootstrapping for penalized regression, we will refer to these obstacles as,

\begin{enumerate}
\item the Bias Tradeoff, and
\item the Epsilon Conundrum.
\end{enumerate}

The Bias Tradeoff refers to the inherent bias introduced by penalization in order to be able to fit over-saturated models. This penalization leads to coverages higher than nominal coverage rates for coefficients with true values at or very near zero and leads to lower coverage when coefficients are non-zero. We argue that coverage in the traditional sense is too ridged of a paradigm to apply to penalized regression inference. Instead, we offer a different perspective and argue that the impact that penalization induced bias has is an inherent feature of penalized regression rather than a flaw. Since high dimensional problems often necessitate such an alternate approach, we offer guidance on how to interpret the confidence intervals for lasso as presented in this paper. Additionally, we compare the proposed method to two other inferential methods which take a different perspective.

Before going further, it will be helpful to specify the behavior of the intervals we are interested in. Specifically, we are interest in intervals that are faithful to the model being fit. This inherently excludes most intervals that attempt to debias. This restriction imposes a behavior on the intervals. Since, bias being introduced into the model has varying effects on coefficients of increasing magitude, this implies that interval behavior will vary with the magnitude of $\beta$.

By limiting focus to intervals that are true to the model fit, we have little control over the center of the intervals. As the magnitude of $\beta$ increases, so generally does the bias. As we will see, the width of proposed intervals also generally does increase, but at a relatively lower rate than the bias. As a direct result, we would expect a "faithful" confidence interval produce different coverage rates as a function of the magnitude of $\beta$.

Instead, we propose a different focus: overall coverage. As a direct result of this, one must expect that such intervals are going to over cover values of $\beta$ that are small and undercover those which are larger in magnitude. The goal then being finding a method that is well calibrated to provide overall coverage near that of nominal.

The second problem, the Epsilon Conundrum, is also related to the shrinkage but arises as an arguably more disturbing manifestation: confidence intervals of length zero or, more often, with a single endpoint that is exactly zero. Some penalized regression methods, particularly the lasso, often result in a sparse solution. If using a traditional quantile based bootstrap confidence interval, this will lead to an interval of [0,0] if a given variable is rarely or never included in the active set. As the dimensionality of the problem grows, this becomes an increasing occurrence leading to a large majority of intervals possessing a length of zero. When the true value of the coefficient is 0, the interval at least contains the truth. However, this issue is particularly troublesome when one considers what happens when the true value is not precisely zero, as is likely the case in most reasonable scenarios. By just shifting the true value by $\eps$, an immediate drop in coverage would occur, hence the name: the Epsilon Conundrum.

\begin{figure}
  \includegraphics[width=\linewidth]{traditional}
  \caption{\label{Fig:traditional} Caption goes here}
\end{figure}

The later of these two problems is addressed in our novel approach to producing bootstrap based confidence intervals for the lasso.

\section{Notation}

Throughout this paper, certain notation will be repeatedly used. $B$ will represent the total number of bootstrap datasets generated. $\boldsymbol{X}^b$ and $\boldsymbol{y}^b$ refer to the $b^{th}$ bootstrap sample. Similarly, $\hat{\beta}^b_j$ will refer to the estimate for $\beta_j$ from the lasso fit to $\boldsymbol{X}^b$ and $\boldsymbol{y}^b$. However, the previous notion with a superscript $b$ will often be used in a nonspecific manner to indicate that an estimate is with respect to a bootstrap draw rather than a specific bootstrap draw.

\section{Lasso Bootstrap Confidence Intervals}

\subsection{An Overview of Proposed Methods}

In what follows, we examine four alternative methods for obtaining bootstrap draws.

The first is the traditional bootstrap which simply involves taking the point estimate for each $\beta_j$ for each respective bootstrap sample. As mentioned previously, and as we will see, this has clear pitfalls which the subsequent three alternatives attempt to address.

The second is the posterior bootstrap. This leverages the Bayesian formulation of the lasso. For each bootstrap sample, a random draw from the full conditional posterior is obtained instead of taking the point estimate (equivalent to the mode) of the posterior.

The posterior bootstrap tends to be too aggressive in the additional variability introduced. As such, the third alternative borrows behavior from both the traditional and the posterior bootstrap and is appropriately called the hybrid bootstrap in this manuscript. The hybrid bootstrap only samples from the full conditional if the point estimate for a given $\beta_j$ is equal to zero for that bootstrap sample. This alternative will be the main focus of the manuscript.

The fourth and final method is a naive approach at producing debiased intervals and as such is referred to as the debiased bootstrap in this manuscript. Instead of taking the point estimate as the traditional method would, this method uses the correlation between the partial residuals obtained from the lasso with $\boldsymbol{x}_j$. As such, one might expect this to address both issues mentioned above. As we will see, it does in fact, however, its performance in general is suboptimal when $p < n$.

\subsection{Intuition behind the hybrid bootstrap}

Given that the hybrid bootstrap is the main focus of this manuscript, it will be worth while to briefly consider the intuition driving this proposed sampling method. Recall, potentially the most striking issue with the traditional bootstrap is that it can produce intervals which are singleton at 0. This occurs when $\hat{\beta}_j^b = 0$ for at least $B * (1 - \alpha)$ draws. To address this issue, then, adjustment is only needed when $\hat{\beta}_j^{b} = 0$. Next, we consider the behavior of this interval from a logical perspective.

For a moment, hold p constant. As n increases and we let $\lambda$ get selected by CV, this method will converge to that of applying the traditional bootstrap to a classical linear model. This is the case because with increasing n, $\lambda_{CV} \rightarrow 0$. On the other hand, as n decreases $\lambda_{CV} \rightarrow \lambda_{max}$ and $\hat{\boldsymbol{\beta}}\rightarrow \boldsymbol{0}$. On this end of the extreme, bootstrap samples are then drawn from the marginal posterior. The coverage behavior is well understood for the bootstrap applied to linear regression as n increases. By only sampling from the posterior when $\hat{\beta}_j^b = 0$, we minimize unnecessary variability as n increases. The behavior on the other end of the extreme is relatively less understood compared to the asymptotics of the traditional bootstrap. However, we argue that leveraging the posterior lends itself to reasonable behavior especially when the alternative would be confidence intervals which are all identical to zero. Such intervals, one could argue, would necessarily have 0\% coverage. Conversely, as we will see, with smaller values of n, the posterior tends to lead us to wider confidence intervals. See Section~\ref{Sec:full-cond}.

\subsection{Conceptual Implementation}
\label{Sec:full-cond}

In this section, we describe the details and important considerations for the sampling performed in the posterior and the hybrid bootstrap.

As with other penalized regression models, the lasso can be formulated as a Bayesian regression model by setting an appropriate prior. This was addressed initially by \cite{Tibshirani1996} and covered more extensively by \cite{Park2008}. For the lasso, the prior is a Laplace distribution, also referred to as the double-exponential distribution:

\as{\p(\bb) = \prod_{j = 1}^{p} \frac{\gamma}{2}\exp(-\gamma \abs{\beta_j}), \gamma > 0}

With this prior, the lasso estimate $\bbh(\lam)$ is the posterior mode of $\bb$ when $\lam = \gamma \frac{\sigma^2}{n}$. We purpose leveraging this relationship for building confidence intervals.

Unfortunately, a Normal likelihood and Laplace prior are not conjugate and in general, the absolute value in the exponent of the Laplace makes many common manipulations for the posterior more difficult. Luckily, however, the full conditional posterior can be shown to be a mixture of a right and left truncated normal where the truncation occurs at zero for right and left tails respectively. To obtain a full conditional posterior, we use the partial residuals, $\r_{-j}$, in the likelihood. This is natural when considering the common CD algorithms used to arrive at lasso estimates. In this manuscript, we will assume that $\X$ has been standardized s.t. $\x_j^T\x_j = n$.

Note that in what follows, the full conditional represents the distribution for a given value of $\lambda$ and $\sigma^2$. Further discussion will be given to selecting these values later, however, in the meantime to reduce notational distractions we will treat them as known quantities and implicitly condition on them.

Then, for $\beta_j$,

\as{
L(\beta_j | \r_{-j}) &\propto \exp(-\frac{n}{2\sigma^2} (\beta_j^2 - 2z_j\beta_j)), \\
\text{ where } z_{j} &= \frac{1}{n} \x_{j}^{T}\r_{-j} \text{, and} \\
P(\beta_j | \r_{-j}) &\propto
\begin{cases}
C_{-} \exp(-\frac{n}{2\sigma^2} (\beta_j - (z_j + \lambda))^2), \text{ if } \beta_j < 0, \\
C_{+} \exp(-\frac{n}{2\sigma^2} (\beta_j - (z_j - \lambda))^2), \text{ if } \beta_j \geq 0 \\
\end{cases}
}

Where, $C_{-} = \exp(\frac{z_j \lambda n}{\sigma^2})$ and $C_{+} = \exp(-\frac{z_j \lambda n}{\sigma^2})$.

What makes this formulation attractive is that a mapping allows tail probabilities from the posterior to be translated to probabilities onto corresponding known normal distributions (i.e. $\N(z_j \pm \lambda, \frac{\sigma^2}{n})$). The allows for numerically stable and efficient sampling to be obtained from the full conditional posteriors. Details of how sampling is performed are outside of the scope of this manuscript and are provided in Supplement~\ref{Sup:A}. For the remainder of the manuscript, we simply refer to this process as ``sampling from the full conditional.''

Again, recall that this solution corresponds to a particular value of $\lam$ and $\hat{\sigma}^2$. Our recommendation is to use CV to select $\lam$ and produce an estimate for $\sigma^2$ and produce bootstrap confidence intervals corresponding to these values. Performance under this recommendation will be focused on in the Results section. However, it should be noted that producing an estimate for $\sigma^2$ in this manner implicitly depends on $\lam$, so new estimates for $\sigma^2$ should be obtained for each value of $\lam$. When referring to $\hat{\sigma}^2$ it will always be as a function of $\lam$ which we drop for notational convenience.

All together this leads to the following steps to obtain CIs through the various alternative bootstraps:

\begin{enumerate}
\item Perform CV using the original data to select $\lam$ and estimate $\hat{\sigma}^2$
\item For b $\in \lbrace 1, \ldots, B \rbrace$:
\begin{enumerate}
\item Obtain a pairs bootstrap sample, $\X^b$ and $\y^b$
\item Fit lasso with $\X^b$ and $\y^b$, obtain $\bbh^b$, $\boldsymbol{z}^b$
\item For j $\in \lbrace 1, \ldots, p \rbrace$:
	\begin{algorithmic}
	\Switch{method}
    \Case{traditional}
      \Assert{$q_j = \bh_j$}
    \EndCase
	  \Case{posterior}
    \Assert{
      $p^* = \Unif(0, 1)$ \\
      \hspace{1.95cm} $q_j = P^{-1}(p^*|\r_{-j})$
    }
    \EndCase
    \Case{hybrid}
      \Assertif{$\bh_j \neq 0$}{$q_j = \bh_j$}
      \Assertelse{
        $p^* = \Unif(0, 1)$ \\
        \hspace{1.8cm} $q_j = P^{-1}(p^*|\r_{-j})$
      }
    \EndCase
    \Case{debiased}
	    \Assert{$q_j = z_j$}
    \EndCase
	\EndSwitch 
	\end{algorithmic}
\item Save $\q$
\end{enumerate}
\item Row bind all B $\q$ vectors to obtain a $B \times p$ matrix of bootstrap draws
\item For each $\beta_j$, compute the quantiles for $p_L = (.5 - \l/2)$ and $p_U = (.5 + \l/2)$ from the $j^{th}$ column of the draws to produce a final confidence interval estimate for significance level $\l$.
\end{enumerate}

\section{Results}

We make the assertion that the ideal scenario for these methods is when the true distribution of $\beta$ follows a laplace (double exponential) distribution. In what follows, the reader can assume that unless otherwise noted $\beta \sim dexp(\tau = 2)$. This may raise the question of how the results change if a different rate is used. However, under a constant Signal-to-Noise ratio (SNR), the results described below are identical. \pb{Would a cleaner way of saying this be: we set SNR = 1 and $\sigma^2=1$, which results in $\tau = 1.47.$} \logan{With the simulation set up, I don't think we can make this direct connection. If SNR = 1 and $\sigma^2=1$ then we have the restriction that $\bb^T\bb = 1$. As I have it set up, I vary $\sigma^2$ to keep a constant SNR, otherwise as we change n it would force the magitude of $\bb$ to decrease. There really then isn't some $\tau$ that sets SNR = 1. Rather, $\tau$ would be arbitrary and we would need to normalize $\bb$. After considering more distributions, however, I'm not even sure if this is a necessary assertion to make. Honestly, it almst seems like the uniform distribution is the ideal distribution and that overcoverage generally results as more density/mass is moved to zero. And, unless we pull Figure~\ref{Fig:true_lambda} out of an appendix, we really didn't end up using that under a laplace we know the true value of $\lambda$.}

\subsection{Interval Behavior}

We compare the proposed CI methods by showing the coverage of each, both overall and as the magnitude of $\beta$ changes, then present results on the width and bias of the intervals as a way of gaining insight into why the methods result in over or under coverage at various magnitudes of $\beta$.

\subsubsection{Coverage}\label{Sec:Coverage}

\logan{What do you think about this flow? I shifted around the order of discussion: overall $\rightarrow$ near / far + decreasing $\rightarrow$ transition to width and bias as explaination. I.e. this section remains very high level.} 

Figure~\ref{Fig:laplace} displays simulation results for CIs obtained via the four previously described bootstrap methods. In this simulation, 100 independent datasets were generated and each bootstrap method was applied using $B = 1000$ bootstrap iterations. Each dataset was simulated as follows. $\X$ was generated independently with $n = p = 100$ and $\bb$ was generated from a Laplace distribution. Then, $\Y$ was constructed as $\Y = \X\bb + \boldsymbol{\epsilon}$, where $\boldsymbol{\epsilon} \sim N(0, \sigma^2_y)$ with $\sigma^2_y$ set such that the $SNR = 1$. The dotted lines represents the average coverage for each method across all variables for all 100 datasets. The solid lines are estimates of coverage as a smooth function of $\beta$.

\begin{figure}
  \includegraphics[width=\linewidth]{laplace}
  \caption{\label{Fig:laplace} Results are from the simulation described in Section~\ref{Sec:Coverage}. The fitted curves are from Binomial GAMMs (one for each method) fit with coverage being modeled as a smooth function of $|\beta|$ and with a random intercept on dataset to account for deviations in coverage specific to a given randomly generated dataset. The data used for modeling contains a row for each variable per simulated dataset. The dashed lines represent the average for each method across all 100 independently generated datasets and the solid black line indicates the nominal coverage rate. The shaded distribution in the background depicts the positive tail of the distribution the $\beta$s were drawn from, $dexp(\tau = 2)$.}
\end{figure}

Before describing the observed coverage behavior, we draw brief attention to Figure~\ref{Fig:laplace} as a depiction that Hybrid is a mixture of Posterior and Traditional, being more like Posterior when $\beta$ is small in magnitude and increasingly more like Traditional as $\beta$ increases in magnitude.

Figure~\ref{Fig:laplace} also shows that, as is generally the case when applied to the lasso, the Traditional bootstrap coverage is far below that of nominal. At the other end of the extreme, Posterior, which draws from the full conditional for every bootstrap draw, tends to produce intervals that significantly over cover relative to nominal coverage. Both Hybrid, which only samples from the full conditional when $\bh = 0$, and Debiased produce coverage near the $80\%$ nominal coverage rate.

To understand the coverage behaviors further, it helps to consider what happens near zero and far-from-zero. Traditional has low coverage both near and far-from zero. Debiased also under covers both near and far-from zero but to a much lesser extent and, of the methods considered here, generally sticks closest to nominal coverage regardless of the magnitude of $\beta$. Conversely, Posterior and Hybrid both display a significant degree of over coverage for values of $\beta$ near zero. However, like the other two methods, Posterior and Hybrid also both under cover far-from-zero but to varying degrees. The over coverage near zero and under coverage away from zero for Posterior and Hybrid can be explored more wholistically by noting the striking feature of Figure~\ref{Fig:laplace} that they have a clearly decreasing coverage with increasing magnitudes of $\beta$. This pattern is typical for these methods and is to be expected from intervals arising from procedures that are faithful to the model fit. As emphasized in Section~\ref{Sec:Difficulties}, introducing bias through penalization is an inherent feature of the lasso. This bias can largely explain the pattern observed. In fact, as we will see, the effect of penalization on the width and bias of the intervals can explain the coverage patterns seen for all four methods, which is where we now turn our attention to.

\subsubsection{Width and Bias}\label{Sec:Width and Bias}

\logan{Make comment for Traditional draws = 0 and their relation to this section. Fully cover observations above through width and bias.}

Intervals are are biased towards zero, and as such tend to over cover when $\beta$ is near zero. Bias tends to increase for increasing values of $|\beta|$ as a result of penalization.

This is due to the fact that Debiased includes only a naive correction that fails to fully address the bias introduced by penalization. 

\logan{Okay to work into this section? Feels a bit more natural.}
A detail that was omitted in the previous section was that Debiased also shows a decreasing behavior, although to a much lesser degree. Now we can see why, debiased does not fully debias.

\begin{figure}
  \includegraphics[width=\linewidth]{laplace_width_bias}
  \caption{\label{Fig:laplace_width_bias} Results are from the simulation described in Section~\ref{Sec:Coverage}. The fitted curves are from GAMMs (one for each method) fit with width and central bias being modeled as a smooth function of $|\beta|$. The data used for modeling contains a row for each variable per simulated dataset. Additionally, the models contain a random intercept on dataset to account for deviations in width and central bias specific to a given randomly generated dataset. Central bias is defined here as the difference between the center of an interval and the true value of $\beta$, with positive values indicating bias towards zero. Further discussion on central bias can be found in Section~\ref{Sec:Width and Bias}}
\end{figure}

Figure~\ref{Fig:laplace_width_bias} largely explains the behavior seen in Figure~\ref{Fig:laplace}. The fitted curves in each plot are constructed in the same manner as for Figure~\ref{Fig:laplace}, albeit on the respective outcome of interest. The left side of Figure~\ref{Fig:laplace_width_bias} shows that the interval width tends to increase as $\beta$ increases in magnitude. This behavior is directly related to the proportion of times a variable is selected to be in the model. Said more explicitly, when a given $\beta$ is near zero, its estimate will often be shrunk to zero which results in less variable bootstrap draws than for a variable which is always selected. This effect is most notable for the Traditional bootstrap, as it is the only method which will result in a draw exactly equal to zero when $\bh^b_j = 0$. The three other methods all introduce some additional variability in this scenario. This also explains why then Traditional produces much narrower intervals for values of $\beta$ smaller in magnitude. This explains the drastic under coverage observed in the previous section and is a manifestation of the Epsilon Conundrum which will be discussed in more detail in Section~\ref{Sec:Epsilon}. This plots also depicts the explains the coverage issue with the Posterior bootstrap, as it produces significantly wider intervals regardless of the magnitude of $\beta$.


\logan{Updated explaination:}

The left side of Figure~\ref{Fig:laplace_width_bias} provides a depiction of the bias of each method for obtaining intervals. Before going forward, however, it is important to emphasize that while informative, this is an imperfect measure of bias. While a clear definition exists for the bias of a point estimate, the bias of an interval lacks an accepted definition. If an interval is symmetric about its point estimate, then it is acceptable to take the bias of the interval as the bias of the point estimate. However, when obtaining quantile based bootstrap intervals, the intervals are very unlikely to be perfectly symmetric. Additionally, the penalization induced bias exacerbates this fact and produces intervals which are skewed, further complicating a definition of center for bootstrap quantile intervals for the lasso. Given the construction of the interval, the median bootstrap draw is the logical definition of center and consequently what we use here for the determination of bias.

\logan{This point get orphaned by the update:}

This explains why the Debiased method fails to fully debias as seen in Section~\ref{Sec:Coverage} where the data are generated under independence. This point can not be emphasized enough, a key source of bias arises when chance correlation occurs between independent variables and both variables are included in the model.

With this point in mind, we see the expected behavior that center of the intervals tend to be increasingly biased towards zero for larger absolute values of $\beta$. The right side of Figure~\ref{Fig:laplace_width_bias} also indicates that Traditional, Posterior, and Hybrid have similar amounts of central bias. Although the Debiased method cuts the bias in about half, we again see that it is unable to completely eliminate bias for the reasons previously described.

What is not as clear from this depiction is that the Debiased intervals sometimes fail to cover $\beta$ due to having \textit{lower} bounds that are too large in magnitude. This happens occasionally for the other intervals, but is more common for Debiased and tends to occur more often for $\beta$ values that are smaller in magnitude. This is why we do not observe over coverage for Debiased like we do for Hybrid, even though they have similar widths and Debias has less bias. \pb{provide an example? e.g., beta = 0.2 and debiased interval is 0.4 - 0.8.}
\logan{Put together an actual example of where this occurs, reference it and put it in appendix / supplement. Maybe in the simulation capture how often this occurs?}

\pb{we can't really dismiss Debiased yet; it does ok here}
\logan{Foreshawdow the downfall of debiased?}
We hope the reader is exceedingly convinced that the Traditional bootstrap is a poor choice. Additionally, as will be demonstrated throughout the remainder of this manuscript, the Hybrid generally provides over coverage except in scenarios with very high amounts of correlation. \logan{Need to emphasize that we don't need intervals wider than Hybrid without making it sound bad.} So then, it should be noted that the additional over coverage for the Posterior Bootstrap seen in Figure~\ref{Fig:laplace} is a hallmark of this method making it a suboptimal option as well. For simple cases of independence, Debiased does tend to produce coverage near nominal, although approaches nominal (with increasing sample size) from below as the uncorrected bias for larger magnitude $\beta$s slowly diminishes. However, while Debiased tends to work well for larger values of n, it tends to undercover for smaller values of n which is magnified under more challenging conditions such as if correlation is introduced. Hybrid does not behave as sensitively as Debiased and tends to approach nominal coverage from above rather than below. As such, the remainder of this manuscript will focus on the behavior of the Hybrid bootstrap.


\subsection{Robustness}

This section explores a number of scenarios to help understand the behavior of the Hybrid bootstrap. It begins with a brief look at how behavior differs at different nominal coverages. This section also gives the first look at how the behavior changes with sample size. From there, focus is shifted to the selection of $\lam$ (and estimation of $\sh^2$) before considering how the Hybrid Bootstrap performs under other distributions of $\bb$. From there, we pivot and take a look at various (including rather extreme) introductions of correlation which includes one simulation with a direct comparison to Ridge Regression. This section ends with a revisit to the Epsilon Conundrum.

\subsubsection{Correlation}

Now, lets consider some cases with higher levels of correlation. In \textbf{Figure B}, the first row contains the coverages across 100 simulated datasets with n = p = 100 with $\beta \sim laplace(rate = 2)$ but with autoregressive correlation added. Note that for a moderate amount of correlation ($\rho = 0.4$), there is little impact of the coverage of the intervals (see Figure~\ref{Fig:laplace} for $\rho = 0$).  As we increase the correlation, however, we begin to see under coverage for larger values of n. That said, even with a significant amount of autoregressive correlation, we still observe a median coverage just under 70\%.

In the second row, we have data generated under a different mechanism. Here, the data size is the same, but the $\beta$ values and $cor(X)$ is different. Here, there are only 5 non-zero $\beta$ values (denoted with A). However, for each non-zero $\beta$, there are 2 noise variables generated with the specified correlation (denoted with B). For the other 85 zero $\beta$s (denoted with N), they are generated independent of the A and B variables but have exchangeable correlation among each other with $\rho = 0.2, 0.5, and 0.6$ respectively. As previously discussed, we see a significant amount of over coverage due to the high level of sparsity. Regardless, the overall trend remains the same in the the overall coverage gradually decreases with increasing levels of correlation.

\logan{Remove exchangeable, add debiased? (For plot)}
\begin{figure}
  \includegraphics[width=\linewidth]{correlation_structure}
  \caption{\label{Fig:correlation_structure} Caption goes here}
\end{figure}


\subsubsection{Distribution of Beta}

As mentioned in the previous section, the overall coverage rate is a trade off based on the value of $\lam$ and the true distribution of $\beta$, which is of course generally unknown. The plots below are from the same simulation setup as the previous section, but with different distributions for $\beta$. Focusing on $\lam_{CV}$, the coverage rate is similar regardless of the distribution of $\beta$, with $\beta \sim N(0, 1)$ producing coverage nearest to nominal.

In classical high dimension scenarios, it is often expected that the solution is sparse. It should be no surprise then that when the true data generating mechanism is more sparse than the laplace indicated by the respective $\lambda$ that this method will provide over greater coverage.

These plots also exemplify the effect of the magnitude of $\beta$ on coverage. The greatest separation is then $\lam = \lam_{max}$ and their coverage converges as $\lam \rightarrow \lam_{min}$. Coverage is of course higher for $\beta$ values smaller in magnitude. At values of $\beta$ that are classified as small in magnitude in this set of plots, their coverage is highest at $\lam_{max}$ and generally decreases as $\lam$ is reduced. On the other end, for $beta$ values largest in magnitude, we a monotonically increasing trend, with those moderate in size falling somewhere in the middle in their pattern depending on the distribution of $\beta$.

\subsubsection{Epsilon Conundrum}\label{Sec:Epsilon}

Its natural to close this section by considering the performance on a scenario that was the main motivator of the Zero Sample method: the Epsilon conundrum. So how does Zero Sample Perform in this scenario? It avoids the downfall we observe with applying the Traditional bootstrap. Clearly, however, it also over covers due to the fact that the large majority of $\beta$s are near zero, but, again, this is to be expected and the important take away is that Zero-Sample provides viable intervals for values of $\beta$ near zero.

\begin{figure}
  \includegraphics[width=\linewidth]{beta_lambda_heatmap_laplace}
  \caption{\label{Fig:beta_lambda_heatmap_laplace} n = p = 100, where $\beta \sim Laplace(rate = 2)$ and $\X$ generated under independence structure. The red line represents the average CV value of $\lam$. The x-axis was truncated over the range of $\lam_{CV}$ and presented relative to each simulations $\lam_{max}$.}
\end{figure}

\subsubsection{Selection of \texorpdfstring{$\lambda$}{lambda} (and Estimation of \texorpdfstring{$\sigma^2$}{sigma squared})}

The reader may be wondering about what happens if the as the value of $\lambda$ is changed. The data in Figure~\ref{Fig:beta_lambda_heatmap_laplace} comes from a simulation where $\lambda$ was evenly distributed on the $\log_{10}$ scale from $\lam_{\max}$ to $\lam_{\min} = \lam_{max} * 0.001$. At each value, confidence intervals were obtained and coverage was recorded. This was repeated 100 times, then a gam was fit to provide a smooth estimate of the coverage rate. The relative coverage is defined as estimated coverage minus nominal coverage.

The pattern is exactly as we would intuitively expect. Depending on the value of $\lam$, values closer to zero see moderate amounts of over coverage whereas there is increasingly greater under coverage as $|\beta|$ increases. The value at where this transition occurs (and at which coverage of a specific value of $\beta$ = nominal) varies considerably over the range of $\lam$. At $\lam_{max}$, this transition occurs at a relatively small $|\beta|$. As the value of $\lam$ decreases from $\lam_{max}$ this transition occurs at increasingly large values of $\beta$ until all values of $\beta$ have an estimated coverage rate at or above that of nominal. To obtain a covariate rate near that of nominal, $\lam$ needs to be selected such that over and under coverage is balanced. In this scenario, $\lam_{CV}$ does a reasonable job at providing such a balance. This balance, of course, depends on the true distribution of $\beta$ (recall that here it is laplace). However, as we will see next, $\lam_{CV}$ generally does well regardless of the distribution of $\beta$.

At this point, it is also important to note that this performance is in spite of needing to provide estimate for the true values of $\lam$ and $\sigma^2$. In fact, we found that knowing these values actually produces worse performance, suggesting that for this method $\lam_{CV}$ and $\sigma^2(\lam_{CV})$ are reliable selections.

\subsubsection{Nominal Coverage}

\begin{figure}
  \includegraphics[width=\linewidth]{nominal_coverage}
  \caption{\label{Fig:nominal_coverage} Caption goes here}
\end{figure}

Figure~\ref{Fig:nominal_coverage} is similar to Figure~\ref{Fig:laplace}, but it focuses only on Hybrid and gives simulation results for three values of n across three different nominal coverage rates. Since the method has a varying coverage rate based on the magnitude of $\beta$, it is important to consider different nominal coverages. Otherwise, it is conceivable that a method could perform well at one coverage rate but not another. However, that is not the case here. Regardless of the nominal coverage, the general pattern remains the same: the method over covers for smaller values of n but coverage converges to the nominal coverage rate relatively quickly. The only difference seen is the compression of this pattern for higher nominal coverage rate. Going forward, the nominal coverage will be set at $80\%$ to reduce compression so that any patterns present are easier to observe.

\begin{figure}
  \includegraphics[width=\linewidth]{zerosample2}
  \caption{\label{Fig:zerosample2} Caption goes here}
\end{figure}

\subsection{Comparison to Other Methods}

There are few other methods for obtaining intervals for the lasso. There are even fewer that provide R packages to provide comparisons with. Two that we were able to identify were Selective Inference (from \texttt{selectiveInference}) and Bootstrap Lasso Projection (from \texttt{hdi}). It should be noted, however, that neither method fits the criteria we were looking for of remaining faithful to the model fit.

Specifically, the Bootstrap Lasso Projection (BLP) is also known as the de-sparsified Lasso and by default the method reselects $\lambda$ for each bootstrap sample. Selective Inference (SI) doesn't directly correct for the bias introduced by penalization but this method does only provide intervals for variables with non-zero coefficients. With these points in mind, we compare the relative performance of the proposed Hybrid Bootstrap with these two alternatives.

\logan{Remove points that will be covered in results later}.

Before considering simulation results, there are a couple important limitations of each method that should be noted. For BLP, it should be noted that by with its default arguments, it is an order of magnitude slower than the Hybrid Bootstrap which itself is an order of magnitude slower than SI. Additionally, as implemented, there is no easy way to specify the $\lambda$ used which defaults to the 1-SE solution from \texttt{cv.glmnet}. SI is much faster, but also tended to be much more fragile in our testing. In general, SI does not work for the $p > n$ case (it can, but only if the number of features selected is less than n) and CIs are only produced for variables in the active set. Additionally, it is cumbersome to implement and infinite bounds were more common than not when applied to data where n was on the order of p. As will be emphasized later, in similar scenarios SI also failed to converge in some iterations of the simulation performed. 

\subsubsection{Similation}

The simulation results presented here are identical to the set up described in Section~\ref{Sec:Coverage}. In fact, the results for Hybrid are the same as used for the earlier figures just displayed differently. 

Referencing Figure~\ref{Fig:laplace_comparison}, both BLP and SI initially appear to perform strikingly well. Both have coverage very near that of nominal and lack the pattern seen with the methods proposed in this manuscript and instead provide rather consistent coverage regardless of the magnitude of $\beta$. However, \ref{Fig:laplace_other} tells a different story. First, although BLP provides average coverage rates the closest to $80\%$, there were a number of cases where the coverage dipped significantly. Potentially more concerning, with $n = 400$, the coverage noticeably drops below 80\%. Although the lasso thought of a model for high dimensional data, it is used across the entire spectrum of datasets, so it would be preferred to see convergence towards 80\% coverage.

The behavior for the Hybrid Bootstrap has been covered previously, specifically that it generally over covers when $n < p$ but has coverage very near nominal as n increases above p. Additionally, although not immune to under coverage, it performed more reliably than the other two method.

Before moving onto the results for SI, there are a couple numbers not in Figure~\ref{Fig:laplace_comparison} which are also important to consider and are provided in Table~\ref{Tab:selective_inference}. When n = 50, 20 of the 100 simulations errored out, with only 6 when n = 100, and none when n = 400. The last column gives the average percentage of variables that had confidence intervals of the simulation iterations that did not produce errors. It is this subset that is included in the coverage plot for SI. So, ignoring the times the method failed, even if on average the coverage look reasonable, the wide spread of coverages, including a staggering number below 50\% coverage is concerning. Although performance does improve, the fact that this behavior is not remedied even with larger n should also not be ignored.

\begin{table}[ht]
  \centering
  \begin{tabular}{cccc}
  \hline
  n & \# Succeeded & \% Vars Included & Non-finite Median Width \\
  \hline
  50  & 80  & 18.5 & 18 \\
  100 & 94  & 30.4 & 12 \\
  400 & 100 & 70.1 & 3 \\
  \hline
  \end{tabular}
  \caption{Selective Inference Results}
  \label{Tab:selective_inference}
\end{table}

Staying with SI but directing our attention towards the interval widths, the concern surrounding this method only grows. Of the 80 that simulations that exceeded for n = 50, the median width of the intervals produced (from the variables included in the model) was infinite for 18 of the simulation. For n = 100, 12 of the 94 had infinite median widths. By n = 400, only 3 of the simulations had infinite median widths. However, even when the medians were finite, they were nearly always extremely wide, even for n = 400. This behavior was also observed when applying the method to real data sets which will be covered next. BLP and Hybrid on the other hand produce intervals which are more similar in width although BLP does tend to produce wider intervals with and with a bit more variability in the width.

The runtime of the three methods also differs considerably with SI being the fastest and BLP being by far the slowest. Additionally with BLP, there is an odd non-monotonic behavior which we looked into briefly but could not find an explanation for. This behavior occurred in multiple reruns of the same simulation. Hybrid and SI both have a monotonically increasing relationship with sample size, although it Hybrid is more affected by the increasing sample size. In our testing, speed was not a concern for SI, was noticeable for Hybrid, and prohibitive for BLP which will be returned to in the \logan{next section}.

\begin{figure}
  \includegraphics[width=\linewidth]{laplace_comparison}
  \caption{\label{Fig:laplace_comparison} Caption goes here}
\end{figure}


\begin{figure}
  \includegraphics[width=\linewidth]{laplace_other}
  \caption{\label{Fig:laplace_other} Caption goes here}
\end{figure}


\subsubsection{Application: Gene expression in the mammalian eye}

\logan{Which to cover}

\begin{figure}
  \includegraphics[width=\linewidth]{comparison_data_scheetz}
  \caption{\label{Fig:comparison_data_scheetz} Caption goes here}ß
\end{figure}

\begin{figure}
  \includegraphics[width=\linewidth]{comparison_data}
  \caption{\label{Fig:comparison_data_whoari} Caption goes here}ß
\end{figure}

\subsection{Ridge Regression Comparison}

We start of with a simple example and compare the intervals produced by Zero Sample with that of Ridge. In this example again n = p = 100, however, only one $\beta$ is non-zero. That is, $\beta_{A1} = 1$ and $\beta_{B1}, \beta_{N_1}, \ldots, \beta_{N98} = 0$. Additionally, the data is simulated such that $\rho(\beta_{A1}, \beta_{B1}) = .99$ but all of the N (noise) $\beta$s are uncorrelated with $\beta_{A1}, \beta_{B1}$, and each other.

100 datasets were generated in this manner and each method was applied and respective confidence intervals obtained.

\textbf{Figure A} depicts the results from the simulation with the plot on the right giving box plots for the lower (red) and upper (blue) bounds across the 100 datasets for 3 variables, A1, B1, and N1. On the right, Confidence Intervals for a randomly selected example from the 100 simulated datasets is displayed.

Focusing on A1 and B1, what we would hope to see is wide intervals. Although A1 truly is the signal variable, its high correlation with B1 should produce a large amount of uncertainty about which variable (if not both) contain signal. Ridge, however, fails in this respect in that the intervals are very narrow. On the other hand, Zero Sample displays the desired behavior. The uncertainty entangled in A1 and B1 is reflected in wider intervals. Additionally, intervals for A1 usually do not contain 0 whereas the intervals for B1 contains 0 over 40\% of the time. In general, there is a clear shift in the intervals for A1 compared to B1 suggesting that even with very high correlation, Zero Sample often attributed more of the signal to A1, which was clearly not the case with Ridge.

\begin{figure}
  \includegraphics[width=\linewidth]{highcorr}
  \caption{\label{Fig:highcorr} Caption goes here}
\end{figure}

\section{Discussion}

\subsection{Space Requirements}

As implemented, a clear limitation of this method is that it takes a numeric matrix size $B \times p$. With $B = 1000$, the sample matrix gets large enough to cause memory concerns even when $p$ is on the order of $1e5$. For many datasets, this is likely not of concern. However, given that lasso is often used for datasets where $p$ is large, it is clearly not an edge case where $p$ is of this or larger order. One could reduce the size of the sample matrix by reducing the number of draws, but this is unsatisfactory and produces little additional leeway for what seems like a unacceptable sacrifice. This is an ongoing are of interest and a valuable areas of research for any high dimensional bootstrap methods. One solution would be using incremental quantile estimation such as the method introduced by \cite{Tierney1983}. An alternative option would be deriving a method with similar properties but which relies on samples statistics that are relatively straightforward to update over sequential samples (i.e. mean and variance).

\subsection{Remember the Name}

Since the main method suggested here falls somewhere between the Traditional Bootstrap and the Posterior Bootstrap, we propose the full name as the Posterior Adjusted Traditional Hybrid (PATH) Bootstrap.

\section*{Acknowledgments}

\section*{Appendix}

\subsection{True lambda / sigma}

\begin{figure}
  \includegraphics[width=\linewidth]{true_lambda}
  \caption{\label{Fig:true_lambda} Caption goes here}
\end{figure}

\section*{Supplement}

\subsection{Supplement A: Sampling from the Full Conditional Posterior}\label{Sup:A}

\logan{Trash?}

\as{
L(\beta_j | \r_{-j}) &= (\sigma \sqrt{2\pi})^{-n} \exp(-\frac{1}{2\sigma^2} (n\beta_j^2 - 2\x_{j}^{T}\r_{-j}\beta_j + \r_{-j}^{T}\r_{-j})) \\
&\propto \exp(-\frac{n}{2\sigma^2} (\beta_j^2 - 2z_j\beta_j)), \text{ where } z_{j} = \frac{1}{n} \x_{j}^{T}\r_{-j} \\
\Rightarrow P(\beta_j | \r_{-j}) &\propto \exp(-\frac{n}{2\sigma^2} (\beta_j^2 - 2z_{j}\beta_j)) \frac{n \lambda}{2 \sigma^2} \exp(-\frac{n \lambda} {\sigma^2} \abs{\beta_j}) \\
&\propto \exp(-\frac{n}{2\sigma^2} (\beta_j^2 - 2 z_j\beta_j +  2 \lambda \abs{\beta_j})) \\
&= \exp(-\frac{n}{2\sigma^2} (\beta_j^2 - 2(z_j\beta_j - \lambda \abs{\beta_j}))) \\
&=
\begin{cases}
\exp(-\frac{n}{2\sigma^2} (\beta_j^2 - 2(z_j + \lambda)\beta_j)), \text{ if } \beta_j < 0, \\
\exp(-\frac{n}{2\sigma^2} (\beta_j^2 - 2(z_j - \lambda)\beta_j )), \text{ if } \beta_j \geq 0 \\
\end{cases} \\
&\propto
\begin{cases}
C_{-} \exp(-\frac{n}{2\sigma^2} (\beta_j - (z_j + \lambda))^2), \text{ if } \beta_j < 0, \\
C_{+} \exp(-\frac{n}{2\sigma^2} (\beta_j - (z_j - \lambda))^2), \text{ if } \beta_j \geq 0 \\
\end{cases}
}

\logan{Discuss notation}

Draws can be efficiently obtained from the Full Conditional Posterior,
\as{
P(\beta_j | \r_{-j}) \propto
\begin{cases}
C_{-} \exp(-\frac{n}{2\sigma^2} (\beta_j - (z_j + \lambda))^2), \text{ if } \beta_j < 0, \\
C_{+} \exp(-\frac{n}{2\sigma^2} (\beta_j - (z_j - \lambda))^2), \text{ if } \beta_j \geq 0 \\
\end{cases}
} through a mapping onto respective normal distributions. To define this mapping, it helps to introduce a concept and some notation. First, the use of "tails" in this supplement refers to the entirety the a distribution between zero and $\pm \infty$. That is, the lower tail is any part of the distribution below zero and the upper tail is any part greater than zero and $P(X \in lower \cup X \in upper) = 1$. Accordingly, we will let the tail probabilities in each of the two normals to transformed on to be denoted $\Phi_{-}$ and $\Phi_{+}$ respectively and the probability in each of the tails of the posterior, denoted $P_{-}$ and $P_{+}$ respectively. $P_{\pm}$ is trivial to compute with any statistical software. $P_{\pm}$ is conceptually simple, although care must be taken to avoid numerical instability as n increases. Now, notice that
\as{
P(\beta_j | \r_{-j})  & \propto
\begin{cases}
C_{-} Pr_{-}, \text{ if } \beta_j < 0, \\
C_{+} Pr_{+}, \text{ if } \beta_j \geq 0\\
\end{cases}
} which implies that $Post_- = \frac{C_{-} Pr_{-}}{C_{-} Pr_{-} + C_{+} Pr_{+}}$ and similarly for $Post_+$. However, to avoid numerical instability, or at least handle it properly when it is unavoidable we need to work on the $\log$ scale. This works well for most of the problem, but computation of $Post_-$ and $Post_+$ need something a bit more since, for example, $\log(Post_-) = \log(C{-}Pr_{-}) - \log(C_{-} Pr_{-} + C_{+} Pr_{+})$. That is, the denominator still must be computed then the $\log$ taken which does not allow operating on the $\log$ scale to fully address potential instability. Instead, $\log(Post_-)$ can be computed with $\log(C_-Pr_-) -  \log(C_+Pr_+) - \log(1 + \exp(\log(C_-Pr_-) -  \log(C_+Pr_+)))$. This still doesn't completely address the issue, however, if $exp(\log(C_-Pr_-) -  \log(C_+Pr_+))$ is infinite then $C_-Pr_- >> C_+Pr_+$ and $\log(Post_-) \approx 0$.

With these values, we can compute quantiles by mapping the corresponding probabilities $p$ for the posterior onto the probabilities $p^*$ for the corresponding normals. Which normal the quantiles of interest ultimately come from is determined based on the values in (2). For example, if $Post_{+} = 0.98$ and $p = 0.1$ the $p$ would be mapped onto the positive normal. As one more example, say $Post_{+} = 0.4$ and $p = 0.5$, then $p$ would be mapped onto the lower normal. The transformation to map a given probability from the posterior depends on which tail the quantile resides in on the posterior (equivalently which normal it is being mapped to, the positive or negative). This map is simply:

\as{
p^* &= p \times (Pr_{\pm} / Post_{\pm}) \\
}

Once this respective probabilities are mapped, one can simply use the inverse of the normal CDFs that the probabilities were mapped to. That being said, there is a nuance worth pointing out. When transforming the probabilities, the step to determine which tail the respective quantile comes from occurs first. With this, the probability should be adjusted so that it refers to the probability between the quantile of interest and the respective tail. After this, then the transformation can be applied.

\begin{enumerate}
	\item Obtain the partial residuals, $r_{-j}$ and compute $z_j$
	\item Compute $Pr_{-}$ = $\Phi(0, z_j + \lam, \frac{\sh^2}{n})$ and $Pr_{+}$ = $1 - \Phi(0, z_j - \lam, \frac{\sh^2}{n})$
	\item Compute $Post_-$ and $Post_+$
\end{enumerate}
\begin{algorithmic}
  \If {$P_r \leq Post_{-}$}
    \State $q_r = \Phi^{-1}(p_r(Pr_{-} / Post_{-}), z_j + \lam, \frac{\sh^2}{n})$
  \Else
      \State $q_r = \Phi^{-1}(1 - (1 - p_r)*(Pr_{+} / Post_{+}), z_j - \lam, \frac{\sh^2}{n})$
  \EndIf
\end{algorithmic}
  
\subsection{Supplement B: Distributions of \texorpdfstring{$\beta$}{beta}}\label{Sup:distributions} 

\begin{figure}
  \includegraphics[width=\linewidth]{distribution_of_beta}
  \caption{\label{Fig:distribution_of_beta} n = p = 100. Sparse 1: $\bb_{1-10} = \pm(0.5, 0.5, 0.5, 1, 2), \bb_{11-100} = 0$, Sparse 2: $\bb_{1-30} \sim N(0, 1), \bb_{31-100} = 0$, Sparse 3: $\bb_{1-50} \sim N(0, 1), \bb_{51-100} = 0$, Laplace: $\bb \sim Laplace(rate = 2)$, T: $\bb \sim T(df = 3)$, Normal: $\bb \sim Normal(0, 1)$}
\end{figure}