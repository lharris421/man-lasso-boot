\section{Introduction}

\logan{Thoughts on a larger emphasis for intervals to cover vs intervals to indicate model uncertainty?? A discussion of what we want from confidence intervals, why this is easy in non-high dimensional cases and what we have to sacrifice in high dimensions.}

The objective function for lasso-penalized linear regression \citep{Tibshirani1996} is
$$Q(\bb|\X,\y,\lambda) = \frac{1}{2n}\norm{\y - \X\bb}_2^2 + \lambda\norm{\bb}_1,$$
where $\y$ is a length $n$ vector of independent outcomes, $\X$ is an $n \times p$ matrix of features, $\bb$ is a length $p$ vector of regression coefficients, and $\lambda$ is a regularization parameter controlling the amount of penalization. Note that the objective function involves the addition of the $L_1$ penalty, $\lambda\norm{\bb}_1 = \lambda \sum_{j = 1}^p |\beta_j|$, to the squared error loss. This typically results in sparse estimates for some of the regression coefficients (i.e., $\bh_j = 0$) depending on the choice of the regularization parameter $\lambda$. Its ability to carry out both variable selection and estimation is particularly attractive, especially in scenarios where both predictive accuracy and interpretability are important. The model performs particularly well in cases where the number of features is large and the underlying model is sparse \citep{HTF2009}, but has become popular in a wide variety of settings.

Nevertheless, inference for the lasso has proven challenging. By introducing both sparsity and shrinkage, the $L_1$ penalty greatly complicates the sampling distribution of the estimators. This complexity has given rise to a wide variety of inferential approaches. The majority of these approaches have focused on controlling the false discovery rate (FDR) of the selected features. Examples include the Covariance test \citep{Lockhart2014}, the Knockoff Filter \citep{Candes2015,Candes2018}, the marginal FDR \citep{Breheny2019}, and the Gaussian mirror \citep{Xing2023}.

There have also been various proposals for constructing confidence intervals (CIs), although the shrinkage/bias introduced by the $L_1$ penalty introduces a number of challenges here. Several methods \citep{ZhangZhang2014,Javanmard2014} focus on ``debiasing'' the original point estimates from a lasso fit to facilitate more traditional forms of inference. An alternative approach, which accounts for the uncertainty in model selection by conditioning on the selected model is known as selective inference \citep{LeeEtAl2016}, although it is worth noting that this approach only produces intervals for variables that were selected by the model.

The bootstrap is often a natural choice for handling complex sampling distributions. However, \cite{Chatterjee2010} demonstrated that when applied to lasso estimators, the bootstrap is inconsistent -- even if the lasso model itself is $\sqrt{n}$-consistent with respect to estimating $\bb$. For this reason, efforts to bootstrap lasso models have focused instead on bootstrapping de-biased (or de-sparsified) versions of the lasso \citep{Dezeure2017}.

In this manuscript, we take up the question of whether bootstrapping lasso estimators is perhaps more useful than the statistical community is giving it credit for. In particular: does the bootstrap not work at all, or does it merely not work \emph{in the classical sense}? And if it doesn't work, can the bootstrap be modified so that it does? We find that the answer is a combination of both. Yes, there does need to be a methodological adjustment to fix the bootstrap, but also a change in perspective. Bootstrapping biased lasso estimators may not yield correct coverage in the classical sense for each individual coefficient, but we show that it \emph{does} result in approximately correct overall, or average, coverage. There is no objectively correct perspective, but we discuss the differences between them and hope the reader finds the debate illuminating. For the sake of simplicity, we focus on lasso-penalized linear regression, but most of the discussion is relevant to any sparse penalty and loss.

Section 2 examines the underlying concepts in more detail and Section 3 introduces a methodological fix. Then Section 4 examines the performance of the proposed method across a number of simulations and includes a comparison to two previously mentioned HDCI alternatives, selective inference and the bootstrapped de-sparsified lasso. Lastly, in Section 5, we show the application of the proposed method to two data sets, one for acute respiratory illness and the other for gene expression data in mammalian eyes.

\section{Issues with Bootstrapping the lasso}
\label{Sec:Difficulties}

There are two fundamental reasons that the bootstrap fails to be consistent when applied to lasso models. One of these issues requires an adjustment to the bootstrap; the other involves a change in perspective. To facilitate discussion, we will refer to these issues as

\begin{itemize}
\item Epsilon Conundrum, and
\item Individual vs Average Coverage
\end{itemize}

The Epsilon Conundrum is a side effect of the sparsity introduced by the $L_1$ penalty. Because the lasso model is sparse, many of the bootstrap draws will be exactly zero. If we then construct bootstrap percentile intervals, there is a substantial probability that one or both of the endpoints will be exactly equal to zero. If the true value of the coefficient is zero, this does not pose a problem. However, if the true value is not exactly zero, but simply very small (say, a distance $\eps$ from zero), the coverage of the bootstrap interval may be far less than its nominal value.

\pb{We need to be clear here what $\lam$ was; I'm assuming that for each data set, it was $\lam_{\CV}$?} \logan{See below.}

Figure~\ref{Fig:ec} shows that this is exactly what happens when a traditional bootstrapping approach is applied to the lasso. In this example, 100 datasets were generated, each with 94 coefficients randomly set to be about $\pm 3 \times 10^{-10}$ with the other 6 being relatively large in comparison. The plot illustrates 20 CIs along with their corresponding point estimates (filled black circle) and true values (open red circle), as applied to one of the datasets. \logan{Here, as is the case throughout this manuscript, $\lam$ was set to be $\lam_{\CV}$ from 10-fold Cross Validation (CV) applied to the original dataset.} Across all 100 datasets, the average coverage was 26.7\%, far below the nominal coverage rate of 80\%. In Section~\ref{Sec:methods}, we propose a method for addressing this issue, which produces the results on the right side of Figure~\ref{Fig:ec}.

\begin{figure}[hbtp]
  \begin{center}
    \includegraphics[width=0.35\linewidth]{epsilon_conundrum_traditional}\includegraphics[width=0.35\linewidth]{epsilon_conundrum}
    \caption{\label{Fig:ec} 100 datasets were generated under independence with 94 of the coefficients having true values randomly assigned to be approximately $\pm$ 3e-10. The other 6 coefficients were larger in comparison ($\approx \pm 0.62, 0.31, 0.15$). The true values are unrounded and are set s.t. SNR = 1 under the restriction that $\sigma^2$ = 1. The plot provides a subset of 20 of the intervals produced by applying a traditional bootstrapping procedure (left) and the proposed Hybrid bootstrapping procedure (right) to the lasso for a single dataset, randomly selected among the 100 generated. The points indicate point estimates from the lasso for the corresponding intervals and the red circles indicate the true values. The average coverage across all 100 datasets was 26.7\% for Traditional and 93.6\% for Hybrid. Nominal coverage was set to be 80\%.}
  \end{center}
\end{figure}

The coverage reported at the top of Figure 1 is the average coverage across all $p$ confidence intervals. This is not, however, the focus of classical frequentist coverage, which is concerned with achieving proper coverage for each model parameter individually. In a penalized regression model, these can be quite different and we refer to this notion as Individual vs Average Coverage.

\pb{Reorganization started here}

Letting $\cA(\y)$ denote a process that produces an interval based on data $\y$, the coverage probability for the process is defined as $\cvr(\theta) = \Pr\{\theta \in \cA(\y)\}$. Classical frequentist inference requires valid intervals to satisfy $\cvr(\theta) = 1 - \alpha$ for all values of $\theta$ (or potentially $\ge 1 - \alpha$). This is, however, incompatible with Bayesian inference. Bayesian credible intervals cannot, in general, have the same coverage for each $\theta$. What they satisfy instead is maintaining the expected coverage with respect to the prior distribution of $\theta$: $\int \cvr(\theta)p(\theta) \, d\theta = 1 - \alpha$ (this is not the definition of credibility, but it is a consequence, as we show later in this section). Unless the prior is uniform, the coverage of a Bayesian credible interval will be greater than $1-\alpha$ for some $\theta$ and less than $1-\alpha$ for other values of $\theta$.

For example, consider the credible intervals for $\theta$ in a $\Norm(\theta, \sigma^2)$ model with prior $\theta \sim \Norm(0, \tau^2)$ with $\sigma = \tau = 1$. Figure~\ref{Fig:laplace} illustrates the coverage probability for the 80\% Bayesian credible interval over a range of $\theta$ values. Where the prior density for $\theta$ is highest, the coverage is above 80\%, whereas regions where the prior density is low have coverage below 80\%. The expected coverage, however, is exactly 80\%. This is fundamentally true of any Bayesian model with an informative prior, which will never satisfy $\cvr(\theta) = 1 - \alpha$ for all values of $\theta$.

In high dimensional problems, there is yet another quantity we can consider: the average coverage. Rather than integrating over a hypothetical distribution of $\theta$ values, we can average over the distribution of parameters present in the model. In other words, we might choose to require that our intervals satisfy $p^{-1} \sum_{j=1}^p \cvr(\theta_j) = 1-\alpha$. This criteria follows more of a Bayesian perspective than a classical frequentist perspective, although it does not specifically require or involve a prior.

Our goal in this paper is not to argue that one of these perspectives is correct and the others are wrong, but rather that the average coverage perspective is a reasonable one and worthy of consideration. It should not be taken for granted that classical ideas developed for single parameter inference are the best way to approach inference for large numbers of parameters simultaneously. Furthermore, the Bayesian perspective seems to make sense in the context of penalized regression, since these models are intentionally imposing shrinkage towards a prior notion of which values of the parameter are more likely.

In Section~\ref{Sec:methods}, we propose a solution to the Epsilon Conundrum and in Section~\ref{Sec:results} we see that the resulting bootstrap confidence intervals, while they do not satisfy classical coverage requirements, perform quite well with respect to the average coverage criterion. We end this section with a short theorem making the explicit connection between Bayesian credible intervals and average coverage.

\pb{Reorganization ends here}

\pb{Also, I don't think the prior needs to be ``correct'' in order for this theorem to work. If you integrate the coverage with respect to the prior, you get $1-\alpha$, period. That's true regardless of whether the prior is correct or not. I restated the theorem so that it matches the above, but didn't touch the proof yet. Wanted to see what you thought first.} \logan{Okay, yeah I agree now. See updated paragraph after proof based on our discussion.}

\begin{thm}
  \label{Thm:bcc}
  If the likelihood is correctly specified according to the true data generating mechanism, $p(\y | \bt)$, then a $1-\alpha$ credible set for any parameter $\theta_j$ will satisfy $\int \cvr(\theta_j)p(\bt) d\bt = 1 - \alpha$.
\end{thm}

\begin{proof}
Assume the likelihood is correctly specified according to the true data generating mechanism. By definition, a $100(1-\alpha)\%$ credible region for $\theta_j$ is any set $\cA_j(\y)$ s.t. $p(\theta_j \in \cA_j(\y)) = 1 - \alpha$. For a given $\theta_j$, the coverage probability can be defined as $\int I(\theta_j \in \cA_j(\y)) p(\y | \bt)d\y$. Then, averaged over $p(\bt)$, the average coverage probability can be defined as

\as{
  \begin{aligned}
  \int \int I(\theta_j \in \cA_j(\y)) p(\y | \bt)d\y p(\bt)d\bt
  &=  \int \int I(\theta_j \in \cA_j(\y)) p(\y | \bt) p(\bt) d\y d\bt  \\
  &=  \int \int I(\theta_j \in \cA_j(\y)) p(\bt | \y) p(\y) d\y d\bt \\
  &=  \int \int I(\theta_j \in \cA_j(\y)) p(\bt | \y) d\bt p(\y) d\y \\
  &=  \int  (1 - \alpha) p(\y) d\y \\
  &=  1 - \alpha \\
  \end{aligned}
}

\end{proof}

It is worth noting that in the above theorem we are focused on the average coverage of a single $\theta_j$ but still integrated over the joint posterior. As a direct result of the theorem, however, then the average coverage of $p$ intervals will also be equal to $1-\alpha$. \logan{In this manuscript, we in fact are not integrating with respect to a prior distribution. Instead we claim that $\frac{1}{p}\sum \cvr(\theta_j) \approx 1 - \alpha$ if the empirical distribution of $\bt$ closely resembles the prior implied by the lasso penalty. In Section~\ref{Sec:robustness}, however, we find that this generally holds even is the empirical distribution differs greatly.}

% NOTE: I'm commenting this out for now...I think we should make these points, certainly, but Section 2 might not be the right place.
%
% Along with solving the issue of the Epsilon Conundrum, the intervals proposed in Section~\ref{Sec:methods} of this manuscript have a Bayesian feel to them in the sense that they obey the proposed collective definition of coverage. Alternatively, an attempt could be made to debias the intervals in order to meet the traditional frequentest definition of coverage. This proposition of a new definition of coverage is not to say that attempts at debiasing are unwarranted, indeed, meeting the traditional definition of coverage also means that this relaxed definition is satisfied. However, intervals that do not attempt to meet the rigid definition for coverage, using methods such as debiasing, are arguably more faithful to the model fit, an idea we will revisit later.}


\section{Lasso Bootstrap Confidence Intervals}\label{Sec:methods}

In this section, we present an alternative method to the traditional bootstrap for obtaining bootstrap draws in sparse penalized regression models for a given $\lam$. Let $b \in \lbrace 1, \ldots, B \rbrace$ indicate the $b^{th}$ bootstrap draw. The \textbf{Traditional bootstrap} simply takes the point estimate, $\hat{\beta}_j^b$, for each $\beta_j$ for each respective bootstrap sample. As suggested previously, this leads to issues if a large majority of $\hat{\beta}_j^b = 0$ for a given $j$. So we propose the \textbf{Hybrid bootstrap}, which focuses on the full conditional distribution of $\beta_j$, viewed as a Bayesian posterior. The traditional bootstrap draw $\hat{\beta}_j^b$ is the mode of this distribution; we propose instead that when $\hat{\beta}_j^b = 0$, this draw is replaced by a draw from full conditional distribution.

This is the main idea of the Hybrid bootstrap. The remainder of this section presents its specific application to lasso-penalized linear regression. Section \ref{Sec:full-cond} defines and derives the full conditional distributions needed for the sampling and the explicit algorithm for the Hybrid bootstrap is presented in Section~\ref{Sec:implementation}.

\subsection{Full conditional distributions for the lasso}
\label{Sec:full-cond}

In this section, we provide a high level derivation of the full conditional posterior distributions for lasso-penalized regression. Complete details, including how to perform sampling, are provided in Supplement~\ref{Sup:A}.

As with other penalized regression models, the lasso can be formulated as a Bayesian regression model by setting an appropriate prior. This was initially noted by \cite{Tibshirani1996} and explored more extensively by \cite{Park2008}. For the lasso, the corresponding prior is a Laplace distribution, also referred to as the double-exponential distribution: $$\p(\bb) = \prod_{j = 1}^{p} \frac{\gamma}{2}\exp(-\gamma \abs{\beta_j}), \gamma > 0.$$ With this prior, the lasso estimate, $\bbh(\lam)$, is the posterior mode for $\bb$ when $\lam = \gamma \frac{\sigma^2}{n}$.

The full conditional posterior for $\beta_j$ is defined as the distribution for $\beta_j$ conditional on $\bb_{-j} = \bbh_{-j}(\lam)$. Once $\lambda$ is selected, $\bb_{-j}$ is set equal to $\hat{\bb}_{-j}(\lambda)$ and $\r_{j}$, the partial residuals, are then defined as $\y - \X_{-j}\hat{\bb}_{-j}(\lambda)$. A normal likelihood and Laplace prior are not conjugate. However, the full conditional posterior can be shown to be a mixture of right and left truncated normals where the truncation occurs at zero for right and left tails respectively. In this manuscript, we assume that $\X$ has been standardized s.t. $\x_j^T\x_j = n$. Then, as shown in Supplement~\ref{Sup:A}, for $\beta_j$,
\as{
L(\beta_j | \bb_{-j}) &\propto \exp(-\frac{n}{2\sigma^2} (\beta_j^2 - 2z_j\beta_j)), \\
\text{ where } z_{j} &= \frac{1}{n} \x_{j}^{T}\r_{j} \text{, and} \\
p(\beta_j | \bb_{-j}) &\propto
\begin{cases}
C_{-} \exp\{-\frac{n}{2\sigma^2} (\beta_j - (z_j + \lambda))^2\}, \text{ if } \beta_j < 0, \\
C_{+} \exp\{-\frac{n}{2\sigma^2} (\beta_j - (z_j - \lambda))^2\}, \text{ if } \beta_j \geq 0 \\
\end{cases}
}
where $C_{-} = \exp(\frac{z_j \lambda n}{\sigma^2})$ and $C_{+} = \exp(-\frac{z_j \lambda n}{\sigma^2})$.

\pb{I think we need a little more detail here; specifically the idea that (1) draw from $\Unif(0,1)$, then (2) determine which tail we're drawing from, then (3) draw from the corresponding normal.}

\logan{I think 1 belongs and is covered below, but I added more detail for 2/3. I more meant this section just to set up the full conditional. Simplified after discussion in meeting.}

What makes this formulation attractive is that quantiles can be efficiently computed from the full conditional posterior. Not directly from the full conditional posterior, but instead from corresponding normal distributions. In brief, first, with a desired probability, one is able to determine what normal the corresponding quantile should come from. Then, second, a mapping allows the quantile to be computed from that normal distribution.

This solution corresponds to a particular value of $\lam$ and $\hat{\sigma}^2$. Our recommendation is to use cross validation (CV) to select $\lam$ and produce an estimate for $\sigma^2$ and to produce bootstrap confidence intervals corresponding to these values. Specifically when using CV to select $\lambda$ and estimate $\sigma^2$, we recommend using the value of $\lambda$ which minimizes the cross validation error (CVE) and to use CVE as the estimate for $\sigma^2$. We will denote this $\lambda$ as $\lam_{\CV}$. Our recommendation for using the $\lambda$ value which minimizes CVE is because this generally serves as a good estimate of the true value of $\lambda$ when it is known.

\subsection{Implementation}
\label{Sec:implementation}

With the full conditional posterior now defined for the lasso-penalized linear regression, we can provide the algorithm for implementing the Hybrid bootstrap. Let $B$ represent the total number of bootstrap datasets generated and let $\boldsymbol{\X}^b$ and $\boldsymbol{\y}^{b}$ refer to the $b^{th}$ bootstrap sample. Similarly, let $\hat{\beta}^b_j$ refer to the estimate for $\beta_j$ from the lasso fit to $\boldsymbol{\X}^b$ and $\boldsymbol{\y}^b$ at a specified value of $\lam$. The Hybrid bootstrap is then implemented as follows:

\begin{enumerate}
\item Perform CV using the original data to select $\lam$ and estimate $\sigma^2$
\item For b $\in \lbrace 1, \ldots, B \rbrace$:
\begin{enumerate}
\item Obtain a pairs bootstrap sample, $\X^b$ and $\y^b$
\item Fit lasso with $\X^b$ and $\y^b$, obtain $\bbh^b$ and $\boldsymbol{r}_j^b$ corresponding to the selected value of $\lam$ in (a)
\item For j $\in \lbrace 1, \ldots, p \rbrace$:
	\begin{algorithmic}
	\Switch{method}
    \Case{traditional}
      \Assert{$q_j = \bh_j^b$}
    \EndCase
    \Case{hybrid}
      \Assertif{$\bh_j^b \neq 0$}{$q_j = \bh_j^b$}
      \Assertelse{
        $p^* = \Unif(0, 1)$ \\
        \hspace{1.8cm} $q_j = P^{-1}(p^*|\r_{j}^b)$
      }
    \EndCase
	\EndSwitch
	\end{algorithmic}
\item Save $\q$
\end{enumerate}
\item Combine all B $\q$ vectors to obtain a $B \times p$ matrix of bootstrap draws
\item For each $\beta_j$, compute the quantiles for $p_L = \alpha/2$ and $p_U = 1 - \alpha/2$ from the $j^{th}$ column of the draws to produce a final confidence interval estimate with significance level $\alpha$
\end{enumerate}

There are various ways to perform the bootstrap and subsequently obtain confidence intervals \citep{Efron1994}. In this manuscript, we use the pairs bootstrap and quantile intervals as this is a very common way of obtaining bootstrap confidence intervals and requires few assumptions. The hybrid idea proposed here can certainly be applied to other bootstrap approaches, although we restrict ourselves to a single method here in order to maintain focus on the conceptual differences between the Hybrid and Traditional bootstrap.

\logan{In addition, one can imagine several variations on the algorithm as proposed above, specifically with respect to how sampling in performed from the posterior. For example, we considered always drawing from the posterior (not just when $\bh_j^b = 0$) however this resulted in substantial over coverage as we are essentially double accounting for uncertainaty in two ways: the bootstrap and sampling from the posterior. Actually, even the hybrid method as proposed tends to be over conservative as we will show. In order to combat this, a modification could be made in the other direction where we draw from a ``restricted'' version of the full conditional posterior, i.e. draw from $\Unif(0.25, 0.75)$ instead of $\Unif(0,1)$. This would address the epsilon conundrum while reducing the over coverage, although one would need to determine the appropriate level of restriction. In a similar vein, attention could also be given to the estimation of $\sigma^2$ which directly effects the variability of the draws, further consideration of which is given in the Discussion.}

\logan{With this all in mind, just like with the various way to perform the bootstrap, we leave these alternative considerations for exploration in future work in order to maintain focus on the conceptual underpinnings being proposed.}

\pb{Just a thought: Should we mention here that there are two potential variations on this idea? First is to always draw from the posterior. But that's too conservative. Second would be to draw from what we might call the ``restricted full conditional.'' Instead of drawing from $\Unif(0,1)$, we draw from $\Unif(0.25, 0.75)$ or something. This would be less conservative, but I think still turned out OK?}

\logan{In agreeance after discussion, see above}

\section{Results}
\label{Sec:results}

We begin by examining the coverage behavior of the Hybrid bootstrap in what might be considered the ``ideal'' scenario, where the assumptions of Theorem~\ref{Thm:bcc} are met (Section~\ref{Sec:Coverage}) and $\bb$ is generated from a Laplace distribution (to match the prior distribution). We then examine how this coverage is affected by a number of deviation to those assumptions to assess its robustness (Section~\ref{Sec:robustness}). Finally, we compare the proposed confidence interval methods to other confidence interval approaches for penalized regression that have been proposed in the literature (Sections~\ref{Sec:Ridge}~and~\ref{Sec:Comparison}) to draw contrast to methods which attempt to produce debiased intervals.

Unless otherwise noted, the nominal coverage rate in all of these experiments is 80\%.

\subsection{Coverage}\label{Sec:Coverage}

We compare the Hybrid bootstrap CIs to the Traditional bootstrap CIs by showing the coverage of each, both overall and as the magnitude of $\beta$ changes.

As originally pointed out in \cite{Rubin1981}, there is a quasi-equivalence between a bootstrap sampling distribution and a Bayesian posterior. This along with the connection made by Theorem~\ref{Thm:bcc} of average coverage and Bayesian credible intervals suggests, there is reason to expect that the Hybrid bootstrap method has approximately proper average coverage when the empirical distribution of $\bt$ matches the prior implied by the lasso penalty. This \emph{can} occur when each $\beta_j$ independently follows a Laplace (double exponential) distribution. For the simulations in this section, we scaled the coefficients after drawing them so that $\frac{\bb^T\bb}{p} = 1$. With $\sigma^2=100$ and independent features, this results in a signal-to-noise ratio (SNR) of 1. It should be noted that under a constant SNR, the choice of what $\bb^T\bb$ and $\sigma^2$ are set to are arbitrary.

We generated 1000 independent data sets; for each data set, $B = 1000$ bootstrap iterations were drawn for each of the methods described in Section~\ref{Sec:methods}. Each dataset was simulated as follows. $\X$ was generated independently from a $N(0, 1)$ with $n = p = 100$. Then, $\y$ was generated as $\y = \X\bb + \bvep$, where $\veps_i \iid N(0, 100)$. The results are shown in Figure~\ref{Fig:laplace}, where the dotted lines represent the average coverage for each method across all coefficients, while the solid lines are smoothed estimates of coverage as a function of $\beta$. The black line indicates the nominal coverage rate, which is set to be 80\%.

\begin{figure}[hbtp]
  \begin{center}
  \includegraphics[width=0.65\linewidth]{laplace}
  \caption{\label{Fig:laplace} Results are from the simulation described in Section~\ref{Sec:Coverage}. The fitted curves are from Binomial GAMs fit with coverage being modeled as a smooth function of $|\beta|$. The dashed lines represent the average for each method across all 100 independently generated datasets and the solid black line indicates the nominal coverage rate. The shaded distribution in the background depicts the positive tail of the distribution the $\beta$s were drawn from, a Laplace.}
  \end{center}
\end{figure}

\logan{I think it might be beneficial to move true lambda / sigma into the body around here}.

Figure~\ref{Fig:laplace} shows that the traditional bootstrap produces coverage far below the nominal 80\%, as is generally the case when applied to the lasso. Furthermore, the traditional bootstrap has consistently low coverage regardless of the magnitude of $\beta$. This occurs since values of $\beta$ near zero often have intervals with endpoints exactly equal to zero. While this issue improves as $\beta$ increases in magnitude, it is offset by the increasing effect of penalty induced bias. Meanwhile, the Hybrid bootstrap method has coverage fairly close to the nominal 80\%, erroring on the side of over covering. The Hybrid has high coverage rates for values of $\beta$ near zero while still having lower coverage rates for values of $\beta$ larger in magnitude. This pattern is to be expected from intervals arising from procedures that do not attempt to debias. This pattern occurs because for values of $\beta$ near zero, the estimates are often zero (the issue with the traditional bootstrap) at which point sampling from the FCP occurs. Since the true $\beta$ values are near zero and sampling fixes the issue of $(0, 0)$ intervals, coverage levels are near 1 since the lasso penalty is in fact encouraging values near the truth. However, as $|\beta|$, and the average effect of the lasso penalty, increases, this eventually leads to lower coverage approaching that of the traditional bootstrap since sampling from the FCP rarely if ever occurs for values of $\beta$ larger in magnitude. This all leads to a coverage pattern that we would expect from Bayesian credible intervals as depicted on the left hand side of Figure~\ref{Fig:laplace} and as described in Section~\ref{Sec:Difficulties}.

We did consider always sampling from the FCP, not just when $\bh_j^b = 0$. This method wasn't, it just provides intervals significantly wider than that of the Hybrid bootstrap as would be expected. As will be demonstrated throughout the remainder of this manuscript, when the Hybrid departs from nominal coverage, it generally does so in the direction of over-covering. Thus, in these scenarios, the wider intervals are strictly worse than those produced by the Hybrid.

\subsection{Robustness}
\label{Sec:robustness}

We will now focus our attention on how the Hybrid method fares under alternative scenarios. It begins with a look at the coverage behavior when there is correlation among the predictors. The results in this section also give the first depiction of how the behavior changes with increasing sample size. Next, we consider how Hybrid performs under various distributions of $\beta$ and then follow this up with a brief revisit to the Epsilon Conundrum. This section is rounded out with a look at how the coverage behavior changes with changes in the $\lambda$ used.

\subsubsection{Correlation}
\label{Sec:correlation}

Now, we consider the behavior of the Hybrid bootstrap where the covariates are generated under increasing levels of autoregressive correlation. Other than the addition of correlation, the set up of the simulation for the results displayed in Figure~\ref{Fig:correlation_structure} is the same as for the first simulation described. The boxplots provide the coverages across the 100 simulated datasets for each value of n. The amount of correlation starts at $\rho = 0.4$ in the left plot and increases to $0.6$ in the middle plot and $0.8$ in the right. The coverage behavior remains largely intact for increasing levels of correlation, with the main effect being a slight shift downward in coverage. For greater amounts of correlation the shift downward in coverage is a direct result of increased bias due to the correlation between covariates. So, in general, Hybrid fares well in the presence of correlation as it generally behaves conservatively for smaller sample sizes where the presence of correlation is more of a concern. It takes the highest amount of autoregressive correlation ($\rho = 0.8$) to produce coverage that is noticeably below nominal. For a moderate amount of correlation ($\rho = 0.4$), there is little impact on the coverage of the intervals (see Figure~\ref{Fig:laplace_comparison} for $\rho = 0$ as a comparison).

\begin{figure}[hbtp]
  \begin{center}
  \includegraphics[width=0.6\linewidth]{correlation_structure}
  \caption{\label{Fig:correlation_structure} This figure presents results for the simulation described in Section~\ref{Sec:correlation}. The boxplots are for the coverage rates across 100 simulated datasets for the Hybrid method and across three different levels of autoregressive correlation among the covariates, $\rho = 0.4, 0.6, \text{ and, } 0.8$. For this simulation, p = 100, and the results for each combination of method and level of correlation are presented across three different sample sizes, n = $\frac{1}{2}$p, p, 4p. The horizontal black line provides reference for the 80\% nominal coverage rate.}
  \end{center}
\end{figure}

\subsubsection{Distribution of Beta}\label{Sec:Distribution}

Now that we have seen that the proposed methods do relatively well in the presence of correlation, we turn our attention to performance under various distributions of $\beta$. Given that we have previously observed that the coverage for a given $\beta$ depends on its magnitude, it may be expected that the coverage will vary wildly depending on how $\bb$ is distributed. Table~\ref{Tab:dist_beta} shows the results of $\bb$ generated under 7 alternative distributions. The first column of the table gives a depiction of the distributions and details of these distributions are provided in the notes for the table. The simulation set up was again the same as the initial simulation discussed, but with the respective generating mechanisms for $\bb$. While it is certainly the case that the distribution affects the coverage, the underlying patterns remain the same. That is, for smaller values of n, Hybrid over covers, but, as n increases, coverage approaches the nominal coverage rate. Additionally, while distributions with most of their mass near zero result in higher coverage, the opposite behavior is not observed. That is, even when most of the density is concentrated away from zero, as with the Beta distribution, the desired coverage properties largely remain intact. Lastly, note, that between n = 400 and n = 1000, there is little change in the coverage rates for distributions that were already near 80\% coverage, and for the others (T, Sparse 1, Sparse 2), although slow, coverage still trends towards nominal.

\begin{table}[hbtp]
  \centering
  \input{tab/distribution_table_reed}
  \caption{\label{Tab:dist_beta_reed} Results are from the simulation described in Section~\ref{Sec:Distribution}. The nominal coverage rate is 80\%. The setup is the same as the previous simulations, except with $\bb$ being generated under 7 alternative (to the laplace) distributions and the addition of a fourth sample size, n = 1000. To maintain the specified SNR of 1, $\bb$ is normalized. Prior to normalization, Sparse 1 had $\bb_{1-10} = \pm(0.5, 0.5, 0.5, 1, 2)$ and $\bb_{11-100} = 0$. Sparse 2 had $\bb_{1-30} \sim N(0, 1)$ with the rest equal to zero.  Sparse 3 had $\bb_{1-50} \sim N(0, 1)$ with the rest equal to zero. All distributions of $\bb$ were centered at zero. For normal, laplace, and uniform, after normalization, the original scale is arbitrary. For the T distribution, df was set to 3 and the Beta distribution was generated from Beta(0.1, 0.1) - 0.5. Note that the distribution for Sparse 1 is fixed and that for Sparse 1 and 2 that, like for the Normal, the choice of scale for the non-zero coefficients is arbitrary.}
\end{table}

\subsubsection{Epsilon Conundrum}\label{Sec:Epsilon}


Its natural to now consider the performance of the hybrid method in the scenario that was the main motivator of the method: the Epsilon Conundrum. The simulation set up used to assess the Epsilon Conundrum is essentially just another distribution of $\bb$ values, although one that can prove difficult for methods which fail to take proper precautions. The setup for this simulation was described in Section~\ref{Sec:Difficulties}. As seen in Figure~\ref{Fig:ec}, the Hybrid CIs avoid the downfall we previously observed in Figure~\ref{Fig:ec} when applying the Traditional bootstrap to the same simulation set up. Although only a subset of the intervals are displayed in Figure~\ref{Fig:ec}, it is evident that the intervals avoid a precipitous drop in coverage for values where $\beta_j$ is near zero by producing intervals that are sufficiently wide and without endpoints not equal to zero. Recall, the Hybrid approaches this issue by sampling from the full conditional posterior when $\hat{\beta}_j^b = 0$. Contrary to Traditional, Hybrid does over cover on average due to the fact that a large majority of $\beta$s are near zero (recalling the tendency of Hybrid to over cover near zero).

\subsubsection{Selection of \texorpdfstring{$\lambda$}{lambda}} \label{Sec:lambda}

Since CV is non-deterministic, $\lambda_{\CV}$ likely takes on a range of values depending on the seed set, so there maybe be question about what happens if the value of $\lambda$ is changed. The data in Figure~\ref{Fig:beta_lambda_heatmap_laplace} comes from a simulation that addresses just that. The data generating mechanism for this simulation is identical to that for Section~\ref{Sec:Coverage}. In this simulation, however, $\lambda$ was evenly distributed on the $\log_{10}$ scale from $\lam_{\max}$ to $\lam_{\min} = \lam_{\max} * 0.001$. At each value, confidence intervals were obtained and coverage was recorded. Note that at each value of $\lam$, $\sigma^2$ was re-estimated using the CVE at that value of $\lam$. This was repeated 100 times, then a gam was fit to provide a smooth estimate of the coverage rate by $\lambda$ and $|\beta|$. Relative coverage is defined here as the estimated coverage rate minus the nominal coverage rate. The x-axis for $\lambda$ is presented relative to $\lam_{\max}$ and the black lines delineate the range of $\lam_{\CV}$ over the 100 simulations. The red line indicates the average $\lambda_{\CV}$ while the blue line represents the value of $\lambda$ which provided coverage closest to that of nominal.

\begin{figure}[hbtp]
  \begin{center}
  \includegraphics[width=0.6\linewidth]{beta_lambda_heatmap_laplace.png}
  \caption{\label{Fig:beta_lambda_heatmap_laplace} The heatmap displays results for the simulation described in Section~\ref{Sec:lambda}. 100 datasets were generated as with previous simulations with n = p = 100, with $\beta \sim Laplace$ and $\X$ generated under independence from a $N(0,1)$. Then, each method was fit across a range of $\lam$ values and confidence intervals and their coverages were obtained and then used to fit a gam to estimate the coverage as a smooth function of the $|\beta|$ and $\lam$. The x-axis for $\lambda$ is presented relative to $\lam_{\max}$ and the black lines delineate the range of $\lam_{\CV}$ over the 100 simulations. The red line represents the relative average CV selected value of $\lam$. The blue line indicates the relative $\lam$ value that provided coverage closest to the 80\% nominal coverage rate.}
  \end{center}
\end{figure}

We will focus on the range of $\lam_{\CV}$ as this is where the most interesting pattern occurs and which falls under our recommendation for selecting $\lam$. Depending on the value of $\lam$, $\beta$s closer to zero see moderate amounts of over coverage which decreases, eventually leading to under coverage, as the magnitude of $\beta$ increases. The $|\beta|$ where this transition occurs (and at which coverage of a specific value equals nominal) varies considerably over the range of $\lam$. At $\lam_{\max}$, this transition occurs at a relatively small $|\beta|$. As the value of $\lam$ decreases from $\lam_{\max}$ this transition occurs at increasingly large values of $\beta$ until all values of $\beta$ have an estimated coverage rate at or above that of nominal. To obtain an average coverage rate near that of nominal, $\lam$ needs to be selected such that over coverage for smaller $|\beta|$s and under coverage for larger $|\beta|$s is balanced. The blue line serves as a general representation of where this tradeoff is met. In this scenario, and in general, $\lam_{\CV}$ does a reasonable job at providing such a balance across the distribution of $\beta$ values. However, it should be no surprise by this point that the average $\lam_{\CV}$ (the red line) is to the right of the relative $\lambda$ value that provided closest to nominal coverage (the blue line), as we have seen previously that the $\lambda$ which minimizes CVE tends to produce over coverage when n = p. That is, as $\lam$ is decreased from the blue line, the average coverage increases above nominal as a greater proportion of the distribution of $\bb$ experiences over coverage.

It should be reemphasized that, as implemented, the estimate of $\sigma^2$ depends on the choice of $\lambda$. That said, by definition, the estimates are increasing in either direction of $\lambda_{\CV}$. So, the general pattern previously described is likely more due to the direct effects of $\lambda$ as opposed to an indirect effect of the estimate of $\sigma^2$, at least over the range of $\lam_{\CV}$ and as estimated using CVE. However, the over coverage at $\lam_{\CV}$ is more of a result of the over estimate of $\sigma^2$, see Supplement~\ref{Sup:B}. Briefly, setting $\lam$ equal to its true value (which we know since $\beta \sim Laplace$), while still estimating $\sigma^2$ using CVE but just at the true value of $\lam$, leads to coverage very similar to that when $\lam$ is selected using CV, likely because $\lam_{\CV}$ was generally near the true value of $\lam$ in the simulations considered. In contrast, if $\sigma^2$ is also set to its true value, coverage was very near nominal regardless of the sample size. Thus, alternative methods to estimating $\sigma^2$ could be considered to reduce the degree of over coverage, a topic which is further explored in the Discussion.

Finally, the balance of over and under coverage, of course, depends on the true distribution of $\bb$ (recall that here it is Laplace). However, as we saw in Section~\ref{Sec:Distribution}, $\lam_{\CV}$ generally does well regardless of the distribution of $\beta$, with average coverage behavior being similar across various distributions of $\beta$, so a similar pattern would be expected for other distributions.

\subsection{Comparison to Ridge Regression CIs}\label{Sec:Ridge}

We now turn attention to a comparison of the intervals produced by Hybrid to those produced by Ridge regression. In this example again n = p = 100, however, only one $\beta$ is non-zero. That is, $\beta_{A_1} = 1$ and $\beta_{B_1}, \beta_{N_1}, \ldots, \beta_{N_{98}} = 0$. Additionally, the data are simulated such that $\rho(\beta_{A_1}, \beta_{B_1}) = .99$ but all of the N (noise) $\beta$s are uncorrelated with $\beta_{A_1}, \beta_{B_1}$, and each other.

100 datasets were generated in this manner and each method was applied and respective confidence intervals obtained.

Figure~\ref{Fig:highcorr} depicts the results from the simulation with the plots on the left giving box plots for the lower (red) and upper (blue) bounds across the 100 datasets for 3 variables, $A_1$, $B_1$, and $N_1$. On the right, confidence intervals for the first 20 variables for a randomly selected example from the 100 simulated datasets is displayed.

Focusing on $A_1$ and $B_1$, what we would hope to see is wide intervals centered somewhere near 0.5. Although $A_1$ truly is the signal variable, its high correlation with $B_1$ should produce a large amount of uncertainty about which variable (if not both) contain signal. Ridge, however, fails in this respect in that the intervals are narrow, with little distinguishable difference in width between $A_1$ and $B_1$ and the noise variables. On the other hand, Hybrid displays more desirable behavior. The uncertainty entangled in $A_1$ and $B_1$ is reflected with wider intervals. Additionally, intervals for $A_1$ usually do not contain 0 whereas the intervals for $B_1$ contains 0 over 40\% of the time. Ridge, on the other hand, produces no intervals that contain zero for $B_1$ and on the flip side hardly ever produce intervals with upper bounds above $0.5$ for $A_1$. Additionally for hybrid, overall, there is a clear shift in the intervals for $A_1$ compared to $B_1$ suggesting that even with very high correlation, the Hybrid bootstrap often attributed more of the signal to $A_1$, which is not the case with Ridge.

\begin{figure}[hbtp]
  \begin{center}
  \includegraphics[width=0.6\linewidth]{highcorr}
  \caption{\label{Fig:highcorr} Provides results for simulation described in Section~\ref{Sec:Ridge}. The right plots show a single example of a intervals produced by Ridge (top), Elastic Net (middle) and the Hybrid bootstrap (bottom) from one (randomly selected) of the 100 datasets for the first 20 variables. The left plot summarizes the resulting CIs for the variables $A_1$, $B_1$, and $N_1$ across the 100 simulations. The blue box plots provide the distributions for the upper bounds and the red boxplots for the lower bounds. Note, nominal coverage for this simulation was 80\%.}
  \end{center}
\end{figure}

Additionally, between the results for Ridge and the results for lasso are results for Hybrid intervals for Elastic Net with $\alpha = 0.8$. These intervals are obtained by using a data augmentation method that allows Elastic net to be fit using the algorithms for fitting lasso. The main point of including this is a demonstration that the methods implemented in \texttt{ncvreg} are not strictly limited to the lasso penalty.

\subsection{Comparison to Other HDCI Methods}\label{Sec:Comparison}

There are few other methods for obtaining intervals for the lasso and even fewer with implementation in companion R packages to allow for easy usage and comparison. Two that we were able to identify were Selective Inference (SelInf) introduced by \cite{LeeEtAl2016} and implemented in \texttt{selectiveInference} and Bootstrap Lasso Projection (BLP) introduced by \cite{Dezeure2017} and implemented in \texttt{hdi}.

Note that both of these methods have markedly different behavior than the Hybrid method proposed here and also from each other. This can be understood by reviewing the methodological reasoning behind the these two alternatives even without getting into any results. To review, BLP proposed by \cite{Dezeure2017} proposed a method for bootstrapping the Low Dimension Projection Estimator (LDPE) proposed by \cite{ZhangZhang2014}. LDPE provides a method to debias the original point estimates from a lasso fit to facilitate more traditional forms of inference. LDPE is also known as the de-biased or de-sparsified Lasso. The process of debiasing is involved and makes it ambiguous how the results relate to a specific lasso model. Alternatively, SelInf aims to account for the uncertainty in model selection by conditioning on the selected model. SelInf doesn't directly correct for the bias introduced by penalization but because of its formulation does only provide intervals for variables selected and generally provides consistent coverage regardless of the magnitude of $\beta$, something we would expect from a debiased method. As a result and as we will see in Section~\ref{Sec:RDA}, the intervals produced by these alternatives are often questionable in relation to their corresponding point estimates, especially when p is greater than n, which leads to an idea we will expand on that the Hybrid intervals are valuable in the sense that they remain more faithful to the lasso model fit.

\subsubsection{Similation Study}

\begin{figure}[hbtp]
  \begin{center}
  \includegraphics[width=0.65\linewidth]{laplace_comparison}
  \caption{\label{Fig:laplace_comparison} Results are from the simulation described in Section~\ref{Sec:Comparison} and identical in setup to that of Section~\ref{Sec:Coverage}. The fitted curves are from Binomial GAMs fit with coverage being modeled as a smooth function of $|\beta|$. The dashed lines represent the average for each method across all 100 independently generated datasets and the solid black line indicates the nominal coverage rate. The shaded distribution in the background depicts the positive tail of the distribution the $\beta$s were drawn from, a Laplace.}
  \end{center}
\end{figure}

This simulation study compares the performance of the methods using their default parameters. With that said, as implemented in \texttt{hdi}, there is no way to specify the $\lambda$ used for BLP, which defaults to the 1-SE solution from \texttt{cv.glmnet}. Additionally, by default, $\lam$ is also re-selected for each bootstrap draw. For the simulation study, we use this ``out of the box'' setup, however, it was unsatisfactory for the comparisons in the subsequent data analyses. For the data analyses, we forked the \texttt{hdi} repository and adjusted it to allow for the specification of $\lambda$. Specifically, to allow for the use of the $\lambda$ which minimizes CVE. We did attempt to use this version in the simulation study, but this highlighted a limitation for BLP which made such usage infeasible under the desired setup. Specifically, BLP only works when the number of non-zero coefficients is less than (not equal to) n, something that is more likely to occur for the 1-SE $\lambda$ than for the $\lambda$ which minimizes CVE. As the simulation is set up, this proved prohibitive as more often than not when n was small relative to p, the method resulted in an error. Although it would be ideal to have the option to compare the methods under the same value of $\lambda$, there is something to be said for comparing the performance of the methods as closely as possible to what a user would experience if they started using one of the methods with its default arguments. However, it must be emphasized that this means the $\lambda$ values used for BLP vary greatly from those used for SelInf and Hybrid, which both use a single $\lambda$ for each dataset with the $\lambda$ used being the value which minimizes CVE from \texttt{cv.glmnet} and \texttt{cv.ncvreg}, respectively, applied to the original dataset\footnote{\texttt{cv.glmnet} and \texttt{cv.ncvreg} generally select a similar value for $\lambda$. \texttt{cv.glmnet} was used for SelInf since \texttt{cv.glmnet} is what is used in examples for the \texttt{selectiveInference} package and \texttt{cv.ncvreg} was used for Hybrid since the Hybrid bootstrap is implemented in \texttt{ncvreg}.}. HDI, on the other hand, has B $\lambda$ values (not returned) chosen as the 1-SE $\lambda$ from \texttt{cv.glmnet} applied to each of the B bootstrap draws from the original dataset.

\begin{figure}[hbtp]
  \begin{center}
  \includegraphics[width=0.65\linewidth]{laplace_other}
  \caption{\label{Fig:laplace_other} Results are from the simulation described in Section~\ref{Sec:Comparison}. Each plot provides corresponding results for each of BLP, Hybrid, and SelfInf for three different sample sizes. The top plot provides boxplots of coverages, the middle plot provides boxplots of the median widths, and the bottom plot provides boxplots of the run times, all across the 100 simulated datasets. Note that the plots for median width and for run time both have y-axes on the $\log_{10}$ scale.}
  \end{center}
\end{figure}

The simulation results presented here are from a set up identical to that described in Section~\ref{Sec:Coverage}. In fact, the results for Hybrid are the same as those used for the earlier figures, just displayed differently. Referencing Figure~\ref{Fig:laplace_comparison}, both BLP and SelInf initially appear to perform strikingly well. Both have coverage near that of nominal and lack the coverage pattern seen with the Hybrid method, instead providing consistent coverage regardless of the magnitude of $\beta$. However, Figure~\ref{Fig:laplace_other} tells a different story. First, although BLP provides average coverage rates the closest to $80\%$, there were a number of cases where the coverage dipped significantly. Potentially more concerning is that when $n = 400$, the coverage noticeably drops below 80\%. Although the lasso is generally thought of as a model for high dimensional data, it is used across the entire spectrum of datasets, so it would be preferred to see convergence towards the nominal rate of coverage. The behavior for the Hybrid Bootstrap has been covered previously, specifically that it generally over covers when $n < p$ but has coverage near nominal as n increases above p. Additionally, although not immune to under coverage, it performed more reliably than the other two methods. Before moving onto the results for SelInf, there are a couple numbers not in Figure~\ref{Fig:laplace_comparison} which are also important to consider and which are provided in Table~\ref{Tab:selective_inference}. The first column gives the average number of variables included on average across the simulations (that succeeded) which also represents the number of variables on average that confidence intervals were provided for. The second column indicates that when n = 50, 20 of the 100 simulations errored out, with only 6 erroring out when n = 100, and none when n = 400. It is this subset (represented by the first and second columns) that is included in the coverage plot for SelInf. Although on average SelInf's coverage is near nominal, the coverages for individual simulations are sporadic, ranging from 0 to 1. The fact that SelInf does not produce intervals for all variables also helps explain the sporadic coverage behavior, since its coverage values are averaged over fewer variables. Accordingly, there are also a considerable number of iterations that had coverage below 50\%, an issue which is not remedied even with larger values of n. So, even if on average SelInf looks good, the underlying behavior is much less desirable.

\begin{table}[hb]
  \centering
  \begin{tabular}{ccccc}
  \hline
  & Variables & \multicolumn{3}{c}{Simulations} \\
  \cmidrule(lr){2-2}\cmidrule(lr){3-5}
  n & \# Included on Average & \# Succeeded & \# Non-finite Median Width & \# Any Non-finite Width \\
  \hline
  50 & 18.5 & 80 & 18 & 40 \\
  100 & 30.4 & 94 & 12 & 63 \\
  400 & 70.1 & 100 & 3 & 55 \\
  \hline
  \end{tabular}
  \caption{Additional information on the results for SelInf in the simulation described in Section~\ref{Sec:Comparison}. The first column indicates the average number of variables included (non-zero) in the lasso model at a given sample size. Note, this column also applies to BLP and Hybrid. The second column indicates the number of simulations that actually succeeded. A majority of the errors occur due to too large of a $\lambda$ value being selected using CV (causing all coefficients to equal zero), however, a failure to satisfy the polyhedral constraint also was a source of some errors. The third and fourth column indicate the number of simulations, among those that succeeded, that had a non-finite median width or any interval with an infinite width, respectively.}
  \label{Tab:selective_inference}
\end{table}

Staying with SelInf but directing our attention towards the interval widths, the concern surrounding SelInf only grows. Referring to column 3 of Table~\ref{Tab:selective_inference}, of the 80 simulations that suceeded for n = 50, the median width of the intervals produced (from the variables included in the model, column 1) was infinite for 18 of the simulations. For n = 100, 12 of the 94 simulations had infinite median widths. By n = 400, at which point one would hope this issue was eliminated, still 3 of the 100 simulations had infinite median widths. Column 4 gives the number of simulations with at least one interval with an infinite width. Additionally, we can see in the second plot of Figure~\ref{Fig:laplace_other}, which excludes the simulations with infinite median widths, even when the medians were finite, they were nearly always extremely wide, even for n = 400 (note the y-axis is on the log scale). This behavior was also observed when applying the method to real data sets which will be covered in Section~\ref{Sec:RDA}. BLP and Hybrid on the other hand produce intervals which are more similar in width although BLP does tend to produce wider intervals and with a bit more variability. As expected, the interval widths for three methods decrease with sample size.

The third plot of Figure~\ref{Fig:laplace_other} provides boxplots of the run times for each of the methods. The runtime differs considerably between the methods with SelInf being the fastest and BLP being by far the slowest with about an order of magnitude difference separating the methods respectively (note the y-axis is on the log scale). Additionally with BLP, there is an odd non-monotonic behavior which we looked into briefly but could not find an explanation for. This behavior occurred in all reruns of the same simulation. Hybrid and SelInf both have a monotonically increasing relationship with sample size, although Hybrid is more affected. In our testing, speed was not a concern for SelInf, was noticeable for Hybrid, and prohibitive for BLP which will be returned to in Section~\ref{Sec:Scheetz2006}. BLP is particularly slow with its default arguments because of the reselection of $\lambda$ (using CV) for each iteration of the bootstrap.

\logan{I could see the reader questioning... well if you overcame a similar issue (just noted how many errors occured) for SelInf... why not do the same for BLP? The issue for BLP however is far worse because an error can potentially arrise in any bootstrap iteration whereas SelInf has to work just once, for the original dataset. That is, BLP fails much more miserably.}

\logan{I did start to track down the error for BLP and it seems to be due to a NaN showing up where they weren't expecting and it not being handled properly. However, I think it would take a considerable amount of time to find the root of the error and potentially fix it... which I could do but not sure if it is worth the effort.}

\section{Real Data Analysis}\label{Sec:RDA}

We conclude the results section by considering the intervals produced by the Hybrid and Debiased bootstrap applied to two datasets: \texttt{whoari} (World Health Organization study on acute respiratory illnesses) and \texttt{Scheetz2006} (Gene expression in the mammalian eye). These two datasets sit on opposite ends of the spectrum in terms of dimensionality. \texttt{whoari} contains 816 observations and 66 features while \texttt{Scheetz2006} contains just 120 observations but with 18975 features.

In this section, we also compare the intervals produced by the proposed methods to those of BLP and SelInf. In this comparison, we wanted the results for the real data analysis to correspond directly back to a single set of point estimates for all three methods in order to put a clear emphasis on how the intervals relate to the corresponding estimates from the lasso fit using a selected value of $\lambda$. The $\lambda$ of interest is the value which minimizes CVE, selected here using \texttt{cv.glmnet}. As previously discussed, this is not an issue for Hybrid or SelInf, which both allow $\lambda$ to be specified, but does require adjustments for BLP\footnote{The unmodified results for BLP are provided in a Supplement~\ref{Sup:C}}. Since \texttt{hdi} was not set up with the flexibility to allow for the specification of $\lambda$, we forked the \texttt{hdi} repo and made the necessary modifications. Recall, without making the adjustment, the option is fixed to be the 1-SE solution from \texttt{cv.glmnet} (although this is not indicated in \texttt{hdi}'s documentation). The second adjustment relative to the ``out of the box'' set up is supported in the arguments for BLP. This adjustment is to set \texttt{boot.shortcut = TRUE}. From hdi's documentation, if \texttt{boot.shortcut = TRUE}, ``the lasso is not re-tuned for each bootstrap iteration, but it uses the tuning parameter computed on the original data instead.'' With these modifications, then, BLP has the same behavior as the other methods. Lastly, it should not be ignored that BLP does provide its own estimates. However, the connection to the the original lasso fit is obscured, so for the purpose of this comparison, we use the the same point estimates across the three methods.

\subsection{World Health Organization study on acute respiratory illnesses (whoari)}\label{Sec:whoari}

\begin{figure}[hbtp]
  \begin{center}
  \includegraphics[width=0.6\linewidth]{comparison_data}
  \caption{\label{Fig:comparison_data_whoari} Confidence intervals produced by three different methods for all 66 variables in the \texttt{whoari} dataset described in Section~\ref{Sec:whoari}}
  \end{center}
\end{figure}

The \texttt{whoari} dataset comes from the study ``Development of a clinical prediction model for an ordinal outcome: the World Health Organization multicentre study of clinical signs and etiological agents of pneumonia, sepis and meningitis in young infants'' written by \cite{Harrell1998}. The study considered a few acute illness in young infants across several countries, and the dataset used here is a subset of 816 infants who presented with pneumonia in the country Ethiopia. As alluded to in the title, collection of this data was done with the intention of building a prediction model to assess the severity of an infant presenting with a serious infection, which represents the main cause of morbidity and mortality in these developing countries for infants under 3 months of age. Diagnosis of severity is a difficult task, and developing rules for grading the severity of disease is important for prompt delivery of treatment for those who need it while also avoiding unnecessarily and costly treatments where possible. The outcome considered here is ordinal (taking on a number from 1 - 5), however, for simplicity we treat the outcome as continuous and feel that the results are reasonable, at least for a comparison of the three methods under consideration. The variables collected contain information on vital signs, family history, and clinical observations and represent a range of data types from binary to ordinal to continuous. With $N \approx 10p$, this dataset is not necessarily high dimensional, but sits on the edge of where classical methods and their resulting inferences may be questionable. Additionally, the use of a model that produces sparsity is beneficial both for interpretation and for ultimately determining factors for assessment in practice, where obtaining predictions from a model may be prohibitive.

Figure~\ref{Fig:comparison_data_whoari} provides the confidence intervals from each of the three methods along with corresponding point estimates. Recall, the point estimates are the same across all three methods and come from the lasso fit on the original data with $\lambda$ selected using \texttt{cv.glmnet}. It is important to emphasize that the range of the x-axis is different for each of the plots corresponding to the three methods. The Hybrid produces the narrowest intervals and SelInf produces by far the widest. Despite the difference in widths, Hybrid and BLP share similar patterns, however, the conclusions that might be drawn could conceivably be quite varied. This may most easily be observed by considering a common point of interest: whether or not an interval includes zero. BLP produces 3 intervals that do not contain zero, SelInf produces 13 that do not contain zero, and Hybrid produces 21 that do not contain zero. SelInf producing 13 intervals not containing zero is only part of the picture, since it is also important to note SelInf selects (and provides intervals for) 37 of the 66 variables and only produces one interval here with an infinite bound (for \texttt{abb}). That said, there are a number of intervals that are unreasonably wide and we believe it would be difficult to provide a convincing interpretation for the intervals produced by SelInf. The comparison between Hybrid and HDI is more interesting since, visually, they appear similar. The patterns observed here are similar to those in the preceding simulation study. As previously mentioned, the intervals from Hybrid are narrower on average. Additionally, while Hybrid's intervals, relative to the point estimates, tend to be skewed towards zero or symmetric, BLP's intervals are often skewed away from zero as a result of debiasing. It appears then, that BLP's higher coverage at larger values of $\beta$, as seen in the simulation study, is likely due to a combination of increased width and the skewness induced by debiasing.

\subsection{Gene expression in the mammalian eye (Scheetz2006)}\label{Sec:Scheetz2006}

\begin{figure}[hbtp]
  \begin{center}
  \includegraphics[width=0.7\linewidth]{comparison_data_scheetz}
  \caption{\label{Fig:comparison_data_scheetz} Confidence intervals produced by three different methods for the 20 variables with the largest absolute point estimates in the \texttt{Scheetz2006} dataset described in Section~\ref{Sec:Scheetz2006}}
  \end{center}
\end{figure}


The \texttt{Scheetz2006} data was obtained from the study ``Regulation of gene expression in the mammalian eye and its relevance to eye disease'', written by \cite{Scheetz2006}. This study involved measuring the RNA levels from the eyes of 120 rats. Of 31000 different probes used, 18976 were detected at a sufficient level to be considered ``expressed''. For this analysis we treat one of the genes, Trim32, as the outcome since it is known to be linked to the genetic disorder Bardet-Biedl Syndrome (BBS). The remaining 18975 genes are used as covariates with the goal of determining other genes which may have expression correlated with Trim32 and thus may also contribute to BBS.

In the simulation study, we saw that when p is large relative to n, SelInf's difficulties are amplified. Applied to \texttt{Sheetz2006}, where $p > 100n$, the issues are unignorable. SelInf provides intervals for 66 of the 18975 features however, every single one of them has a lower or upper bound that is infinite. Additionally, for the bounds that are finite, they are all also extremely large compared to their respective point estimates and in comparison to the intervals produced by Hybrid and BLP. Additionally, none of the intervals contain zero. Potentially even more odd is that of the 66 intervals, 62 of them were completely of the opposite sign as the corresponding estimate.

Like with \texttt{whoari}, Hybrid and BLP produce similar intervals, however with their characteristic differences more prominent due to the dimensionality of the dataset. As such, the intervals of Hybrid are more drawn towards zero. Alternatively, however, the intervals for BLP often do not contain the lasso point estimate or barely do, a point that will be considered further in the discussion.

Despite these differences, depending on the perspective taken, there is not a large discrepancy for the variables deemed significant by the two methods. BLP produces 6 intervals which exclude zero, while Hybrid produces 3. This is twice as many, but in the grand scheme of nearly twenty-thousand variables, this is a relatively minor difference. Both methods have intervals not containing zero for \texttt{1389910\_at}, \texttt{1378319\_at}, and \texttt{1385395\_at}, while BLP produces intervals which do not contain zero for three additional genes.

That said, we emphasize again that a user is not able to obtain these results from the implementation in \texttt{hdi}. Additionally, even with \texttt{boot.shortcut = TRUE} (where $\lambda$ is not reselected for each bootstrap iteration), on a MacBook Pro with 16 GB of RAM and an Apple M1 Pro chip, BLP took over 6 hours to run. This is not all that surprising, however, since LDPE is already computationally expensive, and while bootstrapping provides additional benefits, this only adds to the computational burden. That said, for large p, the expense of bootstrapping is still overshadowed by the p lasso regressions needed for $X_j$ on $\X_{-j}$. \logan{Should update with run on HPC / with more specific time details.}

\section{Discussion}

Since the main method suggested here falls somewhere between the Traditional Bootstrap and the Posterior Bootstrap, we propose the full name as the Posterior Adjusted Traditional Hybrid (PATH) Bootstrap.

The under coverage of coefficients for larger values of $\beta$, admittedly the coefficients that are often of most interest, may at first glance appear to be a major flaw of the bootstrap alternative proposed here. However, there are two observations we believe suggest otherwise.

The first redirects attention back to Theorem~\ref{Thm:bcc}. The coverage properties of the bootstrapped lasso intervals here mimic closely the behavior of the intervals produced by a corresponding Bayesian analysis. Average coverage, weighted by the likely distribution of $\beta$ values is maintained near that of nominal rather than focusing on long run coverage rates at all values of $\beta$. Would one really dismiss a Bayesian analysis on this same account? \logan{I am wondering if spending some time here elaborating on a 2x2 table of selection uncertainty vs. significance on one side and overall and single on the other would be convincing. I.e. Hybrid falls in that overall significance category? Obviously would put into much more elegant terms.}

The second observation is that this behavior is directly related to the properties of the lasso. If there is issue with the coverage of larger coefficients, then we might suggest that the lasso is not the model the practitioner is looking for rather than dismiss the intervals themselves. Alternatively, one might consider other penalities which reduce the bias introduced. One alternative would be to consider the same proposed bootstrap methods but applied to the Minimax Concave Penalty (MCP). The MCP penalty closely resembles that of the lasso near zero but eventually levels out to a constant penalty for larger values of $\beta$ unlike the lasso which applies a penalty proportional to the magnitude of $\beta$. Further exploration of the behavior under alternative penalties is outside the scope of this manuscript, but something that warrants further consideration, especially in contrast to methods that explicitly attempt to debias during inference.

This leads us back to the observations seen in Section~\ref{Sec:Scheetz2006}. The remedy, admittedly a natural one, to this issue would be to provide intervals that are debiased. However, as we saw with BLP, this can lead to intervals which coincide with the original lasso estimates. To be fair, this was not something that the authors of BLP were attempting to do. BLP is essentially producing an alternative model to the lasso, the de-biased or de-sparsified lasso. This should be emphasized. The intervals produced by BLP (and by other debiasing methods) are producing inference on $\beta$ but not inference for a specific lasso fit. This is why earlier in the manuscript we alluded that methods like Hybrid that take an alternative perspective on coverage produce intervals that are faithful to a specific lasso model.

A fair question might be how to interpret intervals which do not have constant coverage. However, we believe the best advice would to be to treat them no differently than any other interval. In such cases, the interpretation will be conservative. If the true value of $\beta$ is small, the coverage is likely much higher than that of nominal. On the other hand, if $\beta$ is large, the coverage is likely below nominal, but the interval likely understates the effect.

The estimate for $\sigma^2$ used throughout this manuscript was based on the CVE. Alternative estimates would affect the resulting width of the Posterior and PATH bootstrap intervals which certainly would impact coverage. We do not explore the impact of alternative methods for estimating $\sigma^2$ here, but \cite{Reid2016} provides a comprehensive overview for estimating error variance for the lasso. Given that CVE generally over estimates $\sigma^2$, an estimate with less bias may result in more desirable coverage behavior for smaller sample sizes. Regardless, we have found that using CVE provides a level of robustness as it does a good job indicating the uncertainty around the estimates.

Further work could extend or propose alternative methods that are more closely calibrated to the nominal coverage rates at any value of lambda.
