\section*{Supplemental materials}

\begin{appendices}
\renewcommand{\thesection}{\Alph{section}}
\renewcommand{\thefigure}{\Alph{figure}}

\section{Sampling from the Full Conditional Posterior}
\label{Sup:A}

\logan{Discuss notation}

The lasso can be formulated as a Bayesian regression model with the prior $\p(\bb) = \prod_{j = 1}^{p} \frac{\gamma}{2}\exp(-\gamma \abs{\beta_j})$ for $\gamma > 0$ and with $\lam = \gamma \frac{\sigma^2}{n}$. With $\gamma$ written in terms of $\lambda$, $\sigma^2$, and $n$, then the prior on $\beta_j$, conditional on $\bb_{-j}$, is proportional to $\exp(-\frac{n \lambda} {\sigma^2} \abs{\beta_j})$. Similarly, the full conditional likelihood can be simplified. Instead of conditioning on $\y$, $\X$, and $\bb_{-j}$ in what follows, we simply condition on $\r_{-j} = \y - \X_{-j}\bb_{-j}$ and implicitly condition on $\x_j$. Assuming $\X$ has been standardized s.t. $\x_j^T\x_j = n$,

\as{
  \begin{aligned}
  L(\beta_j|\r_{-j}, \lambda, \sigma^2) &\propto \exp(-\frac{1}{2\sigma^2} ||\y - \X\bb||_2^2) \\
  &= \exp(-\frac{1}{2\sigma^2} ||(\y - \X_{-j} \bb_{-j}) - \x_{j} \beta_{j}||_2^2) \\
  &= \exp(-\frac{1}{2\sigma^2} ||\r_{-j} - \x_{j} \beta_{j}||_2^2) \\
  &\propto \exp(-\frac{1}{2\sigma^2}( - 2\x_{j}^T\r_{-j} \beta_{j} + \x_{j}^T\x_{j} \beta_{j}^2)) \\
  &= \exp(-\frac{1}{2\sigma^2}( - 2 n z_{j} \beta_{j} + n \beta_{j}^2)) \\
  &= \exp(-\frac{n}{2\sigma^2}(\beta_{j}^2 - 2 z_{j} \beta_{j})), \\
  \end{aligned}
}

where $z_{j} = \frac{1}{n} \x_{j}^{T}\r_{-j}$.

With this the form of the full conditional posterior can be worked out as follows:
\as{
\Rightarrow P(\beta_j | \r_{-j}) &\propto \exp(-\frac{n}{2\sigma^2} (\beta_j^2 - 2z_{j}\beta_j)) \exp(-\frac{n \lambda} {\sigma^2} \abs{\beta_j}) \\
&= \exp(-\frac{n}{2\sigma^2} (\beta_j^2 - 2 z_j\beta_j +  2 \lambda \abs{\beta_j})) \\
&= \exp(-\frac{n}{2\sigma^2} (\beta_j^2 - 2(z_j\beta_j - \lambda \abs{\beta_j}))) \\
&=
\begin{cases}
\exp(-\frac{n}{2\sigma^2} (\beta_j^2 - 2(z_j + \lambda)\beta_j)), \text{ if } \beta_j < 0, \\
\exp(-\frac{n}{2\sigma^2} (\beta_j^2 - 2(z_j - \lambda)\beta_j )), \text{ if } \beta_j \geq 0 \\
\end{cases} \\
&\propto
\begin{cases}
C_{-} \exp(-\frac{n}{2\sigma^2} (\beta_j - (z_j + \lambda))^2), \text{ if } \beta_j < 0, \\
C_{+} \exp(-\frac{n}{2\sigma^2} (\beta_j - (z_j - \lambda))^2), \text{ if } \beta_j \geq 0 \\
\end{cases}
}
where, $C_{-} = \exp(\frac{z_j \lambda n}{\sigma^2})$ and $C_{+} = \exp(-\frac{z_j \lambda n}{\sigma^2})$.

At this point, te reader likely notices that the piecewise defined full conditional posterior is made up of a kernel of two normal distributions. This can be leveraged and draws can be efficiently obtained from through a mapping onto the respective normal distributions. To define this mapping, it helps to introduce a concept and some notation. First, the use of ``tails'' in this supplement refers to the entirety the a distribution between zero and $\pm \infty$. That is, the lower tail is any part of the distribution below zero and the upper tail is any part greater than zero and $P(X \in lower \cup X \in upper) = 1$. Accordingly, we will let the tail probabilities in each of the two normals to transformed on to be denoted $Pr_{-}$ and $Pr_{+}$ respectively and the probability in each of the tails of the posterior, denoted $Post_{-}$ and $Post_{+}$ respectively. $Pr_{\pm}$ is trivial to compute with any statistical software. $Post_{\pm}$ is conceptually simple, although care must be taken to avoid numerical instability as n increases. Now, as noted,
\as{
P(\beta_j | \r_{-j})  & \propto
\begin{cases}
C_{-} Pr_{-}, \text{ if } \beta_j < 0, \\
C_{+} Pr_{+}, \text{ if } \beta_j \geq 0\\
\end{cases}
} which implies that $Post_- = \frac{C_{-} Pr_{-}}{C_{-} Pr_{-} + C_{+} Pr_{+}}$ and similarly for $Post_+$. However, to avoid numerical instability, or at least handle it properly when it is unavoidable, we need to work on the $\log$ scale. This works well for most of the problem, but computation of $Post_-$ and $Post_+$ need something a bit more since, for example, $\log(Post_-) = \log(C_{-}Pr_{-}) - \log(C_{-} Pr_{-} + C_{+} Pr_{+})$. That is, the denominator still must be computed then the $\log$ taken which does not allow operation on the $\log$ scale to fully address potential instability. Instead, $\log(Post_-)$ can be computed with $\log(C_-Pr_-) -  \log(C_+Pr_+) - \log(1 + \exp(\log(C_-Pr_-) -  \log(C_+Pr_+)))$. This still doesn't completely address the issue, however, if $\exp(\log(C_-Pr_-) -  \log(C_+Pr_+))$ is infinite then $C_-Pr_- >> C_+Pr_+$ and $\log(Post_-) \approx 0$.

With these values, we can compute quantiles by mapping the corresponding probabilities $p$ for the posterior onto the probabilities $p^*$ for the corresponding normals. Which normal the quantiles of interest ultimately come from is determined based on $Post_{\pm}$. For example, if $Post_{+} = 0.98$ and $p = 0.1$ the $p$ would be mapped onto the positive normal. As one more example, say $Post_{+} = 0.4$ and $p = 0.5$, then $p$ would be mapped onto the negative normal. The transformation to map a given probability from the posterior depends on which tail the quantile resides in on the posterior (equivalently which normal it is being mapped to, the positive or negative). This map is simply:

\as{
p^* &= p \times (Pr_{\pm} / Post_{\pm}) \\
}


Once the respective probabilities are mapped, one can simply use the inverses of the normal CDFs that the probabilities were mapped to. That being said, there is a nuance worth pointing out. When transforming the probabilities, the step to determine which tail the respective quantile comes from occurs first. With this, the probability should be adjusted so that it refers to the probability between the quantile of interest and the respective tail. After this, then the transformation can be applied. With that, obtaining draws from the full conditional posterior can be summarized as follows (written for a single $\beta$ for simplicity):

\begin{enumerate}
  \item Select $\lambda$, fit lasso and obtain estimates corresponding to $\lambda$, estimate $\sigma^2$
	\item Obtain the partial residuals, $\r_{-j}$, and compute $z_j$
	\item Compute $Pr_{-}$ = $\Phi(0, z_j + \lam, \frac{\sh^2}{n})$ and $Pr_{+}$ = $1 - \Phi(0, z_j - \lam, \frac{\sh^2}{n})$
	\item Compute $Post_-$ and $Post_+$ as detailed above
	\item Obtain the quantile $(q)$ corresponding to the given probability $(p)$ of interest:
  \begin{algorithmic}
    \If {$p \leq Post_{-}$}
      \State $q = \Phi^{-1}(p(Pr_{-} / Post_{-}), z_j + \lam, \frac{\sh^2}{n})$
    \Else
        \State $q = \Phi^{-1}(1 - (1 - p)(Pr_{+} / Post_{+}), z_j - \lam, \frac{\sh^2}{n})$
    \EndIf
  \end{algorithmic}
\end{enumerate}

\newpage

\section{Behavior under true lambda}
\label{Sup:B}

This simulation considers the behavior of the Hybrid intervals if $\lam$ is set to its true value and, additionally, if $\sigma^2$ is also set to its true value, in this case, one. The simulation set up here is identical to the one described in Section~\ref{Sec:Coverage}. Recall that, like the rest of the simulation in the manuscript, the coefficients where generated s.t. $\bb^T\bb = 1$ so that with $\sigma^2=1$ and independent features, a signal-to-noise ratio (SNR) of 1 is achieved. Also recall that the simulation in Section~\ref{Sec:Coverage} generated $\beta$ from a Laplace distribution. Thus, this implies a true Laplace rate of $\gamma=\sqrt{2p}$, since $\sum_{j=1}^p\Ex(\beta_j^2|\gamma=\sqrt{2p}) = 1$. So, the only difference in the simulation here is that $\lambda$ is set to be $\gamma / n = \frac{\sqrt{2p}}{n}$ instead of being selected via cross validation. 

\begin{figure}[hbtp]
  \begin{center}
  \includegraphics[width=0.65\linewidth]{true_lambda}
  \caption{\label{Fig:true_lambda} The results displayed are from a simulation with the same data generating mechanism as described in Section~\ref{Sec:Coverage} but with three different samples sizes. The smooth curves are also constructed in the same manner and provide estimates for coverage as a smooth function of $\beta$. The dashed horizontal lines indicated the average coverage across all 100 simulations. The left plot shows the results where the true value of $\lambda$ was used in the Hybrid bootstrap and the right provides results where both $\lam$ and $\sigma^2$ were set at their true values. The black line indicates the 80\% nominal coverage rate.}
  \end{center}
\end{figure}

Figure~\ref{Fig:true_lambda} shows the results of the simulation described with $\lambda$ set to the true value (left) and with both $\lambda$ and $\sigma^2$ set to their true values (right), see the right plot in Figure~\ref{Fig:nominal_coverage} as a comparison for $\lambda_{\CV}$ and $\hat{\sigma}^2(\lambda_{\CV})$. When just the true value of $\lam$ is known, there is slightly more over coverage (generally $\lam_{\CV}$ is larger than the true value of $\lam$), but otherwise the results are generally comparable to when $\lambda$ is selected via CV, assuming $\hat{\sigma}^2$ is set equal to the minimum CVE. This, like Figure~\ref{Fig:beta_lambda_heatmap_laplace}, suggests that $\lam_{\CV}$ generally provides a reasonable selection of $\lam$. However, the differences are more notable when $\sigma^2$ is also set to its true value of one. All the coverages are near nominal, with slight under coverage for larger values of n. This suggests that using CVE to estimate $\sigma^2$, which generally over estimates variability, especially when $n < p$, is responsible for the over coverage observed throughout the results for the Hybrid bootstrap. However, in absence of knowing the true value of $\sigma^2$, over estimating it and maintain coverage above that of nominal is preferred to the alternative. 

\logan{Now that I come back to this, I am remembering why I added the true value of lambda to the heatmap... wondering if it is worth representing it here with the additional line? Specifically that $\lambda_{\CV}$ is in between the value which provides nominal coverage and the truth.}

\newpage

\section{Unmodified BLP Real Data Analysis}
\label{Sup:C}

\begin{figure}[hbtp]
  \begin{center}
  \includegraphics[width=0.6\linewidth]{comparison_data_original}
  \caption{\label{Fig:comparison_data_whoari_original} Confidence intervals produced by unmodified BLP for all 66 variables in the \texttt{whoari} dataset described in Section~\ref{Sec:whoari}}
  \end{center}
\end{figure}

\begin{figure}[hbtp]
  \begin{center}
  \includegraphics[width=0.6\linewidth]{comparison_data_scheetz_original}
  \caption{\label{Fig:comparison_data_scheetz_original} Confidence intervals produced by unmodified BLP for the 20 variables with the largest absolute point estimates in the \texttt{Scheetz2006} dataset described in Section~\ref{Sec:Scheetz2006}}
  \end{center}
\end{figure}

\newpage

\section{Nominal Coverage}\label{Sup:D}

\begin{figure}[hbtp]
  \begin{center}
  \includegraphics[width=0.6\linewidth]{nominal_coverage}
  \caption{\label{Fig:nominal_coverage}  The results displayed are from a simulation with the same data generating mechanism as described in Section~\ref{Sec:Coverage} but with three different samples sizes across three different nominal coverage rates, indicated by the horizontal black lines. The smooth curves are also constructed in the same manner and provide estimates for coverage as a smooth function of $\beta$. The dashed horizontal lines indicated the average coverage across all 100 simulations}
  \end{center}
\end{figure} 

Figure~\ref{Fig:nominal_coverage} is similar to Figure~\ref{Fig:laplace}, but focuses only on Hybrid and gives simulation results for three values of n across three different nominal coverage rates. Since the Hybrid bootstrap method has a varying coverage rates depending on the magnitude of $\beta$, it is important to consider different nominal coverages. Otherwise, it is conceivable that a method could perform well at one coverage rate but not another. However, that is not the case here. Regardless of the nominal coverage, the general pattern remains the same: the method over covers for smaller values of n but coverage converges to the nominal coverage rate relatively quickly. The only difference seen is the compression of this pattern for higher nominal coverage rates.

\end{appendices}
