\section{Inferential Paradigm}

The method to be proposed falls into a bit of a grey area that isn't well encapsulated by common interential scopes such as "marginal", "conditional", or "joint". To that end, we breifly describe a different perspective here which we call \textit{Order Inference}.

The idea of \textit{Order Inference} focuses on the variable space which inferential methods are conducted. Specifcally, we will let order referece the relation of the current variable under consideration, $\x_i$, to the other variables, $\x_j$, where $j \neq i$ as well as their relation to the outcome variable $\y$. To visualize the concept of \textit{Order Inference}, we will provide a simple example using multiple linear regression.

\textbf{Zeroth Order} ($O_z$): Here, no consideration is given to $\x_j$. All inference for $\beta_i$ is done based on the residuals from $\hat{y}$ as a projection onto the column space of $X_i$.

\textbf{First Order} ($O_1$): In this paradigm, this is the most difficult to capture since it could take a few forms. The way this occurs is by projecting $\y$ first onto the columns space of $\X_j$ then the residuals $\r_j$ onto the column space of $\X_i$. How the first projection occurs is where alternatives forms into play. A literal translation would involve p two-step regressions. An alternate form that is more akin to the approach to be proposed requires one joint regression, calculation of partial residuals $\r_{-i}$ then projecting $\r_{-i}$ onto the column space of $\x_i$.

\textbf{Second Order} ($O_2$): With $n \gg p$ and $\epsilon \sim N(0, \sigma^2)$, this is the traditional analysis most would run: perform joint inference with residuals from $\hat{y}$ as a projection onto the column space of $\boldsymbol{X}$.

\subsection{Bootstrap Sampling}

There are various ways to perform a single iteration of a bootstrap, among them are the pairs bootstrap and the residual bootstrap. For high dimensional problems in general, the pairs bootstrap is attractive. First, it makes the fewest assumptions compared to other methods. Specifically, the only assumption made for the pairs bootstrap is that the original pairs were randomly sampled from some distribution $F$, where $F$ is a distribution on (p + 1)-dimensional vectors (Efron, Tibshirani). Additionally, the pairs bootstrap is simple to perform. Finally, and perhaps most importantly, it treats $\X$ as random which is almost surely the case in high dimensional settings. For this reason, we will solely focus on and use the pairs bootstrap in our proposed procedure.

\subsection{Space Requirements}

As implimented, this method takes a numeric matrix size $Br \times p$. With $B = 100, r = 10$, the sample matrix gets too large enough to cause memory concerns even when $p$ is on the order of $1e5$. For many datasets, this is likely not of concern. however, given that lasso is often used for datasets where $p$ is large, it is clearly not an edge case where $p$ exceeds the order of $1e5$. One could reduce the number size of the same matrix by reducing the number of draws, i.e. let $r = 1$, but this is unstaisfactoy and produces little additional leeway for what seems like a unnacceptable sacrifice. What we propose instead is using incremental quantile estimation as introduced by \cite{Tierney1983}. Although not currently part of the method in \texttt{ncvreg} at the time of this publication, this is an active area of implimentation. Incremental quantile estimation is an ongoing area of research, espeically in feilds like computer networking.

One concern here is that the additional approximation in incremental quantile estimation could result in larger errors, however, it is our belief that these errors will be small in relation to the approximations already a part of the method. Addiitonally, we believe that the errors introduced by incremental quantile estimation will quickly be offset by the ability to have obtained orders of magnitude more draws.

This of course opens up the large question of what method to use for incremental quantile estimation.

We are currently working on implimenting and assessing using the method introduced by \cite{Tierney1983}, but this is an area of ongoing interest for us.

Luckily, as far as problems go, this is a relatively benign use case. The primary obsticle that a method needs to be able to address is adhering to space requirements. Ideally, the storage space required should not grow with the number of draws or should at least grow at a slower rate than the number of draws (i.e. no faster than $\log(\# of draws)$). In fact, per quantile tracked, the method presented by Tierney only requries 6 direct access storage points. As \cite{Tierney1983} notes, his method only has the requirement that F has "a bounded derivative that is continuous at $\xi$", where $\xi$ is the tracked quantile. The only point at which this could be violated for the distributions we are tracking would be if $\xi = 0$, since the derivative is not necessarily continuous at this point. However, since the distribution itself is continous at 0, $\xi = 0$ w.p. 0.

One drawback of using the method as introduced by Tierney is that is only addresses tracking of a single quantile at a time. Recent research has focused on joint quantile tracking. Future exploration into how such methods perform would undoubtably add robustness, however, given the use case here of tracking two quantiles in seperate tails, we accept this limitation with caution.
